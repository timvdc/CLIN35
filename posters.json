{
  "title": "Conference Poster Sessions",
  "description": "Two poster sessions with balanced thematic groupings",
  "sessions": [
    {
      "id": "morning",
      "name": "Morning Poster Session",
      "time": "10:35-11:45",
      "rooms": 4,
      "groups": [
        {
          "id": 1,
          "name": "Large Language Models & Advanced AI",
          "description": "LLM research, RAG systems, and advanced AI development",
          "color": "#8B5CF6",
          "posters": [
            {
              "id": 4,
              "title": "Type and Complexity Signals in Multilingual Question Representations",
              "authors": "Robin Kokot, Wessel Poelman, Miryam de Lhoneux",
              "affiliation": "KU Leuven",
              "abstract": "We investigate how Glot500-m, a multilingual transformer model, encodes structural linguistic properties of questions. Our experiments use binary classification and continuous regression tasks designed to probe the encoding of question types and morphosyntactic complexity in seven languages. We compare the performance of probes to statistical term frequency baselines and the fine-tuned model, using selectivity controls with shuffled labels to contextualize our results. We show that questions in languages with more explicit type markers can be classified accurately using subword frequency features, while those with more implicit strategies benefit from contextual embeddings. We introduce regression probing for complexity metrics and demonstrate that traditional ML algorithms trained on TF-IDF features perform well on composite complexity scores. However, neural probes often achieve better selectivity than linear regression or gradient boosting. Individual submetrics show noticeable patterns: statistical models outperform probes on structural metrics like tree depth and subordinate chain length, while neural probes perform better at predicting token count, with relatively flat performance curves across layers indicating limited layer-wise specialization. We also note that probes on frozen encoder representations often outperform the fine-tuned model on all regression tasks, suggesting that end-to-end updating may degrade performance on linguistic regression labels."
            },
            {
              "id": 6,
              "title": "Does Knowledge Matter? A use case of K-RAG in the Legal Domain",
              "authors": "Daan L. Di Scala, Roos M. Bakker, Romy van Drie, Maaike H. T. de Boer",
              "affiliation": "TNO & Utrecht University",
              "abstract": "Current generative Large Language Models such as GPT are often used as a backbone in chatbots. They do, however, suffer from hallucinations, and are difficult to evaluate on factuality, as answers are often difficult to trace back to their sources.  This is a problem in the legal domain, as chatbots are already used by judges and lawyers, and factually correct answers are essential for them [1]. In our use case, we take the role of someone working at the municipality, who provides information to citizens about the law. We create a dataset based on three Dutch laws (Participatiewet, Vreemdelingenwet and Algemene Ouderdomswet) for which we formulated over 100 question-answer pairs, validated by experts in the legal field. We compare a state-of-the-art Retrieval Augmented Generation (RAG) approach - which uses the laws as natural text - to knowledge-based RAG (K-RAG) approaches that use structured knowledge from the law as formalized into Flint frames [2]. This work builds upon earlier work on attributed question answering [3]. The results will show whether structured knowledge matters in a RAG application.\n \n[1] Ongeloof om Nederlandse rechter die ChatGPT gebruikt in vonnis: ‘Dit kan echt niet’, S Quekel, https://www.ad.nl/binnenland/ongeloof-om-nederlandse-rechter-die-chatgpt-gebruikt-in-vonnis-dit-kan-echt-niet~ae3288e10/ [Accessed 06-06-2025]\n\n[2] Semantic role extraction in law texts: a comparative analysis of language models for legal information extraction, RM Bakker, AJ Schoevers, RAN van Drie, MP Schraagen, MHT de Boer, Artificial Intelligence and Law, 1-35\n\n[3] Attributed Question Answering for Preconditions in the Dutch Law, F Redelaar, R Van Drie, S Verberne, M De Boer, Proceedings of the Natural Legal Language Processing Workshop 2024, 154-165"
            },
            {
              "id": 34,
              "title": "SMILE-X: Small Language Models Improved through Learning from Explanations and Agent Exchange",
              "authors": "Elke Vandermeerschen, Tinne De Laet, Tim Van De Cruys",
              "affiliation": "KU Leuven",
              "abstract": "Transformer-based large language models (LLMs) have significantly advanced natural language processing across diverse applications. Yet, concerns are growing over issues such as bias (Gallegos et al., 2024), opacity (Calderon and Reichart, 2025), privacy (Xiao et al., 2024), and environmental impact (Singh et al., 2025). Open source small language models (SLMs) offer a more transparent, customisable, and sustainable alternative. However, their reasoning capabilities, common sense knowledge and domain-specific expertise often lag behind (Lu et al., 2024). To bridge this gap, SLMs must be guided towards more deliberate, grounded, step-by-step reasoning and given access to common sense and domain knowledge. We propose SMILE-X (Small Models Improved by Learning\nfrom Explanations and Agent Exchange), a framework that enhances SLMs’ reasoning capabilities and knowledge access through two complementary strategies: learning from natural language explanations, and coordinating specialised agents in structured collaboration.\nFirst, SMILE-X uses natural language explanations as learning signals to structure inference, make reasoning explicit, and ground it in logic and common sense, an approach supported by recent work on explanation-guided learning (Lampinen et al., 2022; Li et al., 2022; Magister et al., 2023). This improved reasoning ability enables the model to function effectively as an individual agent within the SMILE-X framework.\nSecond, these enhanced agents are coordinated in a structured multiagent debate, enabling collaboration between agents with distinct roles, tools and capabilities (e.g. reasoning, knowledge retrieval, or value alignment), drawing on recent advances in agent-based coordination (Chen et al., 2024; Wang et al., 2024). Our research investigates the roles, coordination, and decisionmaking strategies of these explanation-strengthened small specialised agents, with a focus on explanation-driven interactions to support adaptability and transparency.\nThe flexibility of our open-source, multi-agent framework supports its use across domains, with education as a key area of focus. In this context, it will be applied to provide student feedback resulting from a debate among specialized agents grounded in course content, informed by feedback theory, and aligned with university values.\n\nReferences\n\nCalderon, N. and Reichart, R. (2025). On behalf of the stakeholders: Trends in NLP model interpretability in the era of LLMs. In Chiruzzo, L., Ritter, A., and Wang, L., editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 656–693, Albuquerque, New Mexico. Association for Computational Linguistics.\n\nChen, J., Saha, S., and Bansal, M. (2024). ReConcile: Round-table conference improves reasoning via consensus among diverse LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7066–7085, Bangkok, Thailand. Association for Computational Linguistics.\n\nGallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N. K. (2024). Bias and fairness in large language models: A survey. Computational Linguistics, 50(3):1097–1179.\n\nLampinen, A., Dasgupta, I., Chan, S., Mathewson, K., Tessler, M., Creswell, A., McClelland, J., Wang, J., and Hill, F. (2022). Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 537–563, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nLi, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J., Peng, B., Mao, Y., Chen, W., and Yan, X. (2022). Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726.\n\nLu, Z., Li, X., Cai, D., Yi, R., Liu, F., Zhang, X., Lane, N. D., and Xu, M. (2024). Small language models: Survey, measurements, and insights. CoRR, abs/2409.15790.\n\nMagister, L. C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A. (2023). Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1773–1781, Toronto, Canada. Association for Computational Linguistics.\n\nSingh, A., Patel, N. P., Ehtesham, A., Kumar, S., and Khoei, T. T. (2025). A survey of sustainability in large language models: Applications, economics, and challenges. In 2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC), pages 00008–00014. IEEE.\n\nWang, Q., Wang, Z., Su, Y., Tong, H., and Song, Y. (2024). Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6106–6131, Bangkok, Thailand. Association for Computational Linguistics.\n\nXiao, Y., Jin, Y., Bai, Y., Wu, Y., Yang, X., Luo, X., Yu, W., Zhao, X., Liu, Y., Gu, Q., Chen, H., Wang, W., and Cheng, W. (2024). Large language models can be contextual privacy protection learners. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N., editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 14179–14201, Miami, Florida, USA. Association for Computational Linguistics."
            },
            {
              "id": 47,
              "title": "Data Sampling Effects In Instruction-Tuning",
              "authors": "Michiel van der Meer, Urja Khurana, Enrico Liscio, Suzan Verberne",
              "affiliation": "Leiden University, TU Delft",
              "abstract": "Instruction tuning (IT) has emerged as a fundamental method for aligning Large Language Models\n(LLMs) with human preferences and task-specific behavior. Typically, a considerable amount\nof data is included to make the model adaptable to many different contexts. Companies go to\ngreat lengths to create an optimal data distribution for proprietary models, though their exact\ncomposition is often not reported. Surprisingly, recent work suggests that alignment can be achieved\nwith remarkably small datasets, compared to the volume of pretraining data, often in the order of\njust a few thousand examples (Less is More for Alignment, Zhou et al., 2023). The unclear best\npractices for data selection and ad-hoc evaluation setups for instruction tuning call for a systematic\nexploration.\n\nAs the societal and industrial uptake of LLMs increases, there is a progressively larger demand\nfor systems that cater to a broad set of needs. In order to deal with the diverse perspectives and\nvalues, we should construct pluralistic and steerable systems (A Roadmap to Pluralistic Alignment,\nSorensen et al., 2024). If alignment is achievable with only a small set of data, then which data\nwe include becomes a major factor in determining the direction of the alignment of a model.\nFurther, the diversity of the data influences how steerable the resulting model will be. Our work\ntests these assumptions by systematically examining the relationship between data\nselection mechanisms and downstream LLM performance, with a specific focus on how\nmodels behave across both objective and subjective tasks. We perform our analyses across\ntwo types of tasks: (1) Objective tasks, where there tends to be high inter-annotator agreement\nand clear ground truths (e.g., math problem solving), and (2) Subjective tasks, where annotation\ndisagreement is common due to ambiguity or personal judgment (e.g., conversation ratings, hate\nspeech detection).\n\nWe explore several data selection methods, including simple heuristics like choosing the longest\nsamples, gradient-based instance selection, and active learning-based methods. In evaluating these\nmethods, we focus both on task performance and performance variance under disagreement within\nand across tasks. We hypothesize that data selection mechanisms significantly influence\nboth mean performance and the controllability of a model. This aligns with recent research\nemphasizing ambiguity and annotator disagreement as intrinsic properties of subjective datasets.\nConversely, in objective tasks, simpler selection heuristics often suffice, and models show lower\nvariance in performance regardless of the data source. We will further explore whether smaller\ninstruction tuning datasets correlate with higher variance in model outputs. Preliminary results\nsupport this intuition: fewer examples lead to less stable performance, particularly when those\nexamples represent only a narrow view of possible task formulations. It is unclear whether this\neffect can be mitigated by careful selection methods that aim to cover more diverse samples within\nthe limited sample budget. Ultimately, we intend to unveil that the choice of the data used for\ninstruction tuning is as important as the quantity, and that thoughtful dataset curation is a crucial,\nunderexplored component of scalable LLM alignment."
            },
            {
              "id": 48,
              "title": "The Impact of Continual Reasoning Learning on Language Understanding in LLMs",
              "authors": "Marzieh Abdolmaleki, Véronique Hoste, Els Lefever",
              "affiliation": "LT3, Ghent University",
              "abstract": "A central objective in artificial intelligence is to create agents capable of lifelong learning, continuously acquiring new knowledge while retaining previously learned information. Large language models (LLMs), despite their impressive natural language understanding (NLU) capabilities, face significant challenges due to catastrophic forgetting (CF), where learning new tasks leads to rapid loss of earlier knowledge. Reasoning tasks, such as deductive, inductive, and abductive reasoning, are essential to human cognition and language understanding, yet their interactions and impact on lifelong learning in LLMs remain underexplored.\nThis research investigates how sequential learning of diverse reasoning tasks affects knowledge transfer and CF during continual instruction tuning in LLMs. We analyze how training on different reasoning types influences model performance in NLU tasks and explore the complex relationships between reasoning and language understanding.\nIn particular, we conducted preliminary experiments on defeasible reasoning, a challenging form of reasoning where conclusions can be revised based on new evidence. Whether the model is trained separately on generating strengtheners (supporting evidence) and weakeners (contradictory evidence) or on both simultaneously, the loss on pretraining data remains nearly unchanged. Our findings indicate that while the model retains prior knowledge during this training (i.e., no catastrophic forgetting occurs), it also fails to exhibit improvements in general language understanding. This lack of positive transfer suggests that defeasible reasoning may be relatively isolated or specialized within the model’s representational space, limiting its contribution to broader NLU capabilities. Alternatively, it may indicate that the model has not learned the new task in sufficient depth, which is consistent with the observation that current LLMs struggle with defeasible reasoning (Rudinger et al., 2020).\nIn this presentation, we aim to highlight the limitations of current models in effectively integrating multiple reasoning skills. A better understanding of these dynamics can guide the development of more robust LLMs with enhanced reasoning and language understanding."
            },
            {
              "id": 50,
              "title": "Exploring reasoning capabilities of Large Language Models on low-resourced languages, a case study on the Vietnamese language",
              "authors": "Tuan Anh Do, Jelke Bloem",
              "affiliation": "University of Amsterdam",
              "abstract": "Originally developed for text generation, Large Language Models (LLMs) have been shown to achieve impressive performance on natural language understanding and reasoning tasks via prompting and instruction tuning methods. Prompting techniques, such as Chain-of-Thought (CoT) and few-shot cross-lingual CoT, which leverage step-by-step answers or guided exemplars to trigger complex intermediate reasoning, have led to substantial improvements in the model's response correctness. However, the development and evaluation of LLMs remain predominantly focused on a few high-resource languages, due to the lack of a dedicated evaluation framework and datasets needed to investigate (local) models on less-studied languages. \n\nThis study addresses this gap by investigating the performance of LLMs on two fundamental reasoning tasks of machine intelligence, specifically commonsense reasoning and arithmetic reasoning tasks, in the Vietnamese language, one of the most widely spoken languages in the world, yet remaining under-researched. We explored the performance of models across three categories: English-based models (LLaMa-2/3, Phi-3/4), multilingual models (BLOOMz, SeaLLM), Vietnamese fine-tuned models (Vistral, VBD_LlaMa, PhoGPT), and locally trained models (PhoGPT). A collection of different prompts with varied levels of perplexity is tested, including Chain-of-Thought, role-playing guidance, cross-lingual prompting, and few-shot learning. As Vietnamese benchmarks for these tasks are lacking, we adapt two analogy datasets from English to Vietnamese and construct two sequence datasets, ensuring a range of structural complexity and difficulty levels. \n\nOur initial results reveal a baseline proficiency in analogical and arithmetic reasoning among the models, with Vietnamese models like Vistral and Llama-2 outperforming other models in multiple tasks. Notably, the effects of Chain-of-Thought and contextual guidance are limited in monolingual settings, while cross-lingual prompting and few-shot learning show promising performance improvements. In addition, we observe instances of potential pattern memorization and catastrophic forgetting in the LLMs. Fine-tuned models show reduced performance when partially instructed with mixed language, and asking the LLMs to predict the next two terms on the sequence completion tasks yields better results than predicting just one immediate next term. For less language-dependent tasks of mathematical reasoning, our findings also indicate that cross-lingual prompting and in-context learning can better guide the model towards the correct reasoning path, even though no significant relationships between temperature setting, prompted language, and accuracy is observed. The findings underscore the feasibility of adapting benchmarks to less-resourced languages and providing insights into the nuanced performance of Vietnamese LLMs, suggesting ways for further model improvements."
            },
            {
              "id": 57,
              "title": "Training-Phase Behaviour of Forward and Reverse KL Losses in Knowledge Distillation for Small Language Models",
              "authors": "Shaozhen Shi, Yevgen Matusevych, Huiyuan Lai, Malvina Nissim",
              "affiliation": "University of Groningen",
              "abstract": "The Kullback–Leibler (KL) divergence is a core objective in knowledge distillation. Recent work describes the forward KL (FKL) as mass‑covering—it spreads probability across all tokens that the teacher deems plausible—whereas the reverse KL (RKL) is mode‑seeking, concentrating on the teacher’s highest‑probability modes and down-weighting low‑probability (rare) tokens (Wang et al., 2024; Wen et al., 2023; Yao et al., 2025). While it has been reported that the two objectives tend to converge after extended training in large language models, their distinct behaviors in small models remain underexplored (Wu et al., 2024). Most importantly, existing work does not analyze the probabilities of generated tokens in specific contexts. \n\nWe address this gap by distilling a 360M-parameter Llama teacher into two 58M-parameter Baby-Llama students  (Timiryasov & Tastet, 2023)—one trained with FKL and the other with RKL. All models are pre-trained on the BabyLM 10M (Strict-Small) corpus, a 10-million-token English dataset rich in child-directed and narrative text released for the 2024 BabyLM Challenge (Hu et al., 2024).\nWe evaluate both student models on sentences from the BLiMP distractor agreement relational noun task, which tests subject-verb agreement based on sentence pairs like \"A story about the Balkans doesn't/*don't irritate a person\". For each token position in these sentences (~150k tokens in total), we compute the difference in the absolute probability of the target token between teacher and each student model, and then compare the resulting values for FKL vs RKL. Additionally, we identify critical tokens—those that differ between grammatical and ungrammatical sentences in the BLiMP minimal pairs (i.e., \"doesn't\" vs \"don't\" in the example above)—and examine whether FKL and RKL models show distinct behaviors on these linguistically important positions.\nAcross the full training run, we observe a clear training-phase trend: the FKL student holds a strong overall lead early on (62% of tokens), the margin narrows steadily, and by the final checkpoint the two losses give almost the same accuracy (52% vs 48%). Our frequency-based analysis reveals that FKL maintains advantages on high-frequency tokens (top 1000 in BOS ranking), whereas RKL narrows the gap and eventually matches FKL on rare tokens (rank > 10,000) as learning progresses. Analysis of critical tokens shows that both learners converge toward similar performance on these grammatically decisive positions, though FKL maintains a slight advantage in early training phases. Our results confirm earlier large-model findings that forward and reverse KL losses eventually converge during training (Wu et al., 2024). In contrast to the lasting reverse-KL advantage reported by Gu et al. (2023), our study shows that forward KL performs better on high-frequency tokens, whereas reverse KL only matches it on rare tokens (Wen et al., 2023). Based on our current experimental setup, we find that loss choice matters chiefly in the early phase of training and becomes less critical as the model approaches convergence. This study therefore offers, to our knowledge, the first systematic token-level comparison of forward and reverse KL under these conditions and may inform more efficient distillation strategies for small language models."
            },
            {
              "id": 80,
              "title": "A Metasemantic Approach to the Grounding Problem in LLMs",
              "authors": "Mayra Huespe",
              "affiliation": "University of Amsterdam",
              "abstract": "The question Do Large Language Models (LLMs) generate meaningful outcomes?—also framed as the grounding problem (Harnad, 1990) for LLMs—has received increasing attention in both computational linguistics and the philosophy of artificial intelligence (Bender & Koller, 2020; Mandelkern & Linzen, 2024; Grindrod, 2024; Gubelmann, 2024a; Gubelmann, 2024b). In the computational linguistics literature, this question has typically been addressed by adopting semantic theories of meaning as normative frameworks for analysis. These frameworks determine which features of LLMs—such as model architecture or training data—are relevant when evaluating whether their outputs can be considered meaningful. For instance, in a very well-known paper, Bender and Koller (2020) argue that, to count as meaningful, LLM outputs must satisfy specific epistemic conditions, such as causal contact with referents, which follow from assuming a referential semantic theory. In this way, the grounding problem is framed as a semantic issue that can be resolved by evaluating whether LLMs meet the conditions set by a given theory of meaning.\n\nIn this paper, I argue that this strategy rests on a conceptual confusion between semantic and metasemantic levels of analysis. While semantic theories provide principles for assigning meaning to expressions, metasemantic theories investigate the facts that justify why these semantic assignments are valid (Stalnaker, 2017). Rooted in this distinction, I claim that the capacity of LLMs to generate meaningful outcomes is not primarily a semantic issue but a metasemantic one. That is, the relevant question is not what meaning a given output has, but which underlying facts—if any—justify interpreting it as meaningful. From this perspective, trying to solve the grounding problem by adopting semantic theories of meaning amounts to a category mistake: it assumes that semantic principles can answer a fundamentally metasemantic question.\n\nIn response, I propose an alternative, metasemantic strategy grounded in Putnam’s externalist theory of meaning (Putnam, 1975). More specifically, I suggest that a promising approach should start by reconstructing the concept of semantic capabilities as it is already used and understood by the relevant expert communities working on these systems—namely, researchers in Natural Language Processing (NLP) and, particularly, within the subfield of Explainable AI (xAI). In recent years, various xAI techniques have been developed to investigate meaning-related properties of LLMs. These include feature attribution methods, such as Integrated Gradients and SHAP, attention-based explanations, probing techniques for syntactic and semantic properties, natural language rationales (e.g., Chain-of-Thought prompting), and concept-based methods, including TCAV (Zhao et al., 2024), among others.\n\nI argue that any philosophical investigation into the grounding problem in LLMs should start by reconstructing the notion of meaning as it is presupposed in these techniques. This approach allows us to reframe the grounding problem in empirically tractable terms, without assuming that LLMs must replicate human-like epistemic conditions for generating meaning. Instead, we ask whether and how these systems can be said to instantiate minimal, model-relative forms of semantic function, as assessed through xAI tools. This strategy represents a shift toward an empirically grounded way of addressing the grounding problem in LLMs.\n\nReferences: \n\nBender, Emily and Alexander Koller (2020). “Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In: Proceedings of the Annual Meeting of the Association for Computational Linguistics. URL: https://api.semanticscholar.org/CorpusID:211029226.\nGrindrod, Jumbly (2024). “Large Language Models and Linguistic Intentionality.” In: Synthese 204.2, p. 71.\n\nGubelmann, Reto (2024a). “Large Language Models, Agency, and Why Speech Acts Are Beyond Them (for Now) – A Kantian-Cum-Pragmatist Case.” In: Philosophy & Technology 37.1, p. 32.\n— (2024b). “Pragmatic Norms Are All You Need – Why the Symbol Grounding Problem Does Not Apply to LLMs.” In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 11663–11678.\n\nHarnad, Stevan (1990). “The Symbol-Grounding Problem.” In: Physica D 42.\n\nMandelkern, Matthew and Tal Linzen (2024). “Do Language Models’ Words Refer?” In: Computational Linguistics 50 (3). URL: https://direct.mit.edu/coli/article/50/3/1191/121670/Do-Language-Models-Words-Refer.\n\nPutnam, Hilary (1975). “The Meaning of ‘Meaning’.” In: Minnesota Studies in the Philosophy of Science 7, pp. 131–193.\n\nStalnaker, Robert (2017). “Reference and Necessity.” In: A Companion to the Philosophy of Language, pp. 902–919.\n\nZhao, Haiyan et al. (2024). “Explainability for Large Language Models: A Survey.” In: ACM Transactions on Intelligent Systems and Technology 15.2, pp. 1–38."
            },
            {
              "id": 25,
              "title": "Reinforcement Learning from Human Feedback for Legal Ontology Information Extraction",
	      "authors": "Jacques Fürst, Roos M. Bakker, Daniele Foffano",
	      "affiliation": "TNO and KTH Royal Institute of Technology, TNO and Leiden University, KTH Royal Institute of Technology",
              "abstract": "Information extraction from texts with complicated structure and vocabulary,\nsuch as from the legal domain, is a notoriously hard problem due to high\nprecision requirements and ambiguity in legal texts. It has been shown that\nLarge Language Models (LLMs) have potential for this type of task [1].\nReinforcement Learning from Human Feedback (RLHF) has emerged as a\nhighly effective approach for aligning large language models with human\npreferences and expectations [2]. Yet, it has not been investigated extensively\nwhether it is possible to increase an LLM’s performance on extracting\nontology-based formal concepts such as facts, acts and preconditions from\ncomplicated domain texts through the means of RLHF. In this study, we\ninvestigate methods to improve the performance of smaller-scale LLMs on a\ncomplex information extraction task by using 1) RLHF and 2) reinforcement\nlearning from synthetic feedback. To explore this, we gather data on facts,\nacts and preconditions extracted by domain experts from legal text, using\nan ontology specifically designed for legal documents. The task the model\nthen has to perform is extracting respective preconditions when given an\nact from the legal text, or subfacts when given a fact. To clarify through an\nexample, if being a member of the library is a precondition for borrowing\na book, we would like the model to return this as a precondition when\ngiven the library membership terms and conditions in a prompt, together\nwith the act of borrowing a book. It should then also find the article or\nsection that states the necessity of membership for borrowing a book. Using\nexpert-annotated data, we create a dataset of 282 of such pairs and their\nposition in the respective legal text. We then generate the model answers\nfor this task and let 6 domain experts and an LLM-as-a-judge give feedback\non the LLM’s answers. The feedback from the LLM-as-a-judge is acquired\nthrough API calls to GPT 4.1, where we provide the model with a system\nprompt that is highly similar to the instructions given to the human experts\nand ask it to evaluate the model answers using the same categories as the\nhuman annotators. For the human expert feedback, we get a moderate inter-\nannotator agreement (Fleiss Kappa) score of about 0.5, which shows that\nthe task is challenging to evaluate, even by experienced practitioners. Our\nresults show that the human feedback improves performance, underlining\nthe efficiency of RLHF. It outperforms the synthetic feedback, which shows\nthat for challenging NLP tasks, a human-in-the-loop approach is beneficial.\n\n\nReferences\n\n[1] Roos M Bakker et al. “Semantic role extraction in law texts: a comparative analysis\nof language models for legal information extraction”. In: Artificial Intelligence and\nLaw (2025), pp. 1–35.\n[2] Long Ouyang et al. “Training language models to follow instructions with human feed-\nback”. In: Advances in neural information processing systems 35 (2022), pp. 27730–\n27744."
            },
            {
              "id": 70,
              "title": "MolGeneration-R1: Reasoning LLMs Help with Molecule Generation?",
              "authors": "Congfeng Cao",
              "affiliation": "",
              "abstract": "The reasoning capabilities of Large Language Models (LLMs), often characterized by decomposing complex tasks via Chain-of-Thought (CoT), have demonstrated strong effectiveness in symbolic tasks. However, applying such reasoning techniques to molecule generation remains largely underexplored, and most LLMs still struggle with this challenge.\nIn this work, we propose \\textbf{MolGeneration-R1}, an effective framework for molecule generation that comprises two optimization steps:\n(1) constructing synthetic CoT data for molecule generation, followed by supervised fine-tuning (SFT) on a base model; and\n(2) applying reinforcement learning using the correctness of the generated molecule as the primary supervision signal to further enhance reasoning capabilities.\nWhen applied to Qwen-7B, MolGeneration-R1 significantly outperforms both strong open- and closed-source LLMs. Our MolGeneration-R1, trained on 4,061 data points, improves the exact match accuracy of the Qwen2.5-7B model on the molecule generation task from 4.92% to 38.77% on the synthetic test set. Furthermore, MolGeneration-R1 improves exact match accuracy on the real-world test set from 0.45% to 4.94%, even outperforming GPT-4o-mini---the model it was distilled from---which achieves 2.27%.\nExtensive ablation studies demonstrate that CoT reasoning is critical to performance gains; removing the CoT examples we produced during training leads to a significant drop in model effectiveness. To the best of our knowledge, MolGeneration-R1-7B is the first reasoning-based model for molecule generation. We release the model along with the accompanying code and CoT training data to the community."
            },
	      {
		  "id": 17,
		  "title": "Using Conversational Agents to Simulate Joint Decision-Making Data for Group Recommender Systems",
		  "authors": "Cedric Waterschoot",
		  "affiliation": "Maastricht University",
		  "abstract": "Current approaches to train and evaluate Group Recommender Systems (GRS) typically rely on individual user data, compiling synthetic groups through random assignment or similarity clustering. However, realistic, large-scale group data, i.e., individual members conversing and working towards consensus, remains non-existent. This gap is the result of practical challenges such as the difficulty of recruiting participants in natural groups (friends, strangers) and logistical complexity (hosting conversations, capturing reliable data).\n\nThis study presents preliminary findings from simulations of group conversations using large language model (LLM)-powered agents. We simulated conversations in which groups of two, four, six, or eight agents chose a restaurant to visit. Conversational agents were assigned a randomly generated personality profile based on the Five Factor Model and a preference history for ten anonymous restaurants (rated on a 0-10 scale). Following earlier work, we generated four types of group configurations: (i) uniform (all similar preferences), (ii) divergent (all different preferences), (iii) coalitional (two similar subgroups), and (iv) minority (all similar preferences except one member). Conversations were round-robin, meaning that each agent could contribute once each round.\n\nA total of 1,000 conversations were simulated across varying group sizes and configurations. Consensus was defined as a simple majority (50% + 1 vote count). We evaluated the group outcomes against several social choice-based aggregation strategies, common baselines in GRS research.\n\nOur preliminary results show that nearly one in five coalitional or divergent groups failed to reach a consensus within the 20-round conversation limit. However, roughly 90% of the groups that did reach a consensus arrived at a result matching one of the included aggregation strategies. As hypothesized, we found stark differences across group configurations, suggesting that recommendation strategies could be tailored to the internal dynamics of each group. We also found that outcomes varied based on both the number of conversational rounds required to reach consensus and the size of the group.\n\nThe framework and outcomes offer a preliminary step towards group decision-making data for GRS research and highlight the potential of using conversational agents to simulate group decision-making. Despite clear limitations, such as the occurrence of unnatural actions and randomly generated personality profiles, this approach provides promising insights into common GRS bottlenecks. In future work, we aim to include real participants in these groups and investigate their decision-making process and contributions in specific scenarios. These cases of interest include being the odd one out in the (potentially unfair) minority configuration or unpredictable, divergent groups. Finally, we will experiment with differing consensus thresholds and conversational (turn-taking) procedures."
	      }
          ]
        },
        {
          "id": 2,
          "name": "Speech Recognition & Sign Language",
          "description": "Speech, audio, and pronunciation analysis",
          "color": "#F59E0B",
          "posters": [
            {
              "id": 46,
              "title": "Graph Connectionist Temporal Classification for Phoneme Recognition",
              "authors": "Henry Grafé, Hugo Van hamme",
              "affiliation": "KU Leuven ESAT-PSI",
              "abstract": "Automatic Phoneme Recognition (APR) systems are often trained using pseudo phoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P) systems. These G2P systems frequently output multiple possible pronunciations per word, but the standard Connectionist Temporal Classification (CTC) loss cannot account for such ambiguity during training. In this work, we adapt Graph Temporal Classification (GTC) to the APR setting. GTC enables training from a graph of alternative phoneme sequences, allowing the model to consider multiple pronunciations per word as valid supervision. Our experiments on English and Dutch data sets show that incorporating multiple pronunciations per word into the training loss consistently improves phoneme error rates compared to a baseline trained with CTC. These results suggest that integrating pronunciation variation into the loss function is a promising strategy for training APR systems from noisy G2P-based supervision."
            },
            {
              "id": 54,
              "title": "Using Prompts with Errors to Improve Reading Mistake Detection in Children's Speech",
              "authors": "Chantal Banga, Lingyun Gao, Wieke Harmsen, Helmer Strik",
              "affiliation": "Radboud University",
              "abstract": "The Drie-Minuten-Toets (DMT) [1], or Three-Minute-Test, is a reading test used in Dutch primary schools to evaluate children’s decoding skills. During the test, students read aloud word lists within a time limit, while a teacher manually marks any reading mistakes. This scoring process is both time-consuming and subjective [2]. Automatic speech recognition (ASR) offers a promising way to support this scoring process by transcribing the child’s speech and detecting reading mistakes automatically.\nYet, recognizing children’s speech remains a challenge, due to limited training data [3], anatomical differences in vocal tract [4], and the wide variability in children’s speech patterns caused by differences in age, developmental stage, and individual characteristics [5]. Detecting reading mistakes is an even greater challenge, as most ASR systems are optimized to produce correct and fluent transcriptions, often ignoring reading mistakes [6], [7], [8].\nTo improve recognition accuracy in reading tasks, prompts can be provided to the ASR model to specify which words the child is expected to read [9]. While this approach improves word recognition, it may introduce a bias toward marking words as read correctly, even when words are misread. Gao et al. [10] addressed this limitation by adding artificial reading mistakes to the prompts. Building on this work, we will further explore how the use of prompts augmented with reading errors affects the detection of reading errors. We will experiment with different amounts and types of errors to determine which settings most effectively improve the model’s ability to detect whether a word was read correctly or not. Therefore, our research question is: \nHow do the type and amount of reading errors added to ASR prompts influence the system’s performance in detecting whether words are read correctly in the DMT?\nTo evaluate ASR performance, we will use a dataset in which 70 primary school children, who were all weak readers, read a DMT. The dataset consists of 240 one-minute recordings. Each child read three word cards of increasing difficulty. The first card contains 150 transparent, one-syllable words without consonant clusters (e.g., boom, dun). The second card also contains 150 transparent, one-syllable words, but with consonant clusters (e.g., klaar, griep). The third card consists of 150 transparent and non-transparent words with two to four syllables (e.g. neushoorn, rietje). The children had one minute per card to read as many words as possible. \nTo measure performance, we will align the ASR transcriptions with the prompts and use confusion matrices to compare the ASR models’ judgment of whether a word was read correctly with a human judgment. In addition, we will compute accuracy, precision, recall, F1-score, and Matthews Correlation Coefficient. We will discuss the results and highlight avenues for future directions."
            },
            {
              "id": 55,
              "title": "Leveraging Beam Search Information for Confidence Estimation in E2E ASR",
              "authors": "Yichen Jia, Hugo Van hamme",
              "affiliation": "KU Leuven",
              "abstract": "Reliable confidence scores are essential for building trustworthy speech transcription systems. They directly impact user experience by indicating potential transcription errors and enable downstream applications such as automatic error correction and semi-supervised training. To estimate confidence for end-to-end Automatic Speech Recognition (ASR) systems, recent research has adopted a Confidence Estimation Module (CEM), which incorporates selected features from the backbone ASR system. In this paper, we propose a lightweight CEM that leverages beam search information to generate confidence scores at token and word levels. Specifically, we compose the scores and ranks of each token in a hypothesis to form features for predicting confidence. This idea comes from the scoring method in a beam search and theoretically all the information about confidence should be reflected in the scores. Unlike existing methods tailored to specific ASR architectures, our approach is independent of the ASR model architecture and operates directly on scoring outputs from beam search. Experiments show that the proposed method achieves effective calibration at both token and word levels on in-domain and out-of-domain data. On the in-domain test set, SR-CEM attains a Maximum Calibration Error (MCE) of 4.50% and an Expected Calibration Error (ECE) of 0.30% at the token level, significantly outperforming softmax confidence, which yields 20.04% and 1.75%, respectively. At the word level, SR-CEM achieves 9.12% MCE and 0.35% ECE, compared to 17.91% and 1.67% from softmax confidence. The proposed method is possible to be used in any beam search based ASR system, as a small auxiliary module, independent to backbone ASR model architecture. Its low complexity and compact size make it suitable for deployment on edge devices and streaming applications."
            },
            {
              "id": 66,
              "title": "Explicit Pronunciation Modelling in LLM-based Speech Recognition",
              "authors": "Jakob Poncelet, Hugo Van hamme",
              "affiliation": "KU Leuven",
              "abstract": "Emerging work in the field of speech and language processing has been exploring the combination of speech models and Large Language Models (LLMs) to build speech-aware LLMs that can process both spoken and written inputs. Such systems aim to bridge the gap between speech recognition, spoken language understanding and textual reasoning by leveraging the rich contextual capabilities of LLMs alongside the acoustic and prosodic features captured by speech encoders. \n\nThe main challenge in this integration is aligning the acoustic representations produced by speech encoders with the semantic embeddings of language models. Effective alignment typically involves training an adapter layer to map the output features of a pre-trained speech encoder into the embedding space of the LLMs. This can be a tedious task, as speech outputs are typically related to pronunciation, whereas LLM embeddings are related to concepts and semantics.\n\nIn general, this mapping is learned through training with a speech-to-text task, where the LLM is given the projected speech features in a prompt and is tasked to transcribe these features to text. While this allows the LLM to learn to interpret the features, the performance is strongly limited by the availability of labeled speech examples.\nTo improve the alignment and the interpretation of the speech features in the LLM, we propose an approach where the LLM is trained to first predict the phonemes from the speech and then generate the final transcript. This intermediate step enhances the model’s knowledge of pronunciation and enables a finer-grained interpretation of acoustic features, thereby reducing the acoustic confusion during recognition of speech. Moreover, predicting phonemes prior to words allows to errors in the transcription to be traced back to the level of misrecognized or mispronounced phonemes, improving the explainability and interpretability of the model’s outputs.\n\nWe empirically validate our proposed method and demonstrate improved speech recognition on several datasets in both Dutch and English. Our method proves particularly effective for more low-resource settings, where it can learn pronunciation-to-word mappings with fewer labelled examples. Additionally, phonemic analysis shows that the produced transcripts more accurately preserve the acoustic characteristics of the original speech when phonemes are predicted first. These findings can open the door to more interpretable and data-efficient speech-aware language models, with promising applications across multilingual and low-resource settings."
            },
            {
              "id": 72,
              "title": "Correcting Dutch and Flemish ASR with Large Language Models: A Comparison of Text-Only and Speech-Aware Reasoning",
              "authors": "Pu Wang",
              "affiliation": "KU Leuven, Department of Electrical Engineering",
              "abstract": "Large-scale foundation models trained on multilingual speech, such as OpenAI’s Whisper, CMU’s OWSM, and NVIDIA’s Canary, have significantly advanced automatic speech recognition (ASR) by leveraging pretraining on vast and diverse corpora. These models demonstrate strong generalization and have established fine-tuning for downstream applications as the prevailing paradigm.  However, despite their broad coverage, the training data remains dominated by standard accents, adult speech, and mainstream lexical forms. As a result, their robustness degrades under domain shifts, particularly in settings involving regional dialects and phonetic variation.\n\nDutch and Flemish illustrate this limitation clearly. Although the two varieties share an orthography, they diverge systematically in pronunciation, prosody, and vocabulary. For instance, Flemish speakers use a soft, voiced fricative /ɣ/, in contrast to the hard /x/ preferred in northern Dutch. Identical phrases such as “Goed gedaan” thus produce acoustically distinct patterns, and models trained primarily on Dutch may misinterpret the Flemish variant, resulting in deletions or substitutions. Dutch also tends to centralize unstressed vowels and omit final consonants like /r/, whereas Flemish retains them. Words such as “Water” differ phonetically despite identical spelling. In informal Dutch, initial voiced stops like /b/ may be devoiced, so “Bergen” may be pronounced /ˈpɛrxən/ rather than the canonical Flemish /ˈbɛrɣən/. These differences lead to alignment errors and degraded recognition accuracy when using Dutch-centric models on Flemish input.\n\nOne potential remedy is to post-correct ASR outputs using large language models (LLMs). A text-only LLM can apply semantic reasoning to improve grammaticality and coherence, but it lacks access to acoustic cues necessary to resolve phonetically ambiguous or dialect-specific forms. Flemish utterances such as “Amai, da weet ik nie” differ both lexically and prosodically from “Nou, ik weet het niet”, and include discourse markers like “Amai” or sentence-final “hé”, as well as region-specific diminutives such as “pintje” versus the Dutch “biertje”. A text-only LLM may incorrectly \"correct\" these dialectally valid expressions into more familiar but erroneous alternatives.\n\nIn this work, we investigate LLM-based correction for Dutch and Flemish ASR, comparing text-only and speech-aware approaches. Corrections are applied to N-best hypotheses generated by speech foundation models, Whisper and OWSM. In the text-only setting, the LLM operates solely on candidate transcriptions. In the multimodal setting, we adopt SpeechGPT, a cross-modal LLM that additionally receives discretized speech tokens, allowing it to ground its predictions in acoustic evidence. This speech-aware reasoning enables the model to resolve subtle phonetic ambiguities and maintain dialectal fidelity. Experiments are conducted on the Spoken Dutch Corpus (CGN), comprising approximately 900 hours of speech, including 270 hours from Flemish speakers."
            },
            {
              "id": 74,
              "title": "Adapting ASR to Dialectal Variation Using Interpretable Metadata-Gated Low-Rank Adaptation",
              "authors": "Pouya Mehralian, Jakob Poncelet, Hugo Van hamme",
              "affiliation": "KU Leuven",
              "abstract": "Automatic Speech Recognition (ASR) has made tremendous strides in recent years, largely thanks to the adoption of large pre-trained models. However, recognition performance remains poor for dialectal speech, particularly in linguistically diverse areas such as Flanders and the southern Netherlands. Dialects often differ in pronunciation, vocabulary, and syntax, and many are under-resourced or lack standardized orthographies. Conventional approaches to handling dialectal variation—such as fine-tuning separate models per dialect or training large joint models—are either computationally expensive or struggle to generalize across the dialect continuum.\n\nWe present GLoRIA (Gated Low-Rank Interpretable Adaptation), a novel, parameter-efficient method that allows a single ASR model to adapt dynamically to dialectal variation using metadata—in this case, geographic coordinates. GLoRIA builds on low-rank adaptation (LoRA), a technique in which small, trainable low-rank matrices are inserted into a frozen pre-trained model. We extend this with a metadata-aware gating mechanism: for each utterance, a small neural network takes the speaker’s location as input and determines how much each LoRA rank-1 component contributes to the model's behavior. These gated low-rank updates are injected into each feed-forward layer of the encoder, making the adaptation interpretable, additive, and continuously conditioned on geography.\n\nUnlike previous dialect-aware models that rely on discrete region labels or dialect classification, GLoRIA operates in a continuous input space. This enables smooth interpolation between dialects, captures gradual language change, and avoids hard regional boundaries. It also enables generalization to previously unseen varieties by leveraging nearby training examples in the geographic space.\nWe evaluate GLoRIA on the GCND corpus, a collection of dialectal Dutch speech recorded across Belgium, the southern Netherlands, and northern France, spanning over nine dialect regions. The corpus contains pure dialect, Not merely accented standard Dutch: speakers were selected for strong local ties and limited mobility, and the recordings capture natural, conversational dialect use. Each utterance is annotated with precise geographic coordinates based on speaker location. While contemporary mobility raises questions about geographic relevance, sociolinguistic research shows that dialect features often persist even after migration, particularly in phonetics and intonation. These residual geographic cues remain valuable for ASR adaptation and can serve as effective proxies for linguistic variation.\n\nOur experiments compare GLoRIA to multiple baselines: (1) dialect-specific and joint full fine-tuning, (2) standard LoRA, and (3) geo-conditioned models that inject coordinates into model input or hidden layers. GLoRIA outperforms all baselines, while updating less than 10% of model parameters. Importantly, it generalizes well to dialects not encountered during training, even when extrapolating to locations outside the geographical range of the training set.\n\nBeyond performance, GLoRIA enables qualitative insights into dialectal structure. We apply non-negative matrix factorization to the adaptation weights and visualize the activation of different adaptation components across the map. The resulting patterns correspond closely to known dialect regions, revealing that the model has implicitly learned meaningful linguistic boundaries. This opens the door to interpretable, location-aware ASR models that both improve recognition performance and reflect linguistic structure, while being computationally efficient."
            },
            {
              "id": 67,
              "title": "Distinguishing Schizophrenia-Spectrum Disorders from Depression and Control Groups Using Multimodal Speech Analysis",
              "authors": "Araya Kiros Hailemariam, Silvia Ciampelli, Defne Abur, Tommaso Caselli, Janna de Boer, Sanne Schuite-Koops, Jan-Bernard Marsman, Iris Sommer",
              "affiliation": "University of Groningen & University Medical Center Groningen",
              "abstract": "Distinguishing between schizophrenia and depression remains a significant challenge in psychiatry due to overlapping symptoms and the absence of definitive biological markers. To address this diagnostic uncertainty, we explore the use of multimodal speech analysis as a scalable, non-invasive tool to examine how differences in speech patterns and linguistic content may help differentiate these mental health conditions, thereby providing clinicians with additional insights to support their diagnostic assessments.\nIn this study, we developed a deep learning framework that integrates both acoustic and textual information from interview recordings. The audio is first transcribed using OpenAI's Whisper for automatic speech recognition, followed by speaker diarization using WhisperX. Speaker roles (interviewer vs. participant) are assigned based on question frequency, and segments are grouped into interviewer-participant sequences for downstream modeling. We extract high-dimensional embeddings using HuBERT for the audio modality and XLM-EMO  for the transcribed text. These embeddings are processed through a Bidirectional LSTM to model temporal dependencies, with a cross-modal attention mechanism fusing the audio and text representations into a unified multimodal embedding.\nOur study uses 422 interview samples in Dutch (schizophrenia = 220, Control = 142, Depression = 60) across three binary classification tasks: Depression vs. Schizophrenia, Control vs. Schizophrenia, and Control vs. Depression. Employing a 70:15:15 stratified splitting ratio with downsampling to address class imbalance, our multimodal fusion model demonstrates superior performance compared to unimodal baselines. The model achieves a macro F1 score of 0.97 for both Depression vs. Schizophrenia (Precision = 0.94, Recall = 0.97) and Control vs. Schizophrenia (Precision = 0.97, Recall = 0.97), with a lower but still significant macro F1-score of 0.74 for Control vs. Depression (Precision = 1.00, Recall = 0.74).\nNotably, the audio-only model (HuBERT + BiLSTM) consistently outperforms the text-only approach (XLM-EMO + BiLSTM) across all classification tasks, achieving F1-scores of 0.94, 0.93, and 0.67 compared to 0.91, 0.90, and 0.64, respectively. The relative difficulty of the Control vs. Depression classification (compared to schizophrenia-related tasks) indicates greater overlap in speech patterns between the depression and control groups, highlighting the need for more nuanced feature extraction in future work.\nWe further examined the correlations between prediction accuracy and demographic factors. For Control vs. Schizophrenia, prediction accuracy shows a moderate negative correlation with participant age (r = -0.28) and a weak negative correlation with education (r = -0.04), while gender has no effect. In Depression vs. Schizophrenia, age again shows a moderate negative correlation (r = -0.26), education shows a weak positive correlation (r = +0.12), and gender has minimal effect (r = +0.05). For Control vs. Depression, education shows a moderate negative correlation (r = -0.26), gender shows a moderate negative correlation (r = -0.33), and age shows a weak negative correlation (r = -0.10).\nThese results offer clear evidence that using information from both speech acoustics and linguistic content may offer additional insight into differences in these two conditions. Furthermore, the observed demographic correlations emphasize the need to consider individual variability in age, education, and gender when developing clinical AI tools to ensure equitable performance across diverse populations."
            },
	      {
		  "id": 38,
		  "title": "Improved Mispronunciation Detection and Diagnosis via Large Language Models",
		  "authors": "Xing Wei, Catia Cucchiarini, Roeland van Hout, Helmer Strik",
		  "affiliation": "Radboud University",
		  "abstract": "Mispronunciation detection and diagnosis (MDD) plays a critical role in Computer-Assisted Language Learning (CALL), yet traditional systems, particularly those based on supervised Automatic Speech Recognition (ASR), face several longstanding limitations. Chief among these is data sparsity: high-quality labeled non-native speech is expensive and time-consuming to collect, and varies widely across different first language (L1) backgrounds and proficiency levels. As a result, models trained on limited non-native data often fail to generalize, especially when confronted with rare or deviant pronunciation errors. Moreover, many such systems rely on acoustic features like MFCCs or derivative metrics such as Goodness of Pronunciation (GOP), which often fail to capture the rich temporal and articulatory nuances of second language (L2) speech. Their outputs typically consist of discrete labels or scores, offering limited explanatory power and pedagogical value.\n \nTo overcome these challenges, we turn to pretrained speech models such as Wav2Vec 2.0 and Whisper. Trained on massive amounts of unlabeled audio, these models can learn context-aware representations directly from raw waveforms. In our preliminary experiments, we have successfully integrated speech embeddings extracted from a fine-tuned Wav2Vec 2.0 encoder with acoustic-phonetic features, yielding promising improvements in MDD performance. While this method enhances mispronunciation detection accuracy, it still terminates in a discriminative decoder, thus underutilizing the explanatory potential embedded within these rich representations.\n \nOn the other hand, Large Language Models (LLMs) have achieved remarkable success across a wide range of generative tasks in vision, speech, and multimodal domains. Their ability to interpret complex inputs and produce coherent, contextually appropriate output makes them promising candidates for producing interpretable, learner-facing feedback. Building on our initial findings, this work introduces a new framework that connects an LLM to these validated speech representations. Through carefully constructed prompts, the LLM is guided to generate phonetic transcriptions or textual descriptions of the L2 speech.\n \nOur central aim is to reframe MDD from a classification task into a flexible generative diagnosis task. The diagnostic capability is realized by aligning the canonical target with the textual output from LLM, allowing the system not only to detect errors but also to describe the phonetic nature (e.g., the /a/ in ‘naam’ was mispronounced as /ɑ/ in Dutch). By unifying recent advances in self-supervised speech and generative language modeling, our ultimate goal is to enable improved CALL tools that are accurate, more interactive, and pedagogically insightful."
              },
	      {
		  "id": 65,
		  "title": "Signs on Paper: exploring multimodal foundation models for classification of still images of signs in print dictionaries",
		  "authors": "Nargess Asghari, Victoria Nyst, Peter van der Putten",
		  "affiliation": "Leiden University",
		  "abstract": "Sign language (SL) and gesture recognition technology is predominantly developed using video datasets. Since videos consist of sequences of frames over time, they can capture the temporal and kinematic characteristics of signs and gestures. However, a substantial amount of SL data, including but not limited to historical SL data, exists in the form of still images in printed SL dictionaries. In this poster we present our work in progress in exploring the extent to which the state-of-the-art multi-modal foundation models can be used for automatic recognition and classification of lexical signs from print SL dictionaries.\n\nOur work builds on Fragkiadakis et al (2021) and Fragkiadakis’ (2024) work on sign recognition in SL video corpora and extends it to the static image domain. While there is some research on hand pose recognition in still images in arts, such as in paintings or mediaeval images (Bernasconi et al, 2023; Jenicek & Chum, 2019; Schlecht et al, 2011), there is no such research on SL recognition. The available still image datasets for SLs are mainly of hand-alphabet images in a particular sign language, which generally only include static letters and exclude letters with movements such as J or Z in ASL, or images of numbers (cf. Garg et al, 2025; Kouvakis et al, 2024; Rajan & Rajendran, 2021). \n\nThere is no linguistic research on automatic recognition and comparison of lexical signs from still images. Our study aims to bridge this gap in sign language linguistics and automatic recognition technology. The resulting dataset constitutes a novel contribution and the first of its kind with still images of lexical signs from a variety of world SLs. \n\nThere is considerable variation in how signs are represented in paper SL dictionaries. Signs may be hand-drawn (cf. Pélissier, 1856) or photographed (cf. Martini & Morgado, 2008). There is no standardised way to represent signs on paper – for example, which frame of the signing sequence (which ‘state’ of the sign) is shown or what the body orientation relative to the viewer is. In order to show the temporal aspect of signs, such as order of signing, two or more frames next to each other may be shown, or the hands at different states may be superimposed (overlaid) on the same frame. Arrows and symbols are often used to indicate motion properties, such as repetition or direction and path of movement.\n\nExtensive variability in sign representation poses a challenge to the task of automatic recognition and classification. We use multimodal foundation models to classify a diverse set of sign images from a variety of print SL dictionaries. The results of this exploratory project can guide us in establishing a baseline in a methodological way. Such a baseline allows us to evaluate how effectively different models perform without additional fine-tuning for extracting features such as number of hands, handshape, location and so on, across a diverse set of input data.\n--------------------------\nReferences\nBernasconi, V., Cetinić, E., & Impett, L. (2023). A Computational Approach to Hand Pose Recognition in Early Modern Paintings. Journal of Imaging, 9(6), 120. https://doi.org/10.3390/jimaging9060120.\nFragkiadakis, M. (2024). Digital Tools for Sign Language Research: Towards Recognition and Comparison of Lexical Signs. Netherlands Graduate School of Linguistics. DOI: https://dx.medra.org/10.48273/LOT0666.\nFragkiadakis, M., Nyst, V., & van der Putten, P. (2021). Towards a User-Friendly Tool for Automated Sign Annotation: Identification and Annotation of Time Slots, Number of Hands, and Handshape. DHQ: Digital Humanities Quarterly, 15(1). https://hdl.handle.net/1887/3158649.\nGarg, B., Kasar, M., Paygude, P., Dhumane, A., Ambala, S., Rajpurohit, J., ... & Kashyap, A. (2025). Sign Language Detection Dataset: A Resource for AI-Based Recognition Systems. Data in Brief. https://doi.org/10.1016/j.dib.2025.111703.\nJenicek, T., & Chum, O. (2019). Linking art through human poses. In 2019 International Conference on Document Analysis and Recognition (ICDAR) (pp. 1338-1345). IEEE. https://doi.org/10.1109/ICDAR.2019.00216.\nKouvakis, V., Trevlakis, S. E., & Boulogeorgos, A. A. A. (2024). Semantic communications for image-based sign language transmission. IEEE Open Journal of the Communications Society, 5, 1088-1100. https://doi.org/10.1109/OJCOMS.2024.3360191. \nMartini, M., & Morgado, M. (2008). Dicionário Escolar de Língua Gestual Guineense. Lisboa: Surd’Universo. https://www.researchgate.net/publication/341494844_Dicionario_escolar_de_Lingua_Gestual_Guineense\nPélissier, P. (1856). Iconographie des signes: faisant partie de l'enseignement primaire des sourds-muets. Paris: Imprimerie et libraire de Paul Dupont. https://gallica.bnf.fr/ark:/12148/bpt6k131991f.texteImage.\nRajan, R. G., & Rajendran, P. S. (2021). Gesture recognition of RGB-D and RGB static images using ensemble-based CNN architecture. In 2021 5th international conference on intelligent computing and control systems (ICICCS) (pp. 1579-1584). IEEE. https://doi.org/10.1109/ICICCS51141.2021.9432163.\nSchlecht, J., Carqué, B., & Ommer, B. (2011). Detecting gestures in medieval images. In 2011 18th IEEE International Conference on Image Processing (pp. 1285-1288). IEEE. https://doi.org/10.1109/ICIP.2011.6115669."
              },
	      {
		  "id": 41,
		  "title": "A Computational Construction Grammar Framework for Modelling Signed Languages",
		  "authors": "Liesbet De Vos",
		  "affiliation": "Université de Namur",
		  "abstract": "Constructional approaches to signed languages are becoming increasingly popular within sign language linguistics (see Wilcox & Martínez, 2025).  Current approaches, however, focus primarily on theoretical description, while formalization and computational implementation remain largely unexplored. This work provides an initial step towards addressing this gap by studying and operationalizing the core mechanisms required for representing and processing manual signed forms using computational construction grammar. These include a phonetic representation of individual manual signs and a formal representation of the complex temporal synchronization patterns between them. \nTo represent the phonetic structure of individual signs, we use the Hamburg Notation System (HamNoSys – Hanke, 2004). HamNoSys is well integrated with modern computer software, having a Unicode font and XML-based representation known as Signing Gesture Markup Language (SiGML – Elliott et al., 2004). Through SiGML, HamNoSys strings can be converted into avatar animations. \n\nTo describe the temporal relationships between manual signs, we first use the ELAN annotation software (Crasborn & Sloetjes, 2008) to align the two manual articulators to a single time-track, derived from the video-recording of the expression. Afterward, the annotation file is used to extract information about the annotated phonetic form of the sign, as well as the temporal relationships between different signs. To encode these temporal relationships, we use a set of temporal alignment predicates inspired by Allen’s interval algebra.\n\nThe implemented mechanisms are integrated into Fluid Construction Grammar (FCG –  Steels, 2011; Beuls & Van Eecke, 2023; van Trijp et al. 2022) and are available as a module within the Babel software library (see https://emergent-languages.org). \n\nTo demonstrate the potential of the developed framework for the computational exploration of sign language constructions, we provide an interactive web demonstration alongside our work: https://anonymous.4open.science/w/SL-processing-demo-1D62/. It illustrates the functionality of our framework through the comprehension and production of an expression in French Belgian Sign Language (LSFB). The broader aim of this demonstration and our work is to showcase the potential of FCG and the implemented sign language processing mechanisms for future computational exploration of sign language constructions.\n\nReferences\n\nBeuls, K., & Van Eecke, P. (2023). Fluid Construction Grammar: State of the art and future outlook. In GURT 2014 (Georgetown University Round Table on Languages and Linguistics) (pp. 41-50).\n\nCrasborn, O., & Sloetjes, H. (2008). Enhanced ELAN functionality for sign language corpora. In 6th International Conference on Language Resources and Evaluation (LREC 2008)/3rd Workshop on the Representation and Processing of Sign Languages: Construction and Exploitation of Sign Language Corpora (pp. 39-43).\n\nElliott, R., Glauert, J., Jennings, V., & Kennaway, R. (2004). An overview of the SiGML notation and SiGMLSigning software system. sign-lang@ LREC 2004, 98-104.\n\nHanke, T. (2004). HamNoSys–representing sign language data in language resources and language processing contexts. In sign-lang@ LREC 2004 (pp. 1-6). European Language Resources Association (ELRA).\n\nSteels, L. (2011). Introducing fluid construction grammar. In Design patterns in fluid construction grammar (pp. 3-30). John Benjamins Publishing Company.\n\nWilcox S, Martínez (2025) R. Constructional Approaches to Signed Language. In: Fried M, Nikiforidou K, eds. The Cambridge Handbook of Construction Grammar. Cambridge Handbooks in Language and Linguistics. Cambridge University Press; 2025:405-436.\n\nvan Trijp, R., Beuls, K., & Van Eecke, P. (2022). The FCG Editor: An innovative environment for engineering computational construction grammars. PLoS One, 17(6), e0269708."
              }
          ]
        },
        {
          "id": 3,
          "name": "Healthcare, Medical & Societal Applications",
          "description": "All health, medical, and clinical applications",
          "color": "#10B981",
          "posters": [
            {
              "id": 9,
              "title": "Unsupervised Detection of Emerging Disease Signals in Dutch Clinical Texts Using Transformer Embeddings",
              "authors": "Gijs Danoe, Maarten Homburg, Marjolein Y. Berger, Tim olde Hartman, Jean Muris, Andreas Voss, Axel Hamprecht, Maarten Brilman, Lilian Peters, Matthijs Berends",
              "affiliation": "University of Groningen & others",
              "abstract": "Early detection of emerging infectious diseases is essential for timely public health interventions. Traditional surveillance systems rely on structured data and confirmed diagnoses, limiting their ability to detect early, non-specific signals of novel outbreaks. This study introduces Early Recognition using Neural Information Encoding (ERNIE), a disease-agnostic natural language processing (NLP) framework designed to identify and characterise atypical disease clusters without predefined diagnostic labels. \n\nERNIE was developed by integrating BERT-based language modelling with autoencoders for anomaly detection, HDBSCAN for clustering, and TF-IDF for key term extraction. The model was trained on primary care consultation data (2015–2023) from the northern Netherlands and validated using external datasets from the eastern and southern regions of the country. Performance was evaluated in relation to known outbreaks (COVID-19, RSV) and a simulated West Nile Virus (WNV) outbreak. Specificity was assessed by analysing a control period (2022). \n\nERNIE identified a potential early COVID-19 cluster in Dutch primary care data before the first confirmed case in 2020. During the 2021 RSV summer peak, the model detected RSV-indicating clusters of paediatric patients. No comparable clusters were found during the same time period in 2022. External validation using datasets not included in training confirmed the detection of anomalous RSV-indicating clusters in previously unseen datasets. The model also flagged WNV-simulated cases, demonstrating its ability to detect previously unobserved symptom patterns. \n\nThis study establishes a new benchmark in digital infectious disease surveillance in primary care by introducing an innovative NLP framework. ERNIE enables the early, interpretable, and disease-agnostic detection of emerging health threats, without reliance on predefined diagnostic labels or laboratory data. This approach redefines how subtle symptom patterns can be leveraged for proactive public health surveillance. Beyond outbreak detection, it lays the foundation for a new generation of AI-driven surveillance systems that can support continuous, scalable, and data-driven public health strategies."
            },
            {
              "id": 14,
              "title": "Predicting palliative phase from textual reports in electronic health records",
              "authors": "Maya Sappelli, Iris Heerlien, Rick ten Tije, Maartje Huveneers, Evi Swinkels",
              "affiliation": "HAN University, Saxion, Ecare",
              "abstract": "The workload in the healthcare sector is increasing, addressing the need for innovative solutions. One solution is AI to assist in clinical decision-making by extracting information from patient’s records. In the TELEMAP use case we investigate the applicability of AI techniques and more specifically text mining to predict the start of the palliative phase in patients with chronic obstructive pulmonary disease (COPD) or heart failure. \n\nFirst, 5 dimensions of interest were established that are important in indicating the palliative phase (i.e. the physical, social, psychological, spiritual and intensifying care dimensions). Via literature research, interviews and focus groups with health care professionals who specialize in palliative care, lexicons were created per dimension containing keywords such as ‘pain’, ‘lonely’, ‘anger’, ‘isolation, ‘fear’ to describe the dimension. The word lists were further expanded using a word2vec model trained on electronic health data and the resulting similar terms were curated by the health care professionals.\n\nThe dataset consisted of electronic health data from patients with COPD and heart failure. The prediction target was the start of the palliative phase which was set at 1 year before death. Data was modelled by identifying the presence of a dimension based on presence of target words from the lexicons in the medical reports. Each dimension was weighted based on the impact of a keyword for a dimension. To prevent high impact of keywords that are present in almost any report (such as 'pain'), inverse document frequency (idf) was calculated for each keyword in each dimension. When multiple keywords were present for a dimension, the average idf-score was taken for that report. Overall, a sliding window of 30 days was applied which gave a dynamically changing weighted average score for each of the dimensions of interest. In addition, indicators of medical events, gender, age and focus areas in care (such as dementia, or tube feeding) of the patient were included in the representation. Note that the number of reports per data point can vary. \n\nA decision tree classifier was trained and evaluated using 5-fold cross validation. Various tree-based algorithms were compared and the best performance was achieved with a RandomForest classifier (precision 0.69, recall 0.89). We optimize for recall, as the model is intended for signalling, not decision making.  The model was incorporated in an application with explainable components. The application was used in a field study with two care companies. The explainable component sometimes led to confusing results such as the absence of \"support stocking\" as care focus area being the reason for a positive prediction. In a second iteration of the model the effect of the care focus areas was investigated. Removing all focus areas lead to a decrease in recall (0.83) but had little effect on precision (0.69) leading to believe that knowledge about the care focus areas is relevant. Future research will include a further investigation of effectiveness of the model in the wild."
            },
            {
              "id": 19,
              "title": "Developing A LLM & RAG-based System for User Question-Answering on Real-World Health Diary Data",
              "authors": "Yee Man Ng, Bram van Dijk, Joris Jansen, Otto Boekesteijn, Pieter Beynen, Marco Spruit",
              "affiliation": "Leiden University & Healthy Chronos",
              "abstract": "Cancer patients and survivors often suffer from long-term symptoms after recovery, such as cancer-related fatigue and depressive symptoms. Adjusting their lifestyle after recovery to manage these symptoms can be challenging, as lifestyle adjustments and symptom experiences as a result of lifestyle adjustments are subjective. In addition, the internet provides an abundance of information, including misinformation, making it difficult for patients to determine what health & lifestyle advice is useful to their specific circumstances. \n\nQuestion-answering systems driven by Large Language Models (LLMs) provide the opportunity to strongly personalize an answer to a given question, as well as consider relevant trustworthy information through Retrieval-Augmented Generation (RAG). \n\nOur study presents the bottom-up development of a pipeline to automatically answer lifestyle-related questions by cancer survivors. We draw upon real-world data from users of a daily diary app targeting cancer survivors, with self-reported daily diary entries containing information regarding their symptoms, activities, and achieved goals. We also collect a Dutch dataset with real questions asked by users, such as: “Hoe slaap ik beter” (How do I sleep better) and “Wat is mijn grootste energieslurper” (Which activity is my biggest energy drainer?). We use LLMs to extract relevant insights from a table containing the user’s diary data and generate relevant, correct, and humanly desirable answers. \n\nTo achieve this, we develop a pipeline consisting of various steps. We first categorize a given question from a user based on themes commonly found in previous questions. We then prompt GPT-4 to provide an answer to the user’s question, using a prompt that includes a representation of a user’s diary data as a table in markdown format and information about the schema and meaning of certain values in the table. In addition, we implement RAG with a database containing sources from kanker.nl, a Dutch platform containing blog posts with health & lifestyle advice about life after cancer, written by medical experts. Through manual evaluation by the users, developers, and founders of the app, we assess the preferences of different groups in the output of the LLM-based system. \n\nWe establish an evaluation framework for our use-case that evaluates on various dimensions, such as correctness, tone, and informativeness, and evaluate our system based on real user experiences of the system. Our research question is as follows: What does a state-of-the-art LLM-RAG pipeline for addressing real user questions look like and how is its quality evaluated by users and developers? We reflect on our preliminary findings obtained with a small sample."
            },
	      {
		  "id": 23,
		  "title": "🧗DutCH-CRAG: Dutch Climate and Health misinformation Correction with Retrieval Augment Generation",
		  "authors": "Rosalien Kinds, Thijmen W. Adam, Tommaso Caselli",
		  "affiliation": "CLCG, University of Groningen",
		  "abstract": "Correcting misinformation is a non-trivial task due to the complex interplay of individual cognitive dispositions, the inherently sense-making nature of misinformation, and the persistence of the continued influence effect. Simply retracting the incorrect information is ineffective and can even be counterproductive. Although some correction strategies have shown their effectiveness, practical methods for integrating them into Large Language Models (LLMs) remain underdeveloped. This work explores the use of Retrieval-Augmented Generation (RAG) combined with instruction-tuned LLMs to implement one such effective correction strategy: providing simple and accessible alternative explanations. We tested our approach to two case studies in Dutch comparing one language specific LLM (GEITje-7B-Ultra) and a family of open-weight models enhanced for reasoning namely, Phi-4. Although we have implemented different solutions for the retrieval component, the selected LLMs will allow us to compare the role of language specific functionalities and model's size. The first case study addresses misinformation related to climate change. A corpus of 650 Dutch-language debunked claims was constructed using data from fact-checking platforms attached to BenEDMO (including Knack, DPA-factchecking and AFP) and reputable media such as NU.nl, NRC and de Volkskrant. Each claim is paired with its corresponding correction text including any additional claims that can be refuted using the same explanation. Relevant evidence for each claim is retrieved using a hybrid pipeline combining BM25 and SentenceTransformers. Retrieved passages are reranked for semantic relevance, and the top 3 relevant passages are used to prompt the LLM to generate a correction aligned with the selected strategy. Non-expert evaluators reviewed three corrections per model and rated them based on language quality, perceived factual accuracy, text length, and persuasiveness. Additionally, evaluators reported their belief in the misinformation claims both before and after reading the corrections to measure any belief change. Results indicate that Phi-4 performed best in terms of language quality and factual correctness, while GEITje-7B-Ultra was perceived as the most persuasive. A full analysis of the evaluation data is currently in progress. The second case study targets health misinformation, particularly claims related to vaccines and infectious diseases. A reliable Dutch health information portal, thuisarts.nl, was used to construct a corpus of medically accurate and reader-friendly texts. These documents were segmented and embedded using the multilingual BAAI/bge-m3 model, which has demonstrated strong performance on multilingual and cross-lingual retrieval benchmarks. To simulate misinformation scenarios, 102 false or misleading health-related claims were extracted from the PubHealth dataset, an English-language resource designed for health misinformation detection [4]. Each PubHealth claim was also embedded using BAAI/bge-m3 to facilitate cross-lingual retrieval from the Dutch corpus. A cross-encoder reranks the top 10 retrieved documents, with the top three being selected as input for generation. The same correction methodology as in the climate case study was applied. As with the climate case, evaluators assess the generated corrections for language quality, factual alignment, and overall persuasiveness. The evaluation of this task is ongoing."
	      },
	      {
		  "id": 43,
		  "title": "Analyzing Cancer Patients' Experiences with Embedding-based Topic Modelling",
		  "authors": "Teodor-Călin Ionescu, Jan Heijdra Suasnabar, Lifeng Han, Suzan Verberne, Anne Stiggelbout (on behalf of 4D PICTURE)",
		  "affiliation": "LIACS, Leiden University; LUMC, Leiden University; LIACS and LUMC",
		  "abstract": "Background:\nShared decision-making (SDM) (Stiggelbout et al. 2015) is a practice in healthcare aiming at better communication between patients and healthcare professionals during the patient pathway, for informed treatment, shared responsibility, more transparent decision making, and better healthcare outcomes. To achieve this, it is important to understand the patient's experiences in more detail for better clinical support. Traditional strategies include both quantitative and qualitative analysis on patient-related data, such as questionnaires and surveys.\n\nMethod and Data:\nIn this work, we analyse patient storytelling data collected in the Metro Mapping Project (Stiggelbout et al. 2023) in an oncology setting. This data set is a collection of interview transcriptions from people involved in three roles: the patient, the interviewer, and the loved one (a friend or family member of the patient).\nWe cast the research question as a topic modelling task, to identify the key concerns during the cancer patient journey. We evaluate BERT-topic and Topic2Vec on this task.\nOur goal is to uncover meaningful themes from cancer patient data and offer insights on how the patient-oriented healthcare practice can be enhanced via AI for social good.\nTo ensure a fair model comparison, BERT-topic and Top2Vec were deployed using similar preprocessing, chunking, and clustering configurations.\nThe three domain specific LMs we investigated include BioClinicalBERT, ClinicalBERT, and MSR BiomedBERT (aka PubMedBERT).\nWe conducted a human evaluation of model outputs using a survey questionnaire to assess topics, scale, coherence, fidelity, usefulness, missing topics, and model feedback.\n\nResults:\nThe results showed that current neural topic modeling techniques can extract a variety of relevant themes from patient interviews.\nThese include emotional experiences, treatment details, personal struggles, and reflections on the treatment processes.\nWhile both techniques produced fairly coherent and easily interpretable topics, BERTopic, especially when paired with an embedding model pre-trained on clinical data, such as BioMedicalBERT, and a sentence-based chunking strategy, delivered more refined and focused results, which better represented the patients' experiences and concerns expressed during the interviews.\nThis makes BERTopic more promising for practical use in clinical environments, as opposed to Top2Vec, given the specific experimental setup and dataset.\n\nConclusions\nThe extracted topics suggest several ways in which topic modeling could support and improve healthcare processes. Most importantly, they could help clinicians identify and understand key moments in a patient's narrative without having to read every transcript manually.\nThis could save time, reduce the workload of clinical staff, and give more visibility to the patient's voice, especially in cases where emotional or psychological concerns might otherwise be overlooked.\n\nImpact:\nAlthough not empirically tested in this study, this type of automated workflow for analysing patient documents could also potentially help reduce the risk of overlooking important details in a patient's history.\nBy eliminating the need for clinicians to read through lengthy transcripts manually, the system may lessen the chance of missing key information due to time constraints or fatigue. Although this study did not involve a clinical trial or professional assistance, the structure of the output, combined with feedback from the conducted small-scale human evaluation, shows clear potential for integrating topic modeling into tools that support patient-centered care."
	      },
            {
              "id": 69,
              "title": "An Agentic AI Framework for Training General Practitioner Student Skills",
              "authors": "Victor De Marez, Luna De Bruyne, Walter Daelemans",
              "affiliation": "CLiPS, University of Antwerp",
              "abstract": "Advancements in large language models offer great potential for enhancing virtual simulated patients (VSPs) in medical education, providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent role playing, scenario generation for VSP use, and providing structured educational feedback. This limits their effectiveness in training clinical reasoning and patient-centered communication skills.\n\nWe present an agentic AI framework for training general practitioner student skills, addressing these challenges. Our system integrates three specialized agents: (1) a generator agent producing customizable, evidence-based medical scenarios with adjustable personality traits based on the Big Five model; (2) a conversational VSP agent delivering accurate, persona-driven dialogue through multi-step reasoning and retrieval-augmented generation; and (3) a critic agent providing automated, standards-based feedback on communication skills and clinical reasoning.\n\nAn evaluation with 13 medical students on two generated cases demonstrates the framework’s effectiveness, confirming medical accuracy, realistic patient answers, engaging dialogue, and valuable instructional feedback. Students reported a significantly increased readiness for Objective Structured Clinical Examination (OSCE) preparation by using VSPs (p = 0.007). Key advantages identified included scalability, independent accessibility, and extensive scenario variety. \n\nThis integrated framework presents a robust approach for developing more dependable and pedagogically valuable VSP training tools. Future work will enhance realism, reduce response latency, incorporate non-verbal communication and physical examination capabilities, and enrich personality representation."
            },
            {
              "id": 71,
              "title": "Reducing Language Barriers in the Dutch Skill-Based Labour Market",
              "authors": "Tom Brand",
              "affiliation": "TNO",
              "abstract": "A skills-based approach is increasingly used to compare and match opportunities in the labour market, as in international classification systems such as ESCO [1] and O*NET [2]. However, many of these descriptions are written in complex language, unintentionally creating barriers for users whose reading abilities do not match this register. This is unfair; for example, someone switching careers should not face descriptions requiring linguistic capabilities irrelevant to the job. With 2.5 million Dutch people being weakly literate [3], the harm of complex language is considerable. Such linguistic barriers perpetuate inequality in a system intended to do the opposite.\n\nTo illustrate, consider this real skill description:\n\"Systematische onderzoeksmethoden toepassen en met de betrokken partijen communiceren om specifieke informatie te vinden en onderzoeksresultaten te evalueren ter beoordeling van de relevantie van de informatie, de desbetreffende technische systemen en de ontwikkelingen.\"\nOur tool flags this sentence as highly complex. A possible revision is:\n\"Methodes toepassen en overleggen met wie een rol spelen om informatie te vinden en de resultaten te controleren. Zo kun je bepalen of de informatie belangrijk is, of de systemen goed werken en wat de ontwikkelingen zijn.\"\nThe description is much more readable, and the flags largely disappear.\n\nTo tackle this problem, we developed a robust tool that assesses and flags linguistic complexity in Dutch skill descriptions. No single metric captures language complexity completely, so we selected a suite of established readability and complexity metrics known to be applicable to Dutch. These include the Flesch-Douma index (Douma, 1960), Dale-Chall readability formula (Dale & Chall, 1948), the Dutch Leesbaarheidsindex A (Staphorsius, 1994), and Cito’s CLIB (Staphorsius & Krom, 1985), among others. Each metric captures different aspects of readability or complexity, and our approach is fully explainable and uses no \"black box\" methods. The software provides domain experts and content authors with intelligible, multi-metric feedback on when a skill description may be unnecessarily complex.\n\nThresholds for each metric are set empirically to indicate when a passage might be too complex. For metrics exceeding their cut-offs, the tool flags potential issues and encourages the author to simplify where possible. Some skill descriptions may require precise or technical language.\nOur tool supports the transformation of skill and job descriptions through the framework of CompetentNL, reducing language bias and opening opportunities for a wider audience. By integrating human domain expertise with targeted NLP methods, we aim to make the skill-based labour market more navigable and more inclusive for all users.\n\n[1] https://esco.ec.europa.eu/\n[2] https://www.onetcenter.org/\n[3] https://www.rijksoverheid.nl/onderwerpen/laaggeletterdheid/aanpak-laaggeletterdheid\n\nDouma, W. H. (1960). De leesbaarheid van de landbouwbladen: een onderzoek naar een toepassing van leesbaarheidsformules. Bulletin, 17. Afdeling Sociologie en Sociografie van de Landbouwhogeschool Wageningen, 1960. Retrieved from https://edepot.wur.nl/276323\n\nDale, E., & Chall, J. (1948). A formula for predicting readability. Educational Research Bulletin, 27, 11–20, 28. Retrieved from https://www.jstor.org/stable/1473169\n\nStaphorsius, G. (1994). Leesbaarheid en leesvaardigheid: De ontwikkeling van een domeingericht meetinstrument (Doctoral dissertation, Universiteit Twente). Arnhem: Cito.\n\nStaphorsius, G., & Krom, R. (1985). CITO leesbaarheidsindex voor het basisonderwijs: Verslag van een leesbaarheidsonderzoek. Arnhem: Cito."
            },
            {
              "id": 73,
              "title": "Explicit Document Representations for Dutch Open Government Data",
              "authors": "Damiaan Reijnaers",
              "affiliation": "Institute for Logic, Language and Computation",
              "abstract": "This work in progress focuses on developing explicit representations for Dutch open government data. As more documents are being published under a new open government act (the ‘Wet open overheid’ or ‘Woo’), tasks like text simplification and information retrieval are becoming increasingly important in this context. This, in turn, demands a rethinking of how such documents are represented computationally, as current methods in NLP often rely on textual features to create embeddings of documents, while failing to capture the elements necessary to understand the data in their legal context. For example, the concepts of ‘relevance’ or ‘similarity’ of and between official texts may not merely rely on textual similarity, but may require legal reasoning. Likewise, the simplification of the contents of such documents may not simply translate to the task of reducing linguistic complexity, but to one about clarifying the underlying legal concepts and the relationships between them. In other words: open government documents may benefit from encodings that are not based on linguistic context, but on features that do justice to their legal context. This work proposes a method of how this may be achieved by (again) resorting to open data to represent documents that involve decisions (Dutch: ‘beschikkingen’) not by their textual contents, but by publicly queryable trees that capture the reasoning structure reflected in their texts. Following this representational structure, trivial path-comparision algorithms may function as an interpretable means to compute similarity between case decisions. Not only does a representation of this kind allow for a more intrinsic assessment of similarity and relevance, it does simultaneously abide by calls from experts at and across the intersection of the judicial and computational domains for retrieval algorithms in governmental contexts to be intrinsically explainable."
            },
	      {
		  "id": 42,
		  "title": "Retrieval-augmented Generation for Automated Written Corrective Feedback: From Dataset to Human Evaluation",
		  "authors": "Jasper Degraeuwe, Thomas Moerman",
		  "affiliation": "Ghent University",
		  "abstract": "In this study we will combine insights and resources from the domain of second language acquisition (SLA) with the power of generative large language models (LLMs), which have become very adept at various language tasks (ranging from text summarisation to machine translation). Specifically, this study will evaluate the use of retrieval-augmented generation (RAG), which allows steering the output of an LLM prompt towards information included in a self-provided knowledge base, to automatically generate feedback for SLA purposes.\nThe acquisition of a foreign/second language encompasses the acquisition of a complex and multifaceted communication system, of which grammar is a main component. Grammatical concepts (e.g., conditional clauses) are commonly practised through close-ended exercises such as fill-in-the-blanks (e.g., \"If they _____ how it would turn out, they would have reacted differently.\" [to know]). One of the factors determining the learning success of such exercises is receiving adequate feedback, which, in turn, puts a huge strain on the already considerable workload of teachers. To overcome this \"feedback bottleneck\", computer-assisted methods for automated feedback generation (AFG) have been proposed. The most simple and rule-based AFG method is checking if the learner's response occurs in a predefined list of correct answers. This, however, excludes targeted and individualised feedback on why a given answer was wrong.\nLLMs offer a means to address this limitation: using the exercise description and the learner's response as input, LLMs can be prompted to output personalised written feedback. Nevertheless, these LLMs lack educational specialisation and grounding in specific course materials, meaning that the accuracy and pedagogical value of their output cannot be taken for granted. Research to analyse and evaluate LLM performance in this regard is thus needed before they can be safely implemented as \"feedback assistants\" (e.g., to be used by teachers or to be integrated in autonomous Intelligent Language Tutoring Systems).\nThe current study aims to contribute to this new area of research by (1) presenting a dataset containing a total of 675 authentic student responses to three types of close-ended grammar exercises for three different languages (Dutch, English, and Spanish); (2) using this dataset to develop a RAG system that automatically generates feedback based on course materials as the knowledge base; and (3) conducting a human evaluation experiment in which teachers assess the automatically generated feedback for accuracy and pedagogical validity.\nRegarding the RAG system, we will use open-source LLMs (e.g., Llama or Mistral) and compare two architectures: a baseline \"dummy system\" where course materials are included directly in the prompt, versus an advanced system using vector database retrieval from the knowledge base. In summary, this research will contribute to understanding how retrieval-augmented generation can leverage educational content to improve automated feedback quality in language learning contexts."
	      },
	      {
		  "id": 7,
		  "title": "The AI Farm Assistant: Ontology-driven Trustworthy RAG for Agricultural Question Answering",
		  "authors": "Daan Di Scala, Daan Vos, Mike Wilmer",
		  "affiliation": "TNO & Utrecht University",
		  "abstract": "In the EU-FarmBook project, the aim is to make publicly available European agricultural knowledge accessible through a uniform and curated knowledge platform [1, 2]. The available data (currently over 5000 documents) contains a range of modalities such as scientific papers, practice abstracts, podcasts, YouTube videos and more. Navigating the platform’s data can be challenging due to the different modalities, potentially complex documents, and different agricultural application areas. For this reason, we developed The Farm Assistant chatbot, a conversational interface that leverages a Large Language Model (LLM), to lower the barrier of entry to access the data for the users of the EU-FarmBook platform [3]. Creating a successful European agricultural chatbot brings its own challenges, as it needs to support multiple European languages, different topics (e.g., crop farming, livestock, forestry),  different language complexities and different types of users (farmers, advisors, scholars). As trustworthiness is a key goal within this high-stakes domain, our research aim is to ensure the Farm Assistant’s trustworthiness, with a focus on achieving the right level of personalization while retaining factuality. For this, we employ a Retrieval Augmented Generation (RAG) module, based on recent Mistral models. Furthermore, the EU-FarmBook ontology [4], which allows for structured and semantic document and metadata processing. This metadata is used as structured input of the RAG system. This allows for personalization based on semantic metadata mapping with relevant attributes (e.g., geographic location, homebase country and language, and their general field of work). This work builds upon earlier work on preference elicitation in conversational AI systems [5].  For further evaluation and validation purposes, we have collected a dataset by surveying ambassadors within the agricultural domain on questions that the Farm Assistant should be able to answer. Preliminary results show the Farm Assistant performing well on factual queries, but difficulties remain with complex queries that need to handle multiple sources or outdated, contradictory or missing data. The results of our research will provide insights on best practices towards trustworthy chatbot answers.\n\n[1] EU-FarmBook Welcome Page https://welcome.eufarmbook.eu/ [Accessed 06-06-2025]\n[2] Kiraly, G., Vago, S., Bull, E., van der CRUYSSEN, L., Arbour, T., Spanoghe, P., & van Dijk, L. (2023). Information behaviour of farmers, foresters, and advisors in the context of digitalisation in the EU.\n[3] EU-FarmBook Farm Assistant Page https://eufarmbook.eu/en/farm-assistant [Accessed 06-06-2025]\n[4] Panoutsopoulos, H., Brewster, C., & Fountas, S. (2021). A Semantic Data Model for a FAIR Digital Repository of Heterogeneous Agricultural Digital Objects. The Case of the EUREKA FarmBook.\n[5] Ziegfeld, Liv, Daan Di Scala, and Anita HM Cremers. \"The effect of preference elicitation methods on the user experience in conversational recommender systems.\" Computer Speech & Language 89 (2025): 101696."
              }

          ]
        },
        {
          "id": 4,
          "name": "Text Analysis & NLP Applications",
          "description": "Text processing, classification, and computational linguistics applications",
          "color": "#06B6D4",
          "posters": [
            {
              "id": 3,
              "title": "Computational Linguistic Profiling and Genre Classification of Narrative and Dialogic Forensic Transcriptions of Police Interviews",
              "authors": "Romane Werner, Sonja Bitzer, Thomas François",
              "affiliation": "UCLouvain",
              "abstract": "The increasing reliance on linguistic evidence in legal contexts requires a detailed understanding of the linguistic features characterizing various legal and forensic texts. Prior research by Venturi (2012) analysed Italian legal sub-genres, highlighting syntactic complexity and lexical density differences. Similarly, Dell’Orletta et al. (2013) studied linguistic variation across literary, journalistic, educational, and scientific genres. However, computational analysis of linguistic variation within forensic transcriptions, particularly comparing narrative and dialogic police interview types, remains underexplored. Addressing this gap, our study investigates the linguistic differences between these transcription types and their forensic and computational implications. The corpus includes two transcription types: narrative monologues and dialogic question-answer interviews. We applied automatic multi-level linguistic annotation to extract lexical, morpho-syntactic, and syntactic features. Statistical analyses were used to identify group differences, complemented by effect size (Cohen’s d) and Dunn’s test for multiple comparisons. A Random Forest classifier, validated via stratified 10-fold cross-validation, assessed feature robustness and predictive power in discriminating transcription types. Model performance was evaluated with accuracy, precision, recall, and F1 score to ensure reliable classification and generalizability. By integrating linguistic profiling with computational methods, this study extends prior genre classification work (Venturi, 2012; Dell’Orletta et al., 2013) and advances forensic text analysis by identifying lexical and syntactic complexities affecting readability and legal comprehension. Results reveal clear linguistic distinctions between transcription types and other subcorpora. Dialogic interviews feature longer sentences, increased syntactic embedding, and greater syntactic flexibility, reflecting spontaneous oral discourse distinct from the nominal, highly structured style typical of legal texts analysed by Venturi (2012). Narrative monologues show higher noun density and more coherent syntactic structures, positioning them between spontaneous speech and formal legal writing. These types diverge from literary and journalistic genres studied by Dell’Orletta et al. (2013), which display shorter sentences and higher lexical density but share features like elevated pronoun and verb frequencies with dialogic speech. Compared to legislative and administrative legal texts, marked by reduced verbal activity and increased nominalization, both transcription types emphasize verbal and adverbial elements, highlighting forensic oral language’s dynamic nature. The forensic implications are significant, as dialogic transcriptions’ syntactic complexity and non-canonical structures may complicate transcription accuracy and courtroom interpretation, potentially affecting evidentiary reliability and trial outcomes. Narrative transcriptions, with more structured and accessible language, may enhance trial comprehensibility. These findings highlight the forensic significance of transcription types and call for tailored transcription protocols alongside computational tools to mitigate risks posed by syntactic complexity and lexical variability. This research underscores the necessity of interdisciplinary approaches combining computational linguistics and forensic sciences to address oral and legal discourse variability in judicial contexts, thus strengthening investigation and trial integrity. Future computational linguistics work will focus on refining feature extraction through advanced methods such as deep learning-based syntactic parsing to better capture subtle forensic transcript nuances. Additional efforts will develop enhanced classification models using ensemble techniques to reduce human error and standardize outputs. These advancements aim to enable scalable, accurate, and interpretable forensic linguistic analyses, bridging computational efficiency with legal reliability.\nDell’Orletta, F., Montemagni, S., and Venturi, G. (2013). Linguistic Profiling of Texts Across Textual Genres and Readability Levels. An Exploratory Study on Italian Fictional Prose. In Proceedings of Recent Advances in Natural Language Processing, Hissar, Bulgaria, 7-13 September 2013.\nVenturi, G. (2012). Investigating legal language peculiarities across different types of Italian legal texts: an NLP-based approach. In IAFL Porto 2012 Proceedings, Porto, Portugal."
            },
            {
              "id": 11,
              "title": "Telling Stories to Win Arguments: Modeling the Impact of Narrative Features in Argumentative Discussions",
              "authors": "Sara Nabhani, Khalid Al-Khatib, Federico Pianzola, Malvina Nissim",
              "affiliation": "University of Groningen",
              "abstract": "Can specific narrative features make an argument more persuasive? The role of narratives in enhancing persuasion is widely studied, but their nuanced role in online argumentation remains underexplored. We introduce a new ChangeMyView corpus annotated with seven fine-grained narrative features, grounded in two established theoretical frameworks, focusing either on textual features or narrative effects. Leveraging both encoder- and decoder-based large models, we train classifiers to detect these features and apply them at scale to assess their impact on argument persuasiveness. Our findings reveal that (i) fine-tuned models outperform LLMs in detecting narrative features; (ii) features like event sequencing are linked to persuasive comments; (iii) combining multiple narrative features often reduces persuasiveness, highlighting the importance of coherence over complexity and offering new insights into how narratives can influence argumentation."
            },
            {
              "id": 21,
              "title": "Analyzing the Impact of Customer Satisfaction Level on Emotional Language and Word Frequency in Amazon Reviews",
              "authors": "Zeinab Rahmanifard",
              "affiliation": "",
              "abstract": "This study explores the interplay between customer satisfaction levels, emotional language, and word frequency in Amazon product reviews. Using sentiment polarity and N-gram analyses, we investigate whether satisfied and unsatisfied customers differ in their use of emotional intensity and linguistic detail, including infrequent words. Our findings reveal that negative reviews frequently feature rare words and more specifically rare adjectives, suggesting greater linguistic specificity, while positive reviews exhibit higher emotional expression and are perceived as more helpful by other customers. These insights illuminate the linguistic markers of satisfaction and provide actionable implications for businesses to enhance customer engagement."
            },
            {
              "id": 24,
              "title": "Multi-Agent Deliberation for Fallacy Detection",
              "authors": "Frieso Turkstra, Khalid Al-Khatib",
              "affiliation": "University of Groningen",
              "abstract": "Detecting logical fallacies is essential for improving the quality of discourse and combating misinformation, yet current NLP methods are often hindered by the limitations of single-model analysis, which restricts reasoning diversity. Recent advancements in multi-agent systems using large language models (LLMs) have shown promise in tasks like fact-checking, but fallacy detection remains underexplored. We introduce a new deliberative multi-agent approach for fallacy detection, where multiple LLM-based agents engage in structured deliberation to resolve their disagreements, mirroring ideal argumentation-theoretic practice. This approach improves reasoning rigor and allows for more subtle evaluations of fallacious arguments, outperforming single-LLM and standard ensemble methods, particularly in complex or ambiguous cases."
            },
            {
              "id": 27,
              "title": "Multi-SimLex for Dutch: Comparing Embedding and Prompt-Based Model Performance on Semantic Similarity",
              "authors": "Lizzy Brans, Jelke Bloem",
              "affiliation": "Universiteit Utrecht, Universiteit van Amsterdam",
              "abstract": "Lexical semantic similarity benchmarks can offer insights into how computational models encode and relate meaning. However for Dutch, such resources remain scarce, limiting both the empirical evaluation of language models and our understanding of how well their representations align with human semantic intuitions. This study introduces a Dutch extension of the Multi-SimLex benchmark, comprising 1,888 word pairs annotated for similarity by native speakers. Our resource enables a detailed evaluation of eighteen language models that vary in architecture (static, encoder, encoder-decoder, and decoder), multilingual training scope, and size.\n\nWe assess semantic similarity using two complementary methods: cosine similarity over word or word-pair embeddings, and prompt-based similarity judgments from generative models. Embedding-based methods were applied to static embeddings (Word2Vec, fastText, GloVe), monolingual encoders (BERTje, RobBERT), multilingual encoders (mBERT, XLM-R, EuroBERT), encoder-decoder models (mT5, FLAN-T5) and decoder-only models (BLOOM, Falcon, Schaapje). Prompt-based evaluation was used for decoder-only models (GPT-4, Gemini 1.5, DeepSeek-V3), querying them in Dutch using zero-shot, few-shot and CoT prompts.\n\nOur results show clear methodological contrasts. Prompting yields the highest alignment with human similarity judgments, with GPT-4 reaching a Spearman ρ ≈ 0.761. Among the embedding-based models, the static model FastText was a top performer, achieving a correlation of ρ ≈ 0.485. Similarly, the monolingual Dutch encoder BERTje also performed strongly with ρ ≈ 0.468. Whereas, embeddings from the decoder-only models (e.g., BLOOM, Falcon, Schaapje) fared poorly in static evaluations.\n\nThis divergence illustrates that large generative models dynamically retrieve lexical knowledge during inference, while embedding-based models rely on fixed learned representations. It also confirms that Dutch-specific training remains essential for embedding-based performance, whereas prompt-driven tasks benefit from model scale and instruction tuning. \n\nIn addition to methodological insights, this work contributes a reliable Dutch semantic similarity benchmark, enabling future research on Dutch NLP and cross-lingual evaluation. By comparing across architectural paradigms, it shows how different model types encode Dutch meaning and which strategies most closely approximate human judgments. This dual focus on benchmarking and method comparison helps clarify when to rely on embeddings and when to opt for prompt-based reasoning.\n\nUltimately, the study advances our understanding of language models in a mid-resource language setting. It highlights the need for both model adaptation and evaluation diversity to ensure semantic accuracy across languages. The Dutch Multi-SimLex benchmark and evaluation scripts will be made publicly available to support reproducible research."
            },
            {
              "id": 32,
              "title": "Harmonizing data for Sentiment Analysis in Dutch Conversations",
              "authors": "Mert Yazan, Frederik Situmeang, Suzan Verberne",
              "affiliation": "Hogeschool van Amsterdam, Leiden University",
              "abstract": "Being responsive to customer sentiments is crucial for customer support bots. Recognizing the sentiment helps chatbots to be more empathetic and allows them to come up with a suitable answer tailored to the emotional state of the customer. Despite the existence of multiple datasets in the Dutch language for sentiment analysis in various domains, they don't follow the same structure. The label set for sentiments varies; some focus on binary (positive/negative) classification, while others might have an extra class for neutral samples. Some studies go beyond sentiment analysis by extending it with emotion classification, but there isn't a consensus on the number of emotions; it ranges from 5 to up to 28 [1, 2]. This makes it challenging to apply domain adaptation or to combine datasets. Furthermore, most of the datasets do not contain conversational data. In a customer support scenario, tracking the sentiment through the conversation is crucial because the current sentiment of the customer stems from previous turns. Even in English, the number of conversational sentiment analysis datasets is scarce, making it hard to study the evolution of sentiment across turns. \n\nTo address these challenges, we consolidate publicly available Dutch sentiment analysis datasets into a unified resource. We standardize labels into a three-way classification: positive, neutral, and negative. Label harmonization is achieved through majority voting, aided by large language models, while retaining the original labels (e.g., emotion categories) as supplementary metadata to enable further analysis of fine-grained affective states. To make our resource suitable for conversational settings, we also include conversational datasets and employ a two-pronged labeling strategy for them: (a) a label assigned based solely on the current turn, and (b) a context-aware label derived by considering the preceding conversation turns. Due to the limited availability of conversational sentiment analysis datasets in Dutch, we extend our dataset to include English sources as well. For these, we provide both the original English texts and their machine-translated Dutch versions. To validate our data, we run extensive experiments across domains, datasets, and language models, and compare zero/few-shot learning with fine-tuning. With this work, we aim to provide researchers a simple and intuitive way to experiment with sentiment analysis in Dutch. \n\n[1] De Bruyne et al., 2021, Prospects for Dutch Emotion Detection: Insights from the New EmotioNL Dataset\n[2] Labat et al., 2023, EmoTwiCS: a corpus for modelling emotion trajectories in Dutch customer service dialogues on Twitter"
            },
            {
              "id": 64,
              "title": "Push and Pull: Training Sentence Encoders with Contrastive Learning for Distance-Based Multi-Label Text Classification",
              "authors": "Jens Van Nooten, Andrew Kosar, Walter Daelemans",
              "affiliation": "University of Antwerp",
              "abstract": "Distance-based classification (DBC) is a method that assigns labels to text by measuring semantic similarity between the text and the label representations. Due to the complexity of multi-label text classification (MLTC), DBC remains underexplored for this task. Previous studies have focused on determining optimal thresholds, reaching promising results with contextual sentence encoders. We demonstrate that the performance of these models can be further improved by training them with contrastive losses, i.e., by bringing text representations closer to the corresponding true label representations in an embedding space. We experimented with three contrastive losses: First, a contrastive loss that minimizes the distance between a text and its true labels, in addition to maximizing the distance with K hard negative labels and forcing a margin between positive and negative labels. Second, we experiment with a variant of this loss by forcing a margin between a text and K hard negatives. Third, we implement a pairwise contrastive loss from sentence transformers that minimizes the cosine distance with true text-label pairs and maximizes it for false text-label pairs. These contrastive losses were evaluated using three sentence encoders (stella-400M-v5, GIST-Large-Embedding-V0 and bge-base-en-v1.5) on six datasets (SemEval, BioTech, Reuters, AAPD, LitCovid, and IMDB) and showed consistent substantial improvements over base sentence encoders, thereby narrowing the gap between DBC methods and fine-tuned or zero-shot approaches."
            },
	      {
		  "id": 29,
		  "title": "Constructing and Applying Personalised Lexical Profiles for Lexical Alignment in Dutch Large Language Model Responses",
		  "authors": "Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx",
		  "affiliation": "Centre for Language Studies, Centre for Language and Speech Technology, Radboud University; Behavioural Science Institute, Radboud University",
		  "abstract": "Lexical alignment, the tendency of two speakers to use similar terms and phrases across conversations, is recognised to be essential for successful communication. While this concept has been researched in both human-human and human-agent dialogue, its implementation in conversational agents that use large language models (LLMs) for response generation remains underexplored, particularly for the Dutch language. This study presents a two-step approach to achieving lexical alignment in LLM-generated responses for spoken human-agent dialogue in Dutch.\n\nIn the first step, we drew on strategies used for personalising conversational agents and explored constructing stable and personalised lexical profiles that reflect individual speakers' lexical preferences. These profiles contained the most frequently used terms per different part-of-speech (POS) category and n-grams, derived from transcribed speech data from the speaker. We systematically varied the amount of data used to construct the profiles and the number of terms included per POS category and evaluated profile performance across time using recall, coverage, and similarity metrics.\n\nWe ran our experiments on a corpus of transcriptions of 50 interviews with Dutch veterans. Results showed that compact profiles, those constructed using 10 minutes of transcribed speech including 5 adjectives, 5 conjunctions, and 10 adverbs, nouns, pronouns, and verbs each, achieved the best balance between data requirements and performance.\n\nIn the second step, we explored the integration of these profiles into LLM prompts to achieve lexical alignment between the generated responses of an agent and the speaker. Specifically, LLMs were prompted twice, once to generate a follow-up question and once to generate a reflection, based on a conversational dialogue snippet included in the prompt. We compared two conditions: an alignment condition, in which the model was instructed to use terms and phrases included in the profile provided in the prompt, and a baseline condition, in which no such instruction and profile were provided. We tested four LLMs capable of Dutch language generation: two models finetuned specifically for Dutch (Qwen1.5-7B-Dutch-Chat and GEITje-7B-ultra) and two widely used multilingual models (GPT and Mistral). Preliminary results suggested a promising potential of the multilingual models to lexically align, as shown by increased use of profile terms and phrases in their generated responses. The Dutch-specific models, on the other hand, showed limited differences across conditions. Ongoing analysis of the generated responses will further evaluate the lexical alignment abilities of the different models. Additionally, we will assess the quality and contextual relevance of the generated responses to get an indication of the overall response generation capabilities of the models.\n\nOverall, our study shows the possibilities of alignment strategies for personalised speaker adaptation in LLM-based conversational agents."
	      },
	      {
		  "id": 75,
		  "title": "Investigating Ambiguous Plural Reference in Large Language Models",
		  "authors": "Anh Dang, Rick Nouwen, Massimo Poesio",
		  "affiliation": "Utrecht University, Queen Mary University of London",
		  "abstract": "Large language models (LLMs) have been shown to struggle with language ambiguity (Liu et al., 2023; Wildenburg et al., 2024; Stengel-Eskin et al., 2023). It has been found that LLMs often provide incorrect responses to ambiguous input (Sedova et al., 2024; Stengel-Eskin et al., 2023) and are often unable to detect or generate ambiguous expressions (Liu et al., 2023; Kamath et al., 2024). Since ambiguity is always present in communication, this may lead to misunderstanding and the inability of LLMs to fully comprehend language use in context. One prominent example of a task involving ambiguity is coreference resolution and, in particular, resolving anaphoric plural reference. There has been much research on computational models of coreference (Poesio et al., 2023) and on how LLMs interpret coreference (Gan et al 2024, Le and Ritter, 2024), but plural coreference is still relatively understudied (Yu et al., 2021). There is evidence from Cokal et al. (2023) that in certain contexts the preference for a singular or plural pronoun is less clear. For example, consider the sentence “The engineer attached the engine to the boxcar and sent it to London”. The engine and the boxcar are combined, creating a new object, which is a train. As such, the pronoun following the verb \"sent\" can be \"it\" or \"them\". Cokal et al. (2023) found that humans prefer using “it” in these cases. Moreover, they prefer considering the mereological object “train” to be the referent of the pronoun “it” rather than one of the two constituents (i.e., the engine and the boxcar). In this study, we focus on the processing of both unambiguous and ambiguous plural reference in LLMs. We find plural reference particularly interesting because it is highly dependent on how humans conceptualize plurality. In psycholinguistic research, formulating plural entities is usually a complex process, which is strongly influenced by how humans group entities (Cokal et al., 2023; Koh and Clifton Jr., 2002; Moxey et al., 2012, 2004; Asher and Wang, 2003). Our goal is to study how LLMs represent and interpret plural reference in ambiguous and unambiguous cases. We ask the following research questions: (1) Do LLMs exhibit human-like preferences in representing plural reference? and (2) Are LLMs able to detect ambiguity in plural anaphoric expressions and identify possible referents? To address these questions, we design a set of experiments, namely pronoun production, pronoun interpretation and ambiguity detection. In the pronoun production task, we compare the probabilities of singular and plural pronouns in ambiguous and unambiguous cases. In the pronoun interpretation and ambiguity detection task, we use different prompting strategies to elicit LLMs’ representations of the pronouns’ referents. We then compare how comparable LLMs are to humans in formulating and interpreting plural reference. We found that LLMs show a similar preference to humans in simple, unambiguous cases of plural reference. However, in some ambiguous cases, LLMs do not align with human preference. The findings also reveal the inconsistencies in the results across different types of experiments.\n\nReferences\nDerya Cokal, Ruth Filik, Patrick Sturt, and Massimo Poesio. 2023. Anaphoric reference to mereological entities. Discourse Processes, 60(3):202–223.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, and 1 others. 2024. Aya expanse: Combining research breakthroughs for a new multi-lingual frontier. arXiv preprint arXiv:2412.04261.\nAli Emami, Paul Trichelair, Adam Trischler, Kaheer Suleman, Hannes Schulz, and Jackie Chi Kit Cheung. 2019. The knowref coreference corpus: Removing gender and number cues for difficult pronominal anaphora resolution. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3952–3961. \nYujian Gan, Massimo Poesio, and Juntao Yu. 2024. Assessing the Capabilities of Large Language Models in Coreference: An Evaluation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1645–1665, Torino, Italia. ELRA and ICCL.\nGaurav Kamath, Sebastian Schuster, Sowmya Vajjala, and Siva Reddy. 2024. Scope ambiguities in large language models. Transactions of the Association for Computational Linguistics, 12:738–754. Sungryong Koh and Charles Clifton Jr. 2002. Resolution of the antecedent of a plural pronoun: Ontological categories and predicate symmetry. Journal of Memory and Language, 46(4):830–844.\nLe, N. T., & Ritter, A. (2023). Are Large Language Models Robust Coreference Resolvers?. arXiv preprint arXiv:2305.14489.\nAlisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. 2023. We’re afraid language models aren’t modeling ambiguity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 790–807.\nLinda M Moxey, Anthony J Sanford, Patrick Sturt, and Lorna I Morrow. 2004. Constraints on the formation of plural reference objects: The influence of role, conjunction, and type of description. Journal of Memory and Language, 51(3):346–364.\nLinda M Moxey, Anthony J Sanford, and Karen Tonks. 2012. Representing characters in a scenario: What makes two individuals a set? Language and cognitive processes, 27(9):1405–1424. Nikole D Patson. 2014. The processing of plural expressions. Language and Linguistics Compass, 8(8):319–329.\nAnastasiia Sedova, Robert Litschko, Diego Frassinelli, Benjamin Roth, and Barbara Plank. 2024. To know or not to know? analyzing self-consistency of large language models under ambiguity. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 17203–17217.\nElias Stengel-Eskin, Kyle Rawlins, and Benjamin Van Durme. 2023. Zero and few-shot semantic parsing with ambiguous inputs. arXiv preprint arXiv:2306.00824.\nPoesio, M., Yu, J., Paun, S., Aloraini, A., Lu, P., Haber, J., & Cokal, D. (2023). Computational models of anaphora. Annual Review of Linguistics, 9(1), 561-587.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprintarXiv:2302.13971.\nKellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. 2018. Mind the gap: A balanced corpus of gendered ambiguous pronouns. Transactions of the Association for Computational Linguistics, 6:605–617.\nFrank Wildenburg, Michael Hanna, and Sandro Pezzelle. 2024. Do pre-trained language models detect and understand semantic underspecification? ask the dust! In Findings of the Association for Computational Linguistics ACL 2024, pages 9598–9613.\nJuntao Yu, Nafise Sadat Moosavi, Silviu Paun, and Massimo Poesio. 2020. Free the plural: Unrestricted split-antecedent anaphora resolution. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6113–6125."
              },
	      {
		  "id": 2,
		  "title": "The CLARIN Knowledge Infrastructure for Linguistic and Language Technology Research",
		  "authors": "Vincent Vandeghinste, Bente Maegaard, Vesna Lušicky",
		  "affiliation": "",
		  "abstract": "While CLARIN is widely recognized for its technical infrastructure of language resources, tools, and services, its Knowledge Infrastructure is a vital and complementary pillar that supports researchers, educators, and developers in effectively using, sharing, and extending language data and technologies.\nThe CLARIN Knowledge Infrastructure promotes expertise exchange and best practices, and provides training and support across the research community. We present its main components and their relevance for the computational linguistics and digital humanities communities, with a focus on the Dutch and Flemish research landscape.\n\nKey Components\n1. Knowledge Centres (K-Centres): Centres of expertise that offer user support, consultancy, and training in specific domains or on specific languages. K-Centres cooperate in a yearly workshop to discuss common topics. They receive certification from CLARIN for terms of three years. K-Dutch, the Knowledge Centre for Dutch, hosted by the Instituut voor de Nederlandse Taal serves as a central hub for expertise on Dutch language resources, tools, and best practices, facilitating access to corpora, lexica, and NLP tools.https://kdutch.ivdnt.org.\n\n2. Best-practice papers: A collection of guidelines, workflows, and case studies that support researchers in adopting efficient, replicable practices when working with language data. https://www.clarin.eu/content/collection-best-practice-papers-clarin \n\n3. Tour de CLARIN: A communication initiative that showcases national consortia, use cases of CLARIN resources and services, and experiences from researchers across disciplines. Tour de CLARIN aims to raise awareness of available knowledge and tools over different user communities.\n\n4. CLARIN Cafés: Informal and interactive online space for discussion, which is held several times a year. For a list of topics: https://www.clarin.eu/content/clarin-cafe \n\n5. The Learning Hub: (a) Curated collections of webinars, tutorials, and educational resources covering topics like data creation, annotation, and advanced NLP workflows; (b) DH Course Registry, an online directory of Digital Humanities courses offered by European universities (together with DARIAH): \n\n6. Funding instruments: CLARIN provides support for workshops, collaborative initiatives, and for mobility grants, which fund researchers to visit CLARIN centres in other countries.\nCLARIN Annual Conference: The central networking and knowledge exchange event where developers, infrastructure providers and researchers present their work to the community. It features a PhD session, providing early-career researchers with a platform to engage with the community, and a bazaar for work-in-progress, offering an informal space for showcasing ongoing projects and prototypes.\n\n\nFor the CLIN community, the CLARIN Knowledge Infrastructure provides:\n- Easy access to expert support and learning resources.\n- Opportunities to discover and adopt best practices in language resource development and use.\n- A platform for showcasing research outputs and collaborative projects.\n- A bridge between computational linguistics and the broader digital humanities landscape.\n\nBy combining technical solutions with human expertise and community engagement, the CLARIN Knowledge Infrastructure enhances the accessibility, usability, and impact of language resources and technologies."
	      }
          ]
        }
      ]
    },
    {
      "id": "afternoon",
      "name": "Afternoon Poster Session",
      "time": "15:20-16:30",
      "rooms": 4,
      "groups": [
        {
          "id": 5,
          "name": "Machine Translation & Cross-lingual NLP",
          "description": "Pure machine translation focus, cross-lingual processing",
          "color": "#3B82F6",
          "posters": [
            {
              "id": 10,
              "title": "Exploring More Efficient Techniques for Synthetically Generated In-Domain Data Sets for MT",
              "authors": "Thomas Moerman, Arda Tezcan",
              "affiliation": "Ghent University, LT3",
              "abstract": "Fuzzy match (FM) augmentation is a data augmentation technique that improves machine translation (MT) by incorporating similar translations from existing parallel corpora to guide the translation of new source sentences. This approach has proven highly effective for improving neural MT (NMT) quality in domain-specific scenarios when large bilingual datasets are available. Recent work has further demonstrated that FM augmentation can be successfully combined with back-translation techniques when substantial monolingual data is available in the target language, enabling Neural Fuzzy Repair workflows that significantly enhance translation performance.\n\nHowever, domain-specific translation scenarios frequently encounter the dual challenge of limited bilingual training data and scarce monolingual resources. When monolingual data in the target language is available, back-translation has been successfully employed to augment FM techniques, but when such data is unavailable, these methods become severely constrained, leaving practitioners with limited options for improving specialized translation quality.\n\nThis work investigates a novel approach to address this resource scarcity by generating synthetic monolingual data in the target language using Large Language Models (LLMs). We employ back-translation on this LLM-generated content to create corresponding source sentences, producing synthetic source-target sentence pairs that can then be utilized for FM augmentation in specialized translation contexts. This approach combines the established benefits of fuzzy matching with the generative capabilities of modern LLMs to overcome traditional data availability constraints.\n\nOur experimental framework evaluates this methodology across three language directions: English→Ukrainian, English→French, and French→English, testing performance across two specialized domains with varying dataset sizes to assess both scalability and domain adaptability. The approach builds upon established FM augmentation and back-translation techniques while extending their applicability to resource-constrained scenarios where neither large bilingual nor monolingual datasets are available.\n\nThis work addresses a critical gap in domain-specific machine translation by enabling effective FM augmentation without dependence on additional monolingual resources. The combination of LLM-based synthetic data generation with established FM techniques represents a promising direction for improving translation quality in specialized domains with limited linguistic resources, advancing practical solutions for real-world translation scenarios where data scarcity remains a significant challenge."
            },
            {
              "id": 13,
              "title": "A Machine Translation Pilot for the Belgian Federal Parliament",
              "authors": "Vincent Vandeghinste",
              "affiliation": "",
              "abstract": "The Belgian Chambre of People Representatives, a.k.a De Kamer is the Belgian Federal Parliament, which functions, in general, in two languages, French and Dutch.  \nFor a pilot study for one of the translation services of De Kamer, which translates from French into Dutch, we have tested several state-of-the-art approaches towards machine translation. We had received a translation memory of about 90000 sentence pairs and have set up an evaluation experiment using the metrics BLEU  (Papineni et al. 2002), chrF  (Popović et al. 2015), TER  (Snover et al. 2006), BERTScore  (Zhang et al. 2020) and COMET (Rei et al. 2020), to determine which would be the best approach towards setting up a custom-built MT service at De Kamer in order to speed up the manual translation process.\nWe are testing the following approaches:\nNo Language Left Behind (Costa-jussà et al., 2022), with three  model variations, baseline and finetuned up to 10 epochs\nmBART (Liu et al. 2020), baseline and finetuned up to 10 epochs\nmT5 (Xue et al., 2021), with three model variants, baseline and finetuned up to 10 epochs\nFlaN (Fine-tuned Language Net) (Wei et al. (2022), with three model variants, baseline and finetuned up to 10 epochs\nOpus-MT: Marian MT with the model from Tiedemann et al. (2023).\nOpenNMT (Klein et al. 2017) model trained from scratch on data available from Opus (Tiedemann 2009)\n\nPreliminary conclusions show that finetuned NLLB and mBART show the best results, although models trained from scratch come close. Future work consists of further extending the tests, and working with additional data from De Kamer to see how translation quality improves with data growth. In a more distant future, the goal is to do human evaluation in a real life professional translation setting.\n\nReferences:\nCosta-jussà et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\nKlein et al. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. ACL\nLiu et al. 2020. Multilingual denoising pre-training for neural machine translation. TACL.\nPapineni et al. 2002. Bleu: a method for automatic evaluation of machine translation. ACL.\nPopovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. WMT. \nRaffel et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR.\nRei et al. 2020. COMET: A neural framework for MT evaluation. EMNLP.\nSnover et al. 2006. A study of translation edit rate with targeted human annotation. AMTA\nTiedemann 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces. RANLP.\nTiedemann et al 2023. Democratizing neural machine translation with OPUS-MT. LRE. \nWei et al. 2022. Finetuned language models are zero-shot learners. ICLR.\nXue et al. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. NACL.\nZhang et al.. 2020. Bertscore: Evaluating text generation with bert. ICLR."
            },
            {
              "id": 35,
              "title": "The MaTIAS project: Machine Translation to Inform Asylum Seekers",
              "authors": "Lieve Macken, Margot Fonteyne, Arda Tezcan, Ella van Hest, Michaël Lumingu, Katrijn Maryns, July De Wilde",
              "affiliation": "Ghent University",
              "abstract": "We present interim findings from the MaTIAS project, which aims to develop a multilingual notification system for asylum reception centres in Belgium. The system makes use of machine translation (MT) to enable staff to communicate practical information to residents in their preferred languages via WhatsApp, thereby enhancing communication effectiveness. The system supports three source languages (Dutch, French, and English) and fourteen target languages, including low-resource languages such as Somali and Tigrinya. \n\nThe system is based on Django and supports three main functionalities: user registration, message composition and delivery, and access to previously sent messages. Staff can register residents using their unique identification number, preferred language, and phone number. The interface resembles traditional email programs, allowing staff to specify the subject, source language, department, specific recipient groups (e.g., residents with children), and scheduling.  \n\nBased on observations made at reception centres, an inventory of messages was compiled. This collection covers topics including house rules, hygiene and safety, administration and services, opening hours and holidays, appointments, work and classes. Professional human translations were obtained in all target languages via translation agencies.  \n\n Automatic evaluation metrics were employed to compare the translation quality of several commercial MT systems: Google Translate, Microsoft Translator, ModernMT, and a customised version of ModernMT enhanced with a translation memory containing 100 messages. The evaluation was conducted using a fully parallel dataset comprising an alternative set of 100 messages. Among the available metrics, ChrF was selected as the most robust and reliable across all target languages. \n\nOverall, the customised ModernMT system outperformed its baseline counterpart without translation memory in most cases, and it achieved the highest ChrF scores for nine out of the fourteen languages. On the basis of its strong performance and ease of adaptability, ModernMT was selected for integration into the MaTIAS system.  \n\nThe evaluation also revealed significant variability in translation quality across different target languages. Languages such as Tigrinya, Somali, and Persian consistently yielded lower scores, reflecting broader challenges in MT for low-resource languages. Additionally, we examined the influence of the source language on translation quality within the customised ModernMT system. The results clearly demonstrated that messages translated from English consistently achieved higher ChrF scores than those translated from Dutch or French. \n\nA manual evaluation was conducted using the Cross-Lingual Semantic Text Similarity (XSTS) methodology, involving two native speakers per target language, to assess the perceived usefulness of the machine-translated messages. Most languages achieved scores indicating acceptable preservation of meaning, suggesting that MT can effectively support operational communication in asylum reception centres for the majority of target languages. However, translations into Tigrinya and Somali were consistently rated as too low in quality to be considered reliable for conveying critical information. For these low-performing languages, we plan to further expand the translation memory with additional manually curated translations, aiming to enhance MT performance."
            },
            {
              "id": 56,
              "title": "Improving error span prediction for human post-editing",
              "authors": "Alina Karakanta, Fleur van Tellingen, Gautam Ranka",
              "affiliation": "Leiden University Centre for Linguistics",
              "abstract": "In automatic machine translation (MT) evaluation and quality estimation there is a growing tendency to move beyond a single quality score. Driven by advances in large language models (LLMs), new metrics are being developed which allow for fine-grained error analysis, explanation of translation errors and correction suggestions. Recent work has investigated the usefulness of word-level error spans to aid the post-editing process (Shenoy et al. 2021; Sarti et al. 2025). However, automatically predicted error spans overlap poorly with human post-edits, which raises doubts about their usefulness in guiding post-editors toward potential errors. In this work, we explore ways for improving the predicted error spans, by filtering them based on automatic post-editing suggestions and error explanations generated from Large Language Models (LLMs). \n\nTo evaluate methods for improving predicting error spans for post-editing, we evaluate the automatically-predicted spans against spans derived from human post-editing. The data comes from the QE4PE dataset (Sarti et al. 2025) and consists of translations from English into Dutch in biomedical and social media domains. We use two models for obtaining error spans: XCOMET-XXL (Guerreiro et al., 2024) and GEMBA-MQM (Kocmi and Federmann, 2023). Our method involves using xTower (Treviso et al. 2024) to obtain error explanations and translation corrections based on XCOMET-predicted error spans. Based on these explanations, we generate error spans in two ways: 1) keep only the errors that were actually edited by xTower, 2) mark as errors the spans that were edited by xTower as part of the correction suggestions (automatic post-editing - APE). For the latter, we project the spans in the MT output based on Levenshtein distance. We evaluate the resulting spans using Average Precision (AP) and Area Under the Precision-Recall Curve (AUC) between automatic spans and error spans derived from human post-editing (oracle). \n\nOur findings show that GEMBA-MQM performs better than XCOMET-XXL on the Biomedical domain (AP=0.228 vs 0.191, AUC=0.537 vs 0.531) but on the Social media domain XCOMET-XXL has similar or slightly higher scores (AP=0.210 vs 0.214; AUC=0.547 vs 0.581). Filtering the errors based on xTower corrections does not lead to improvements, while there is even a drop in the social media domain (AP=0.193, AUC=0.557). However, the automatic post-edits (APE) based on translation corrections of xTower lead to an increase in scores for both domains (Biomedical AP=0.305, AUC=0.614; Social media AP=0.313, AUC=0.674). This shows that APE suggestions align more closely with human post-edits than error spans identified by quality estimation methods. In future work, we will evaluate this observation in real post-editing tasks to assess the impact of APE suggestions on productivity and output quality."
            },
            {
              "id": 62,
              "title": "Translating Metaphors with Machines: A Comparative Study of LLMs and Commercial NMT Systems in News texts",
              "authors": "Jiahui Liang, Aletta G. Dorst, Alina Karakanta, Jelena Prokic, Stephan Rijmakers",
              "affiliation": "Leiden University Centre for Linguistics",
              "abstract": "Metaphors are pervasive in everyday discourse and an essential cognitive tool in human understanding. They allow us to think and talk about abstract, complex and unfamiliar concepts in terms of more concrete and familiar ones (Lakoff & Johnson, 1980). Their interpretation requires contextual awareness, sociocultural knowledge, and conceptual reasoning, posing challenges for both machine translation (MT) and broader natural language processing (NLP) tasks.\nThe rise of large language models (LLMs) offers new possibilities for MT, outperforming earlier neural machine translation (NMT) systems. WMT24 findings confirm that several LLMs (e.g., GPT-4, Tower70B) surpass traditional systems across language pairs, excelling in human rankings and complex test suites (Kocmi et al., 2024). In metaphor translation, Li and Chen (2025) found LLMs use more human-like strategies than NMTs and match novice translators’ quality, though they struggle with novel metaphors, highlighting the need for metaphor-specific training. Beyond this, research on LLM metaphor translation remains scarce, leaving room for further exploration.\nOur project investigates:\n1.\tTo what extent do LLMs correctly translate metaphors in Chinese and English news texts, compared to traditional NMT systems?\n2.\tWhat types of metaphor translation shifts are predominantly used by LLMs versus NMT systems?\nWe selected four LLMs (GPT4.1, Gemini2.5, DeepSeek_V3, and xTower (Treviso et al. 2024)) and two leading NMT systems (DeepL and Google Translate), based on performance in prior MT tasks and small-scale metaphor translation tests. To ensure regional diversity and minimize geographical bias, we included models developed in the US, China, Germany, and Portugal. The systems include both general-purpose LLMs and translation-specialized models.\nOur data come from two established metaphor corpora annotated using the Metaphor Identification Procedure VU University Amsterdam (MIPVU) (Steen et al., 2010a): the PSU Chinese Metaphor Corpus (PSUCMC) (Lu & Wang, 2017; Wang et al., 2019) and the VU Amsterdam Metaphor Corpus (VUAMC) (Steen et al. 2010b). Both provide token-level annotations across multiple genres. We focused on news subcorpora, rich in metaphors due to their topical relevance. To target semantically important metaphors less likely to be omitted in translation, we limited analysis to those expressed by verbs, nouns, adjectives, and adverbs. We randomly selected 150 metaphor-containing sentences per language, yielding 409 English and 245 Chinese metaphors.\nAfter obtaining translations in both Chinese and English using a simple instruction prompt, we aligned each annotated source metaphor with its translation. Each was evaluated for correctness based on whether a professional translator would edit. We also annotated translation shifts, including omission and shifts in form, function, meaning, or cohesion. For Chinese data, paraphrases were provided as reference for evaluation.\nAt the preliminary stage, 100 sentences per direction have been annotated. In results, LLMs showed lower error rates than DeepL in both directions: 10.8–11.9% vs. 15.6% for English–Chinese, and under 6.2% vs. 9.3% for Chinese–English. Form shifts were most frequent, followed by function and cohesion shifts. This suggests that LLMs perform well in metaphor translation, warranting further review in annotation and investigation with larger datasets.\nOverall, the study provides reference for application and improvement of machines in metaphor translation."
            },
            {
              "id": 79,
              "title": "Effect of finetuning one language pair on the machine translation performance of other language pairs",
              "authors": "Shilin Chen, Kushal Jayesh Tatariya, Miryam de Lhoneux",
              "affiliation": "KU Leuven",
              "abstract": "When finetuning transformer-based neural machine translation models with a certain language pair, the model’s performance on other language pairs sometimes also improves (i.e. cross-lingual transfer) as demonstrated by existing studies. \n\nThe present study aims to find whether there will be a consistent effect of finetuning a certain language pair on the MT performance of other language pairs, across different MT models and different language pairs. \n\nIn the present study, five models are examined, including three multilingual-to-multilingual models, i.e. “NLLB/3.3B”, “NLLB/1.3B”, “mBART50”, an English-to-multilingual model “Opus Mt En Mul”, and a multilingual-to-English model “Opus Mt Mul En”.\n\nIn the first experiment, four models (the three multilingual-to-multilingual models and “Opus Mt En Mul”) were finetuned with a large English-Chinese parallel corpus named UM-Corpus, with the direction of English to Chinese. In the second experiment, four models (the three multilingual-to-multilingual models and “Opus Mt Mul En”) were finetuned with the same corpus, with the direction of Chinese to English. FLORES is chosen as the evaluation benchmark for the MT performance of the underlying original models and the finetuned models. \n\nCHRF scores are calculated for English to Chinese, English to French, English to Japanese, French to Chinese, and Japanese to Chinese language pairs for all finetuned models and their underlying original models in the first experiment, with the exception that, for “Opus Mt En Mul”, only the three language pairs with English as source are considered, due to its weaker multilingual capabilities. Similarly, in the second experiment, CHRF scores are calculated, and directions were reversed compared to the first experiment.\n\nThe results show that, for all four models in the first experiment, finetuning with English to Chinese corpus improved the MT performance for all the three directions with Chinese as target (i.e. showing cross-lingual transfer effect for French to Chinese and Japanese to Chinese directions) but worsened for English to French and English to Japanese directions. In the second experiment, only Chinese to English direction saw an improvement, and MT performance of all other 4 directions worsened for almost all models, indicating that cross-lingual transfer did not happen when Chinese to English corpus was used for finetuning. This means that, the effect of finetuning one language pair on MT performance of other language pairs was inconsistent across language directions (i.e. finetuning with English to Chinese corpus, vs finetuning with Chinese to English corpus), but was almost consistent across different models (this includes different model families, same model family with different parameter sizes, and models with different multilingual capacities).\n\nTo interpret these results, possible reasons are discussed, including the interaction between CHRF calculation method and the different lexical natures of the two East Asian languages vs the two European languages. As a direction for future research, more comprehensive studies (i.e. more models, language pairs, training and evaluation datasets) on cross-lingual transfer can be conducted."
            },
            {
              "id": 81,
              "title": "Self-Supervised Data Filtering for Knowledge Distillation in Low-Resource Neural Machine Translation",
              "authors": "Maria Zafar, John Organ, Rejwanul Haque",
              "affiliation": "SETU Ireland",
              "abstract": "Transformer, a deep learning-based approach, represents the current state-of-the-art in machine translation (MT) research. The large-scale pre-trained Transformer models produce state-of-the-art performance across a wide range of MT tasks for many languages. One potential challenge with such models is their high demand for data, computation, memory, power, and energy. Training and deploying these models often requires powerful GPUs or large-scale computing clusters, raising concerns about their environmental impact and sustainability. To address this, knowledge distillation (KD) that transfers knowledge from large, complex models (teachers) to smaller, more efficient models (students) has become a widely adopted approach in MT.\nTranslations generated by the teacher models (e.g. NLLB) for low-resource languages that are often noisy or of low quality. Hence, an important part of pseudo-label distillation is selecting only high-quality translations produced by the teacher model for training. To do this, one usually needs ground truth labels to identify and remove low-quality examples.\nThis could pose a challenge for translation service providers (TSPs), as they may have limited budgets for training. To address this, we propose a label-free data filtering for KD in MT. The teacher model's confidence estimates are used to remove instances from the distilled training data where its confidence is low. We tested our methods on a low-resource Urdu-to-English translation task, operating within a constrained training budget in an industrial translation setting. Our findings show that confidence estimation-based filtering can significantly reduce the cost and CO2 emissions associated with training a student model, without drop in translation quality -- making it a practical and sustainable solution for the TSPs."
            },
	      {
		  "id": 20,
		  "title": "Open Challenges in Language Identification",
		  "authors": "Rob van der Goot",
		  "affiliation": "IT University of Copenhagen",
		  "abstract": "Automatic language identification is a core problem of many Natural Language Processing (NLP) pipelines. A wide variety of architectures and benchmarks have been proposed with often near-perfect performance. Although previous studies have focused on certain challenging setups (i.e. cross-domain, short inputs), a systematic comparison is missing. We propose a benchmark that allows us to test for the effect of input size, training data size, domain, number of languages, scripts, and language families on performance. We evaluate five popular models on this benchmark and identify which open challenges remain for this task as well as which architectures achieve robust performance. We find that cross-domain setups are the most challenging (although arguably most relevant), and that number of languages, variety in scripts, and variety in language families have only a small impact on performance. We recommend that language classification should always be evaluated cross-domain. Unsurprisingly, a straightforward solution for cross-domain performance improvement is to train on multiple domains, even when testing on new, unseen domains.\n\nWe also contribute practical takeaways: training with 1,000 instances per language and a maximum input length of 100 characters is enough for robust language identification, and even simple models perform very close, and sometimes more robust than their much more costly deep learning counterparts.\nBased on our findings, we train an accurate (94.41%) multi-domain language identification model on 2,034 languages, for which we also provide an analysis of the remaining errors.\n\nBecause of the large scale and diversity of the data used for this task, a non-negligible part of the labels contain errors. Based on our qualitative analysis, we employ an iterative process to improve quality of language classification models, where a variety of models is used to detect errors in the development split, which can then rigorously be cleaned in all data splits using heuristics. This approach shows to be effective in identifying common errors in the data, for example, remaining markup data and full instances from languages with wrong labels."
	      },
	      {
		  "id": 5,
		  "title": "Relating Language Modeling Performance to Morphology",
		  "authors": "Wessel Poelman, Thomas Bauwens, Miryam de Lhoneux",
		  "affiliation": "KU Leuven",
		  "abstract": "The extent to which individual language characteristics influence tokenization and language modeling is an open question. Differences in morphological systems have been suggested as both unimportant and crucial to consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter alia). We argue this conflicting evidence is due to confounding factors in experimental setups, making it hard to compare results and draw conclusions. We identify confounding factors in analyses trying to answer the question of whether, and how, morphology relates to language modeling. We re-assess three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative languages results in higher perplexities than fusional languages: they look at morphological alignment of tokenization, tokenization efficiency, and dataset size. We show that each conclusion includes confounding factors. We introduce token bigram metrics as an intrinsic way to predict the difficulty of causal language modeling, and find that they are gradient proxies for morphological complexity that do not require expert annotation. Ultimately, we outline necessities to reliably answer whether, and how, morphology relates to language modeling."
	      }
          ]
        },
        {
          "id": 6,
          "name": "Gender Bias & Fairness in AI",
          "description": "All gender bias and fairness studies, including gender-focused MT papers",
          "color": "#EF4444",
          "posters": [
            {
              "id": 16,
              "title": "Investigating Gender Bias in Large Language Models - Through Dutch Text Generation",
              "authors": "Matthijs ten Tije, Rastislav Hronský",
              "affiliation": "Tilburg University",
              "abstract": "This research investigates the presence and mechanisms of gender bias in the Dutch output of Large Language Models (LLMs). Building on recent findings of gender bias in English LLMs (Soundararajan and Delany, 2024), this study replicates their methodology for Dutch\nwhile extending the scope to probe (i) where in the LLM pipeline gender bias arises and (ii)\nwhat influence alignment strategies and sampling temperature exert on the amplification or attenuation of gender bias.\n\nThe first step in our research involved an embedding analysis to identify Dutch gender-encoded adjectives. While different embedding models produced varying rankings of the most gendered adjectives, consistent patterns nevertheless emerged across approaches: men were predominantly associated with adjectives related to action, status, and competence, while women were linked to appearance, sexuality, and relational qualities.\n\nBuilding on the overlapping set of gendered adjectives identified across embedding models,\nwe selected the 30 most female-coded and 30 most male-coded Dutch adjectives. These adjectives were then used to generate sentences across four Llama model variants (Grattafiori et al., 2024): pre-trained, uncensored, chat-aligned, and QA-specialized. Under varying temperature settings, we prompted the models to produce both stereotypical and counterstereotypical sentences, generating a total of 14,248 valid output sentences.\n\nTo quantify gender bias, we trained BERT-based classifiers to distinguish whether generated sentences were stereotype-consistent (e.g., male noun with male-coded adjective) or counter-stereotypical (e.g., female noun with male-coded adjective).\n\nUsing this approach, we conducted a True Positive Rate (TPR) gap analysis to measure how accurately the classifier detects male versus female associations. The baseline, pretrained Llama3 model, demonstrated a bias favoring male-coded language. All classifiers revealed\nhigher accuracy in detecting sentences containing male characteristics compared to female\nones, even prior to alignment procedures or Reinforcement Learning through Human Feed-\nback (RLHF) for this baseline model. This asymmetry was more pronounced in counter-\nstereotypical contexts, where classification accuracy disparities increased by a factor of 4.3 relative to stereotype-consistent sentences. These findings indicate that when generating\nlanguage that challenges traditional gender norms, LLMs gravitates toward male-oriented\nlanguage patterns.\n\nAnalysis of odds ratios, which measure how much more likely gendered adjectives are to\nappear in one context versus another, revealed mixed results for bias mitigation. Alignment\nand RLHF strategies can modulate bias expression, either reducing or amplifying it. However, none of the tested models eliminate gender bias entirely. Bias persists across all model variants. Counter-stereotypical contexts again showed more pronounced bias patterns than stereotype-consistent scenarios.\n\nIn conclusion, gender bias persists throughout the entire model development pipeline, transforming rather than disappearing as models progress from static embeddings to RLHF-aligned systems. Alignment strategies do not uniformly reduce bias and may paradoxically\namplify certain stereotypes, while temperature settings showed minimal impact on bias mitigation. These findings suggest that gender bias primarily stems from training data imbalances rather than specific linguistic features, and that post-training interventions modulate rather than eliminate such bias."
            },
            {
              "id": 30,
              "title": "Is She Even Relevant? When BERT Ignores Explicit Gender Cues",
              "authors": "Jonas Klein, Eva Vanmassenhove",
              "affiliation": "Tilburg University",
              "abstract": "If a model knows what a plumber is, but always imagines them as a man, what exactly does it know? Where humans can understand that a plumber can be anyone, regardless of their gender, language models lack the real-world grounding and mainly rely on statistical patterns.\nThis becomes specifically problematic when those patterns reflect harmful gender stereotypes, which are often unknowingly reproduced and even exacerbated in downstream applications (e.g. machine translation, text generation).\nTo get more insights into the roots of this problem, we investigated where and when gender bias emerges in a Dutch BERT model (Devlin et al., 2018). As opposed to prior work, which largely focuses on English, our work focuses on the Dutch language. Dutch is an interesting language when it comes to gender given that it is morphologically more complex compared to English; many professions have gendered forms while at the same time, the grammatically masculine form can often be used in a generic way.\nInspired by research of Van der Wal et al., 2022 we saved multiple checkpoints throughout the training process of a language model which allowed us to investigate the evolution of gender during training. We trained a Dutch BERT model from scratch and saved multiple check-points throughout the training process. This allowed us to investigate how gender information evolves along the way. At each checkpoint, we extracted contextual embeddings of thousands of gendered word occurrences and trained linear Support Vector Machines (SVMs) to classify gender, which allowed us to construct a dynamic gender subspace across training time. We evaluated how accurately gender could be predicted from these embeddings. We tracked classification performance both across all embedding dimensions and at the level of individual dimensions to identify where gender information is encoded. Finally, we projected profession terms, extracted from controlled sentence templates such as ”Zij is een loodgieter”, on the final gender subspace, to test whether the model aligns with the gender provided by the subject in the sentence or falls back on stereotypical expectations.\nOur findings show that gender becomes linearly encoded around epoch 20 and that there is an asymmetry when it comes to the encoding of masculine vs feminine information whereby masculine information is more distributed while the feminine one is concentrated into a smaller subset of the dimensions.\nAdditionally, we looked into gender-profession pairing using controlled sentence templates. Here we observed that the BERT model often defaults to the gender most stereotypically associated with a profession rather than using contextual cues. Forms that are grammatically masculine but semantically neutral (used to refer to any gender, e.g. ”verpleegkundige”), are still interpreted by the model as masculine forms, even when it is clear from the sentence context that the referent identifies as female. As a result of this ’male bias’ we consistently observe higher prediction accuracies for male referents in both stereotypical and non-stereotypical sentence contexts.\nIn summary, our findings suggest that, rather than using provided contextual cues, the model tends to rely on learned priors, especially when working with supposedly semantically neutral word forms."
            },
            {
              "id": 31,
              "title": "Can We Interpret Gender? Using Contrastive Explanations to Understand Gender Choices by Translation Systems",
              "authors": "Janiça Hackenbuchner, Arda Tezcan, Joke Daems",
              "affiliation": "Ghent University",
              "abstract": "Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) systems or large language models (LLMs). Yet, this has barely been explored in relation to a long-lasting problem in these systems: gender bias, which, although widely studied, has not yet been solved (Savoldi et al., 2025). One study by Attanasio et al. (2023) has explored the interplay between these two domains and found that interpretability is a “valuable tool for studying and mitigating bias in language models”. This interplay is the focus of this research.\n\nIt has been shown that certain contextual cues influence a human’s perception of gender in an ambiguous sentence context (Hackenbuchner et al., forthcoming). We aim to understand whether the same contextual cues influence an MT system when it translates a person’s gender into a certain inflection. To study this, we look at saliency scores to analyse which input tokens are most relevant for a certain translation decision. Specifically, we focus on contrastive explanations, which have been shown to outperform non-contrastive ones, to analyse which input tokens lead a model to produce one output instead of another (why is X predicted instead of Y?), as introduced in Yin and Neubig (2022). This interpretability technique helps understand which contextual cues (input tokens) in the source sentence affect the model’s choice of a certain gender inflection instead of another in the target translation. \n\nThis research is conducted on the dataset of gender-ambiguous sentences introduced in Hackenbuchner et al. (forthcoming) and compared to that study’s human annotations of contextual cues affecting their gender perceptions. Using the inseq toolkit (Sarti 2023), we realise contrastive explanations by computing the difference between two attribution outputs (the difference in probability between two options) based on the target translation (taken from the original dataset) and a translation contrasting in terms of gender. To exemplify this, we analyse which input tokens in the source “The business writer from Miami” lead to a higher probability of being translated into male (e.g., DE: Schriftsteller) or into female (e.g., DE: Schriftstellerin).\n\nPreliminary results show that there is a noticeable overlap between human perceptions and model attribution. Contextual cues (words) that influence human perception of gender are among the most frequent tokens that influence a model’s translation of gender in the target (i.e. that increase the probability of a certain gender output over another). This shows that humans and models seem to be influenced by very similar (if not the same) contexts in regards to gender (even in ambiguous scenarios). With this study, we contribute to the very limited research conducted on interpretability measures of model decisions in the translation of gender.\n----------------------------------------------------------------------------------------------------------------------------------------------\nBeatrice Savoldi, Jasmijn Bastings, Luisa Bentivogli, Eva Vanmassenhove. 2025. A decade of gender bias in machine translation. In: Patterns. Volume 6, Issue 6.\n\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, and Oskar van der Wal. 2023. Inseq: An Interpretability Toolkit for Sequence Generation Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 421–435, Toronto, Canada. Association for Computational Linguistics.\n\nGiuseppe Attanasio, Flor Miriam Plaza del Arco, Debora Nozza, and Anne Lauscher. 2023. A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3996–4014, Singapore. Association for Computational Linguistics.\n\nJaniça Hackenbuchner, Arda Tezcan, Joke Daems. Forthcoming: CLIN vol 14. Gender Bias and the Role of Context in Human Perception and Machine Translation.\n\nKayo Yin and Graham Neubig. 2022. Interpreting Language Models with Contrastive Explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 184–198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
            },
            {
              "id": 33,
              "title": "Diverse Voices, Diverse Choices: Demographics and Empathy Drive Human Preferences in LLM Responses",
              "authors": "Yishan Wang, Amanda Cercas Curry, Flor Miriam Plaza-del-Arco",
              "affiliation": "TU Eindhoven, CENTAI Institute, Leiden University",
              "abstract": "Language models (LLMs) are increasingly used in subjective domains such as moral reasoning, personal advice, and emotional support. Thus, understanding whose preferences these systems support and why is crucial. Without targeted evaluation, LLMs risk amplifying the dominant perspectives of overrepresented groups, marginalizing alternative viewpoints, and compromising the fairness of AI-mediated decisions. Recent work has evaluated LLM responses to emotional or ethical dilemmas using human judgments. However, important gaps remain in understanding how factors such as participant demographics, and personality traits influence preference rankings, and what linguistic features drive these differences. Our work addresses this gap by examining how user demographics and empathy levels shape preferences for LLM-generated responses to dilemmas.\n\nWe conduct a large-scale empirical study examining how user preferences for LLM responses to emotional dilemmas are shaped by self-reported demographic variables—including age, gender, education, country of origin, and frequency of AI use—as well as by individual empathy levels. To capture empathy more precisely, we use the Basic Empathy Scale, which distinguishes between cognitive and affective components. Participants evaluated pairs of LLM responses to emotional dilemmas, drawn from an extended version of the EmoBench dataset. We aggregated these pairwise comparisons to generate overall model rankings and examined how these rankings varied across demographic and empathy-based subgroups. To understand the linguistic factors driving these preferences, we conducted both quantitative and qualitative analyses of model outputs. We extracted interpretable features using tools such as LIWC (for cognitive and affective language markers) and connotation frames (to assess power and agency dynamics).\n\nOur findings demonstrate that both empathy levels and demographic factors significantly influence human preferences for LLM responses to emotional dilemmas. Notably, we observe substantial variance in model rankings across subgroups, with preferences for certain models—such as LLaMA-3-70B and Qwen1.5-110B—differing sharply depending on users’ empathy profiles and demographic characteristics. Further linguistic analysis of the model outputs reveals corresponding differences in features such as agency, emotional tone, and cognitive framing. While we do not explicitly model linguistic predictors of human preferences, these patterns suggest that certain features—such as power verbs, modal expressions, or emotionally expressive language—may serve as markers that resonate more strongly with particular populations. For example, models favored by high-affective-empathy users often used language that emphasized emotional validation and support, while those preferred by low-empathy users adopted a more direct, goal-focused tone.\nOur findings have important implications for both LLM evaluation and development. They highlight the limitations of relying solely on aggregate human judgments, which risk obscuring meaningful variation across user groups. Instead, our results underscore the value of demographically stratified evaluation frameworks that can better capture diverse user expectations. Moreover, they suggest that LLMs might benefit from adaptation not only for task-specific performance but also for stylistic alignment with varied emotional needs. As language models increasingly shape subjective decisions in contexts like mental health, education, and civic discourse, designing systems that account for this diversity is essential to building equitable, inclusive, and trustworthy language technologies."
            },
            {
              "id": 37,
              "title": "Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation",
              "authors": "Chiara Manna, Afra Alishahi, Frédéric Blain, Eva Vanmassenhove",
              "affiliation": "Tilburg University",
              "abstract": "While gender bias in modern Neural Machine Translation (NMT) systems has received much attention in the last decade, traditional evaluation metrics often fail to fully capture the extent to which these systems integrate contextual gender cues. We propose a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures the extent to which MT models integrate and rely on gender cues for gender disambiguation. MPA is designed to go beyond surface-level gender accuracy metrics by focusing on whether models adapt to gender cues in minimal pairs -- sentence pairs that differ solely in the gendered pronoun, namely the explicit indicator of the target’s entity gender in the source language (EN). We evaluate a number of NMT models on the English-Italian (EN–IT) language pair using this metric, we show that they ignore available gender cues in most cases in favor of (statistical) stereotypical gender interpretation. We further show that in anti-stereotypical cases, these models tend to more consistently take masculine gender cues into account while ignoring the feminine cues. Furthermore, we analyze the attention head weights in the encoder component and show that while all models encode gender information to some extent, masculine cues elicit a more diffused response compared to the more concentrated and specialized responses to feminine gender cues. This discrepancy highlights a fundamental asymmetry in how gender cues are handled. Overall, our work provides new insights into the limitations of modern NMT systems. It demonstrates that they frequently ignore contextual gender information in favor of stereotypical patterns. Furthermore, we reveal areas where intervention could be explored to reduce gender bias in machine translation. While we target the English-Italian (EN–IT) language pair, the framework is broadly applicable to any setting where gender must be explicitly marked in the target language."
            },
            {
              "id": 61,
              "title": "Unveiling Gender Bias: Transformer Models and Explainability Techniques in Dutch Job Ad Analysis",
              "authors": "Megi Dragoti, Merle Beaujon",
              "affiliation": "Tilburg University, TNO",
              "abstract": "Biased language in recruitment remains a structural barrier to inclusive labor markets, filtering out qualified candidates before they even apply. Although the Dutch Equal Treatment Act prohibits discrimination, biased phrasing in job advertisements continues to shape hiring outcomes. Existing tools such as Kat Matfield’s Gender Decoder and Omdena’s gender bias detection demo focus on English-language recruitment texts, leaving a gap in bias detection tools tailored to Dutch. This study addresses that gap by building on the dataset and initial discrimination detection work by Vethman et al. (2022), expanding it with additional transformer-based models and explainability techniques (SHAP and LIME) to develop a robust, interpretable pipeline for identifying explicit gender bias in Dutch job advertisements.\nWe fine-tuned three transformer models—two Dutch (BERTje and RobBERT) and one multilingual (XLM-RoBERTa)—on a curated dataset of 5,947 Dutch job ad sentences annotated by domain experts from Dutch labor and human rights authorities (NLA, UWV, NIHR). The dataset was constructed using a baseline method that flagged potentially biased content based on legally informed gender-related terms from expert interviews and court rulings. Five experts annotated each sentence using a three-point scale: Highly Suspected Discrimination (HSD), Not Discriminatory, or Uncertain. Only HSD and non-HSD cases (28.8% vs. 71.2%) were retained. Sentence-level annotation balanced contextual clarity with efficiency. Inter-annotator agreement on a stratified 600-sentence subset yielded a Fleiss’ Kappa of 0.557, with stronger consensus on HSD cases (κ = 0.602).\nModel performance was evaluated using a stratified 70/30 train/test split. All models were fine-tuned via Optuna hyperparameter optimization and 3-fold cross-validation. Evaluation metrics included Average Precision (AP) and ROC AUC, appropriate for imbalanced binary classification. To assess data efficiency, we performed a learning curve analysis using repeated 10-fold cross-validation on incrementally larger training subsets. Finally, we applied SHAP and LIME to interpret model outputs, offering both global and instance-level insight into lexical cues driving classification.\nAll models achieved ROC AUC scores above 93%, confirming their ability to distinguish biased from neutral phrasing. XLM-RoBERTa reached the highest AP score of 86.5%, exceeding the prior benchmark of 83.2% by +3.3%. RobBERT, a Dutch-specific model, demonstrated the most consistent performance across training sizes and remained effective with limited data. Learning curves showed diminishing returns beyond 70–80% of the training data, suggesting efficient deployment is possible with modest annotation budgets.\nSHAP and LIME analyses surfaced key lexical indicators such as “vrouwelijk” (female), “jongeman” (young man), and “mevrouw” (madam), revealing both systemic patterns and edge cases. These tools enhance model transparency and offer practical value for auditing and regulation.\nThis study contributes a scalable, interpretable pipeline for detecting explicit gender bias in Dutch job ads. By comparing monolingual and multilingual models and embedding interpretability throughout, it bridges methodological rigor with applied fairness concerns, supporting both NLP development and automated legal compliance."
            },
	      {
		  "id": 78,
		  "title": "Form and Meaning in BERT Embeddings: Analyzing the verbal 'te'-infinitival complement clause with optional complementizer 'om' in Dutch and multilingual BERT models",
		  "authors": "Marije Kouyzer, Jelke Bloem",
		  "affiliation": "University of Amsterdam",
		  "abstract": "This paper researches how linguistic constructions that have the same meaning but a different form are encoded in the output of Dutch and multilingual BERT models, compared to how linguistic constructions with both a different form and meaning are encoded. The main linguistic construction that is researched is the verbal 'te'-infinitival complement clause with and without the optional complementizer 'om'. These constructions have the same meaning but a different form. A linguistic construction with both a different form and meaning is also examined for comparison. This construction includes the word 'niet', meaning 'not', and the verbal 'te'-infinitival complement clause without the complementizer 'om'. Sentences with these three linguistic construction are extracted from the Lassy Groot corpus. Four Dutch and multilingual BERT models, RobBERT-2023, BERTje, EuroBERT and mBERT, are used to construct sentence embeddings for these sentences. To compare the cluster coherence of the different linguistic constructions, the internal cluster analysis methods Silhouette Score and Davies-Bouldin Index are used. The data is divided into two groups, one that differs only in form and one that differs in both meaning and form. The cluster coherence of these groups are compared and the effects of sentence length and the different BERT models is examined. Overall the clusters had a lot of overlap, but a better cluster coherence was found in the group that contained sentences that differed in meaning as well as form than in the group that only differed in meaning. Cluster coherence was also better for longer sentences than it was for shorter sentences. The monolingual Dutch models were better at separating the constructions than the multilingual models. RobBERT-2023 performed best, suggesting a potential advantage for models based on the RoBERTa architecture when it comes to distinguishing linguistic constructions."
	      },
            {
              "id": 82,
              "title": "Value Preferences in Multilingual LLMs: Exploring Effects of Language and Prompt Design",
              "authors": "Kathelijne Wendt, Erxun Liao",
              "affiliation": "Leiden University",
              "abstract": "Identifying the value preferences of large language models (LLMs) is critical for understanding their decision-making behavior in culturally sensitive contexts (e.g., Santurkar et al., 2023; Jin et al., 2022). This study presents a partial replication and extension of Chiu et al. (2025), who introduced the DailyDilemmas dataset to assess value preferences in LLMs via everyday dilemmas. Each of these dilemmas offers two actions – “to do” or “not to do” – associated with human values such as trust, honesty, and transparency.\n\nWhile the original study focused on English and a broad set of values, we narrowed the scope to the self-expression vs. survival dimension, following the framework of the World Values Survey (WVSA, 2023). The investigated subset of DailyDilemmas consists of 34 dilemmas in which the two actions are aligned with either self-expression or survival. Eight OpenAI models (ranging from GPT-3.5-turbo to GPT-4.1-nano) were evaluated across four languages: English, Dutch, German, and Chinese. This multilingual approach allowed us to examine to what extent value alignment is culturally sensitive or shaped by model training.\n\nIn line with Chiu et al.’s findings, we observe that all tested models prefer self-expression over survival in English. Crucially, this preference generalizes to all tested languages – including Chinese, even though Chinese society does not show strong preference for either value dimension (WVSA, 2023). The models’ cross-linguistically uniform behavior points to a potential Anglocentric value alignment, likely reflecting the influence of predominantly Western training data. We observe that smaller models exhibit weaker preferences, suggesting that model scale plays a role in the internalization of such biases.\n\nInterestingly, when we alter the instruction prompt by reversing the order of the two actions (from “to do – not to do” to “not to do – to do”), the bias toward self-expression largely disappears across all models and languages (with the exception of GPT-3.5-turbo in English). This indicates a high sensitivity to prompt formulation, even when the logical content of the prompt remains unchanged.\n\nOur study underscores the importance of multilingual, prompt-sensitive evaluations in assessing LLMs’ value alignment and their capacity for culturally grounded reasoning.\n\n\nReferences\n\nChiu, Y. Y., Jiang, L., & Choi, Y. (2025). DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life. ArXiv. https://doi.org/10.48550/arXiv.2410.02683\n\nJin, Z., Levine, S., Gonzalez, F., Kamal, O., Sap, M., Sachan, M., Mihalcea, R., Tenenbaum, J.B., & Schölkopf, B. (2022). When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. ArXiv. https://doi.org/10.48550/arXiv.2210.01478\n\nSanturkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., & Hashimoto, T. (2023). Whose Opinions Do Language Models Reflect? ArXiv. https://doi.org/10.48550/arXiv.2303.17548\n\nWVSA. (2023). The Inglehart-Welzel World Cultural Map - World Values Survey 7. https://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=findings"
            }
          ]
        },
        {
          "id": 7,
          "name": "Multimodal AI & Accessibility",
          "description": "Multimodal systems, accessibility, and visual-linguistic tasks",
          "color": "#EC4899",
          "posters": [
            {
              "id": 1,
              "title": "Tailoring LLM-generated image captions to user needs",
              "authors": "Ivonne van der Heiden, Emiel van Miltenburg",
              "affiliation": "Tilburg University",
              "abstract": "In the last decade, image captioning technology has grown from a toy task to a helpful tool that already helps thousands of blind or visually impaired users worldwide. Where researchers originally treated image captioning as a conditional generation task, where single-purpose models generated one-size-fits-all captions, we now have access to multilingual multimodal language models, that can (at least in theory) generate output that is tailored to our individual needs. We put this idea to the test by having different LLMs generate Dutch image captions that vary in their length (short versus long) and their literacy level (regular versus low literacy). In our study, we compare a popular commercial model (ChatGPT) with different open weights models that can be run on a regular laptop (e.g., LLaVA v1.5 7B; Qwen2.5-VL 7B). We discuss the variation in output quality both between the different models and between the different experimental conditions, and conclude that while current models still generate sub-optimal captions, they can be used to speed up the image description process by generating a first draft that can be improved by human authors."
            },
            {
              "id": 8,
              "title": "AI-assisted interlinear glossing of a low-resource language: A comparative evaluation of NotebookLM and ChatGPT",
              "authors": "Julia Heemskerk",
              "affiliation": "Utrecht University, ILS",
              "abstract": "This study compared ChatGPT and NotebookLM in generating interlinear glosses for Frisian, a low-resource language in the Netherlands. These two Large Language Models (LLMs) differ in their approaches: while NotebookLM uses user-provided documents in a restricted, retrieval-based environment, ChatGPT is a general-purpose LLM trained on a broad corpus, including internet data. Documenting low-resource languages is often time-consuming and urgent, but these same settings pose challenges for AI systems because they rely on large-scale training data to perform well (Joshi et al., 2020). Therefore, Frisian offers a compelling test case: it is underrepresented in most AI corpora despite the availability of high-quality grammatical resources online. This study further builds on recent work on AI-assisted glossing for low-resource languages (e.g., Ginn et al., 2024; Elsner & Liu, 2025). Moreover, supporting the digital documentation of Frisian contributed to its preservation. \n\nIn a pilot study, I prompted both models with 40 Frisian sentences, which were chosen from Frisian Wikipedia (Wikipedia, 2024). This source was chosen for its public accessibility, consistent register, and variety of sentence structures. A document based on these sentences was prompted, and each sentence had a four-line format (Frisian – Frisian morpheme gloss - English gloss – English translation), designed to reflect standard linguistic practice. Additionally, a document was provided that included a comprehensive conjugation paradigm for weak and strong verbs, noun and adjective inflection, and syntactic rules, which were extracted from Frysker.nl and the Taalportaal (Frysker, 2024; Taalportaal, 2024). The input included two texts of different types: a formal historical passage and an informal narrative excerpt that was prompted to be glossed by the models. The models were evaluated on segmentation accuracy, morphological label correctness, and formatting consistency. While ChatGPT produced a slightly higher accuracy score (90.7%) than NotebookLM (86.3%), a repeated-measures ANOVA showed no significant difference. However, error analysis revealed patterns in the types of errors made by each model. ChatGPT frequently made segmentation and labeling errors, whereas NotebookLM tended to overannotate and produce semantic mislabels. These findings show that when given structured input, LLMs can assist with glossing tasks. However, human oversight is still required when glossing Frisian. It also shows the importance of prompt engineering, linguistic grounding, and dataset quality when using AI models for morphosyntactic annotation tasks.\n\nThe aim of the follow-up research will be whether these findings can be generalized across sentence types, on the one hand, and to what extent training on user-specific linguistic resources can improve model reliability, on the other hand. Ultimately, the results may inform best practices for using AI in linguistic fieldwork and contribute to the preservation of low-resource and minority languages such as Frisian. To support this, the current dataset of the pilot study will be expanded up to 100 Frisian sentences. This will contain more lexical and syntactic diversity while also having a more robust statistical analysis. A comparison between the pilot and expanded datasets will also be done to research the importance of prompt structure and relevant examples in low-resource glossing as in the research of Elsner & Liu (2025)."
            },
            {
              "id": 22,
              "title": "Challenging Visual Language Models to Explain Misinformation",
              "authors": "Katja Kamyshanova, Tommaso Caselli",
              "affiliation": "CLCG - University of Groningen",
              "abstract": "In the past 10 years, the Internet has radically changed moving away from being an only-text platform to an increasingly image- and video-oriented one. This has created new opportunities for the spread of misinformation with multi-modal instances becoming more frequent than ever. Multi-modal misinformation is harder to detect than its text-only counterpart as its misleading nature stems from the interplay between modalities. Effective detection requires real-world knowledge, reasoning, and cross-modal understanding functionalities that uni-modal architectures struggle to support.\n\nPrevious work has concatenated textual and visual embeddings, applied attention mechanisms, or used transformer models with external evidence to address the task. These methods, however, lack understanding of multi-modal connections and a knowledge base large enough to detect deceiving content. The recent development of Large Visual Language Models (LVLMs), with their end-to-end architectures trained specifically on multi-modal tasks, have already shown strong performance on several benchmarks. However, their application to  multi-modal misinformation detection is still at an early stage. \n\nIn this study, we analyze by means of different prompt combinations and experiment settings (zero-shot and few-shot) how well LVLMs can detect misinformation and how well they can explain it. We use MiRAGeNews, a multi-modal misinformation dataset of 15,000 image-caption pairs, including real entries from major English news outlets (New York Times, CNN, BBC) and fake entries with images generated with diffusion models (Midjourney, DALL·E 3, SDXL) based on GPT-4 text captions. Each sample contains an image, caption, and a label. This dataset proved to be challenging to the human evaluators, with only 60% average accuracy among 112 participants. For our experiments, we selected three LVLMs: LLaVA-v1.6-Mistral-7B, Qwen2-VL-7B-Instruct, and Idefics2-8B-Llama3. All these models support few-shot settings, have a comparable number of parameters (ranging between 7 and 8 billions), are open-weight, trained on open-access datasets, and are computationally efficient. Each model utilizes a unique training strategy and visual data processing techniques, and features different backbone LLMs, creating an interesting comparison between distinct architectures and model families.\n\nWe frame our task as follows: LVLMs are prompted to classify entries from MiRAGeNews as true or fake and explain their decision. In the few-shot setting, models received labeled examples with or without manually written explanations. We experimented with a maximum of six shots in jumps of two. LLaVA-v1.6-Mistral-7B (six-shot + explanations) qualifies as the best model, with a macro-F1 of 0.7108 - followed by the same model with a different example order (six-shot + explanations) (0.7107) and its four-shot variant (four-shot + explanation) (0.7091). For LLaVA-v1.6-Mistral-7B, we also evaluate the quality of the generated explanations by assessing their alignment with the classification labels along the following metrics: BLEU, ROUGE-L, and BertScore for text similarity with the input; specificity, abstraction, and negation for model explanations. This evaluation phase is ongoing."
            },
            {
              "id": 26,
              "title": "Seq2Seq Models for AAC Pictogram-to-Text Translation: Input Representation and Decoding Strategy Evaluation",
              "authors": "Robin Kokot, Vincent Vandeghinste",
              "affiliation": "KU Leuven",
              "abstract": "We present a survey of neural methods for modeling pictogram-to-text translation as a natural language sequence-to-sequence generation task. We evaluate four input representation strategies: direct pictogram IDs, ARASAAC keywords, rule-processed tokens, and hybrid combinations. Our experiments compare three encoder-decoder architectures (BARThez, French T5, and mT5) across different generation strategies, including greedy decoding, beam search, and nucleus sampling. Focusing on French as a target language, we determine optimal configurations for AAC applications and report results on automatic metrics (BLEU, ROUGE-L, WER) and lexical diversity and fluency measures. Our findings demonstrate that input representation design critically impacts generation quality, with text-based approaches achieving superior performance over hybrid and direct configurations. While decoding strategy choice affects quality less, we find distinct patterns: French-tuned models benefit most from greedy search, whereas multilingual models show more robust performance across all strategies. Moreover, we find that multilingual models (mT5) achieve performance on par with French-tuned models within 2-3 BLEU points, supporting the application of cross-lingual transfer for pictogram-based generation systems. As anticipated, hybrid approaches combining pictogram IDs with keywords underperform pure text-based methods, suggesting that semantic scaffolding through metadata is more effective than symbolic fine-tuning for seq2seq architectures."
            },
            {
              "id": 44,
              "title": "Uncovering under-represented perspectives from cultural heritage data using a neuro-symbolic approach",
              "authors": "Ruhi Mahadeshwar, Tommaso Caselli, Andreas van Cranenburgh, Malvina Nissim",
              "affiliation": "University of Groningen",
              "abstract": "Reconstruction of the past happens via evidence, which can be in multiple forms such as historical newspapers, autobiographical writings and literary books. Cultural Heritage (CH) institutions offer a considerable amount of such data in digitized formats. This data serves as recorded history, corresponding to events which occur through time. Accessing and processing this data makes it possible to better understand past times. Coupling CH data with artificial intelligence (AI) methods through large language models (LLMs) gives rise to amazing opportunities when reconstructing the past from different perspectives. But, recorded history is not always impartial. Perspectives are often excluded due to biases, politics and religion. When trying to uncover such under-represented perspectives, often corresponding to marginalized communities, the related CH data is sparse and has a long-tail distribution. This makes uncovering such perspectives complicated. However, exploring multiple perspectives can be valuable in many ways: develop a more critical understanding of the past, understand the roots of modern sociocultural inequalities, and restore misinterpreted and incorrect stories.\n\nRecent advances in AI show that it is possible for pre-trained LLMs to analyze features and extract information encoded in small and sparse amounts of textual data. This is done through transfer learning techniques, such as zero-shot and few-shot, and knowledge injection through knowledge graphs from various sources to prevent hallucination. We outline an approach to uncover under-represented perspectives in given sparse CH data: a neuro-symbolic, multi-turn question-answer (QA) approach constructed through transfer learning, natural language processing and knowledge representation techniques. The truthfulness of these uncovered perspectives is preserved by providing explicit knowledge representation and improving data recall while reasoning is made more coherent through multi-turn QA. Further research can also be done to evaluate how historically valid these uncovered perspectives are by conducting qualitative analyses involving experts in the CH domain."
            },
            {
              "id": 49,
              "title": "Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?",
              "authors": "Anna Bavaresco, Raquel Fernández",
              "affiliation": "University of Amsterdam",
              "abstract": "A common assumption in AI is that multimodal models learn language in a more human-like way than language-only models. Due to their training on data from extra-linguistic modalities (e.g., images or audio), they are thought to reflect some semantic aspects that cannot be learnt from text alone. However, there is little to no work investigating which these semantic aspects are. Here, we aim to fill this gap by addressing the following question: Do recent multimodal models learn some facets of meaning related to perceptual experiences that language-only models cannot capture?\n\nTo approach this question, we operationalise the ‘extra-linguistic’ information that multimodal models allegedly learn using an existing norm-based semantic model—Exp48—which has been shown to significantly align with human brain activity during work processing. Our main experiment aims at assessing whether Exp48’s experiential information is reflected in word representations yielded by language-only and multimodal (vision-language and audio-language) models, and whether this results in improved brain alignment.\n\nWe analyse a set of 320 nouns, spanning objects and events. We derive representations for these nouns using a set of transformer-based models: SimCSE (language-only), MCSE (vision-language), and CLAP (audio-language); those were fine-tuned with similar, contrastive learning objectives, and share the same BERT-based architecture. For reference, we also evaluate BERT and its vision-language extension VisualBERT. To derive word representations, we embed them in neutral sentence templates and feed them to the language encoder of the models; we then extract the hidden states corresponding to the target nouns. Through representational similarity analysis, we evaluate the alignment between models’ word representations and word representations by Exp48 (n-dimensional arrays, where each entry corresponds to aggregated human ratings on an experiential dimension, e.g., Colour or Sound). We apply the same procedure to compute the alignment between model representations and brain responses, i.e., existing fMRI recordings collected while 36 participants viewed word stimuli.\n\nOur results reveal an unexpected trend: Representations by language-only models reflect more experiential information and are more brain-aligned than those by the multimodal models. More specifically, the vision-language models VisualBERT and SimCSE exhibit moderate alignment with both Exp48 and fMRI responses, while CLAP does not correlate significantly with any. Additionally, a partial correlation analysis reveals that the language-only models reflect further brain-relevant information that is not shared with Exp48.  \n\nWe assess the robustness of our findings by conducting two follow-up analyses. The first concerns the impact of the sentence templates and consists in including the word stimuli in different templates with a structure more similar to image captions. The second aims at inspecting differences between more concrete vs. abstract word stimuli. The results from both analyses confirmed the superior brain alignment of language-only models, independent of the templates or words considered.\n\nAltogether, our study invites caution against assuming that multimodal models are necessarily more human-like than language-only ones, and indicates that there is significant room for improving current computational language models so that they learn the brain-relevant experiential information they currently lack—how to concretely achieve this remains an open question."
            },
            {
              "id": 68,
              "title": "Genericity as (in)coherence in Synthetic Text",
              "authors": "Martial Pastor, Søren Fomsgaard",
              "affiliation": "Radboud University, UNICAEN, ENSICAEN, CNRS, GREYC, Normandie Univ",
              "abstract": "LLMs seem to always give us the information we want, magically offering the\nhighly coveted ideal response, although their responses are often factually incor-\nrect, or their references are hallucinated. Still, we continue to use them because\nthey present the information we seek with confidence, even when the model itself\n‘knows’ that it is wrong (Azaria & Mitchell, 2023) or in the absence of evidence.\nIn this sense, the uncritical use of LLMs poses a threat to the quality and integrity\nof our information environment. We explore whether LLMs create the illusion of\ncorrectness (thereby encouraging their continued use) through a reliance on generic\nlanguage. We hypothesize that genericity can be modeled through the use of gener-\nics, i.e. utterances that feature generalizing statements about categories and kinds\n(Leslie & Gelman, 2012), which in turn play a role in (over-)generalizations and\nstereotyping (Bosse, 2024) for example in ethnoracial, class and gender categories\n(Mannheim, 2021) or in scientific discourse and communication (Sumner et al.,\n2016). More specifically, we would like to examine how the overuse of generics\naffects discourse coherence, as a way to investigate how the reader processes the\ntext. In this article, we tackle the question whether LLM-generated text exhibits a\nhigher degree of genericity than their human-written counterparts through the lense\nof generics. We first assess various ways to measure genericity by way of discrete\nlinguistic features proposed in the literature, using two common machine-generated\ntext benchmark datasets. Then, we use discourse parsers to analyze ’highly generic’\ntexts and provide an account of key discourse patterns, with the aim of character-\nizing how they differ from more argumentative or informational texts. Finally, we\ndiscuss insights into discourse strategies that rely on vagueness, such as generics,\nand their impacts on their audience as a powerful tool for persuasion (Cimpian\net al., 2010), which we argue implicitly contributes to the convincing nature of the\nanswers given by LLMs."
            },
	      {
		  "id": 15,
		  "title": "A Distributional Semantic Model of Comprehension Plausibility in Persian Compound Nouns",
		  "authors": "Mihan Mohagheghzadeh, Shahla Raghibdoust",
		  "affiliation": "Saarland University, Allameh Tabataba'i University",
		  "abstract": "This study models the concept of comprehension plausibility in Persian noun-noun compounds using a computational framework that combines semantic vector representations with neural learning. While plausibility has been modelled in English and other languages, its application to Persian, a morphologically rich language, remains underexplored. Drawing from cognitive linguistics, the study conceptualizes plausibility as a graded cognitive judgment shaped by semantic compatibility, prior knowledge, and conceptual integration.\nTo operationalize this framework, the study adopts a distributional semantics approach grounded in the distributional hypothesis, which holds that words appearing in similar contexts tend to have similar meanings. A normalized corpus of over one million tokens from the Hamshahri corpus was used to construct the semantic space. From this corpus, 250 high-frequency content words (nouns, adjectives, and adverbs) were selected and filtered based on grammatical and topical relevance. Their contextual co-occurrence patterns were used to build a distributional matrix and generate semantic vectors.\nIn parallel, over 400 attested compound nouns, all morphologically noun-noun, were collected from authoritative Persian lexical resources. These compounds were analyzed to extract morphosyntactic and semantic patterns that defined well-formedness constraints and informed the design of novel test items.\nTo extend the semantic modelling framework, the study algorithmically constructed 100 semantically novel noun-noun compounds by recombining high-frequency nouns from the corpus that had never previously co-occurred. From this set, 25 compounds were selected to serve as test items in a human evaluation phase. Using a custom-built web interface, 82 native Persian speakers rated the plausibility of each compound, providing the graded evaluation used to train and assess the predictive model.\nA custom positional-weighted function computed the semantic vector for each constituent by emphasizing nearer co-occurrences. These vectors were used as input to a feedforward neural network with two hidden layers, trained to predict plausibility scores. Training data included 265 attested compounds derived from combinations involving the high-frequency words.\nThe model achieved 77.36% agreement with human evaluations, confirming that semantic proximity and conceptual integration play central roles in plausibility assessment. These findings demonstrate that comprehension plausibility in Persian can be computationally predicted with high accuracy, offering both theoretical insight and practical tools for semantic evaluation in natural language processing."
	      },
	      {
		  "id": 53,
		  "title": "Automatic Animacy Classification for Latvian Nouns",
		  "authors": "Ralfs Brutāns, Jelke Bloem",
		  "affiliation": "Universiteit van Amsterdam, Vrije Universiteit Amsterdam",
		  "abstract": "We present the first automatic animacy classifier for Latvian, a morphologically rich yet under-resourced language in the field of natural language processing. Animacy, the linguistic property distinguishing living from non-living referents, plays a role in Latvian grammar, affecting syntactic behavior such as word order and case marking. Despite its importance, no computational approaches to animacy classification have been developed for Latvian to date. To address this gap, we propose a type-based classifier that assigns animacy labels—human or non-human—to Latvian noun types. Previous work uses language-specific WordNet, a lexical resource consisting of synonym sets linked with each other by semantic and lexical means. However, due to the limited coverage and incomplete hierarchy in Latvian WordNet, we devised a cross-lingual approach to construct animacy-labeled training data. Specifically, we extracted human and non-human noun lists from the English, Japanese, and Lithuanian WordNets via hypernymy relations and projected these labels onto Latvian nouns using automatic translation of entries from the Tēzaurs online dictionary. Nouns receiving consistent labels across all three WordNets were used to build a training set. We extracted word vectors from both static fastText embeddings and contextual LVBERT embeddings. More specifically, we used both layer 0 embeddings from LVBERT, which mimic static embeddings, and a concatenation of the last four contextual layers of LVBERT for classifier training. Finally, we trained our classifiers using the K-Nearest Neighbors, Random Forest, and Multi-Layer Perceptron algorithms. All models were evaluated using a type-based split as well as a token-based evaluation on manually annotated Wikipedia data. The best performance was achieved by the LVBERT-based MLP classifier using concatenated final-layer embeddings, reaching 91.6% accuracy (in a type-based evaluation) and 93.8% (in a token-based evaluation), outperforming baselines and prior work on comparable languages. To validate our projection method, we trained additional classifiers using animacy labels derived from single WordNets and found that Lithuanian WordNet alone achieved nearly comparable performance to the multi-WordNet approach, suggesting that typological proximity of a language may outweigh resource size in some low-resource settings. This work demonstrates the feasibility of animacy classification for a low-resource language without a fully developed WordNet. Our method enables scalable creation of animacy-labeled datasets using multilingual lexical resources and translation and shows that type-based training with contextual embeddings can yield robust performance in a token-based task. Beyond animacy, this cross-lingual projection approach may generalize to other semantic classification tasks in under-resourced languages. The developed classifier could provide a basis for implementation in downstream NLP applications such as coreference resolution, contributing to the broader goal of Latvian language modernization in digital contexts."
	      }
          ]
        },
        {
          "id": 8,
          "name": "Digital Humanities & Language Resources",
          "description": "Infrastructure, resources, and digital humanities applications",
          "color": "#84CC16",
            "posters": [
            {
              "id": 18,
              "title": "Adapting and Evaluating Entity Linking for Dutch",
              "authors": "Flor Alberts, Collin Krooneman, Gosse Bouma",
              "affiliation": "University of Groningen",
              "abstract": "Entity Linking (EL) is the task of identifying entity mentions in unstructured text and linking\nthem to entries in a structured knowledge base. EL plays a crucial role in various downstream NLP tasks, including question answering, relation extraction, and biomedical text mining. Despite rapid advancements in recent years, EL remains a challenging task, especially for languages other than English, and in multilingual settings. In particular, while there is no shortage of evaluation data for Dutch, and a number of multilingual EL systems exist, no evaluation results are known for state-of-the-art  EL systems on Dutch. Furthermore,, there is no recent monolingual EL system for Dutch. This paper aims to fill those gaps.\n\nAs evaluation resources we use the Dutch language sections of WiNNL (Cartier and Peetermans, 2024), DaMuEL (Kubeša and Straka, 2023), and MultiNERD  (Tedeschi and Navigli, 2022). All these contain text (newswire and Wikipedia) annotated with named entities and their corresponding (non-ambiguous) Wikidata QID. Although WiNNL was manually annotated, we still found many links to non-existing QIDs. Where possible, we replaced these with links to the correct Wikidata QID. DaMuEL and MultiNerd are both derived from Wikipedia text, where internal Wikipedia links are automatically replaced by Wikidata QIDs. A difference between the three datasets is the scope of the entities for which QIDs are provided. WiNNL primarily (over 90%) provides links for persons, locations, and organisations. The other two datasets provide QIDs for named entities, but also for concepts (animals, diseases, plants, food, etc.) and dates. The latter are problematic for automatic EL systems that rely on named entity recognition as a first step, as dates and (lower cased) concepts are normally not considered to be named entities.  In our evaluation, we report on precision and recall for named entities (ie mostly persons, locations, organisations) only. \n\nWe evaluate the performance of  two baseline systems  ( SpaCy + Wikidata API and WikiNeural (Tedeschi et al., 2021) + Wikidata API). Where Spacy does NEC only, WikiNeural also generates a possible Wikipedia title.  The recognized entities or their Wikipedia title are sent to the Wikidata API for disambiguation. We evaluated two state-of-the-art multilingual entity linkers: mGENRE  (De Cao et al., 2022) and BELA (Plekhanov et al., 2023).  SpEL (Shavarani and Sarkar, 2023) is a monolingual EL system trained for English. We retrained it using Dutch Wikipedia as well as the Dutch section of MultiNERD to obtain a monolingual EL system for Dutch. \n\nOur preliminary evaluation results indicate that mGENRE and BELA generally outperform the baseline systems, but that in some cases high recall comes with lower precision. The DaMuEL data turns out to be challenging, as it contains many long texts. If gold standard named entities are given, and the task is restricted to enity disambiguation (ED) only, both precision and recall improve significantly, indicating that one of the major reasons for lower performance on the end-to-end task is lower accuracy of named entity recognition."
            },
            {
              "id": 28,
              "title": "The Common Crawl Creative Commons Corpus (C5)",
              "authors": "Bram Vanroy",
              "affiliation": "",
              "abstract": "In the development of large language models (LLMs), the availability and quality of training data are the fundamental starting point. And yet, while large-scale web-crawled corpora have become standard, their use raises significant legal and ethical concerns, particularly regarding copyright and licensing. To address this, I introduce the Common Crawl Creative Commons Corpus (C5), a novel dataset derived from Common Crawl that focuses explicitly on annotating Creative Commons licensed content. Unlike prior efforts that rely on string-based pattern matching (e.g., common-pile/dolma-cccc, C4Corpus), C5 employs a parsing approach, enabling fine-grained extraction of licensing metadata and contextual information from the document structure. While the extraction process is computationally intensive compared to regex-based methods, it yields significantly more interpretable and filterable results. False positives may still occur, but the inclusion of structural metadata enables downstream filtering strategies to mitigate this.\n\nThe automatic annotations include the type and version of the license (e.g., CC-BY 4.0, CC0) and the HTML element in which it was found (e.g., meta, link, a, json-ld) and its position within the document (head, body, footer). This structural metadata is valuable for assessing the reliability of license claims, as licenses embedded in meta tags within the head are generally more trustworthy than hyperlinks in the body. When multiple licenses are available for a document, the dataset includes a \"best guess\" license field, derived from prioritizing licenses' tag type and document location. Potential conflicts in terms of type (e.g. CC-BY) between multiple licenses on a single page are also flagged. Documents without annotated licenses are not included in the published dataset.\n\nC5 covers eight languages (Afrikaans, German, English, French, Frysian, Italian, Dutch and Spanish). Because the dataset is small, no deduplication or further quality filtering has been done. This deliberate choice ensures that other researchers can decide for themselves how strict further filtering is done depending on the requirements of the project or the volume that is needed. However, to give an indication of whether a document is of high quality, an additional field describes whether or not the document is present in the pre-filtered, large-scale FineWeb(-2) datasets.\n\nC5 and its accompanying codebase are open-source and designed to foster transparency and potentially a step towards legal clarity in LLM training data. This work was supported by TNO and the Flemish Supercomputer Centrum."
            },
	      {
		  "id": 36,
		  "title": "Automated Text Simplification Evaluation Metric In Dutch",
		  "authors": "Xiaoyue Zhao, Vincent Vandeghinste",
		  "affiliation": "KU Leuven",
		  "abstract": "Text Simplification refers to a natural language processing task that aims to transform complex input into more accessible and understandable text while preserving the core semantics. A key challenge in this task lies in developing an evaluation metric that can approximate human judgment in accurately assessing the quality of simplified output.\n\nThis study proposes and rigorously evaluates a novel metric for text simplification in Dutch. Our investigation is structured around three key experiments. In experiment 1, we train the metric using a pre-trained encoder-decoder model (T5) on a human-annotated dataset with scores for accuracy, fluency, and complexity. We assume that if the proposed metric exhibits a higher correlation with human judgments, it would indicate its higher reliability over traditional metrics. In experiment 2, we conducted a comprehensive comparison with existing evaluation metrics. We calculated the correlation between the proposed metric and human evaluation scores, and compared its performance against widely used metrics including SARI, BLEU, and BERTScore. In experiment 3, we investigated the robustness and generalizability of the metric across different subsets of textual characteristics. The test set was divided into 3 subsets based on sentence length (short vs. long), lexical complexity (low vs. high, measured using Flesch-Kincaid scores), and edit distance (minor vs. major edits, based on Levenshtein distance). For each subset, we assessed the correlation between the metric's predicted scores and human score to evaluate its consistency across varying text characteristics.\n\nThrough these experiments, we aim to demonstrate that the proposed approach provides a more reliable and human-aligned metric for text simplification, thereby contributing to more effective development and evaluation of text simplification systems."
	      },
            {
              "id": 51,
              "title": "TermWerk. An integrated web application for Dutch terminology extraction and management.",
              "authors": "Kris Heylen, Mathieu Fannée, Ruud de Jong, Jesse de Does",
              "affiliation": "Dutch Language Institute",
              "abstract": "Accurate and consistent terminology is essential for effective communication within specialized domains. Terminology management systems support both domain specialists and communication professionals like translators in maintaining terminological consistency. Within these systems, termbases serve as a structured repository or database that contains domain-specific terminology along with associated information. The compilation of termbases typically proceeds in two steps. First, candidate terms are extracted from a domain-specific text collection or corpus using an automatic term extraction tool. In a second step, a selection of relevant terms is imported into a termbase editor where terminologists can organize them on the concept level and add definitions, translations, examples of contextual use, and other relevant information.  \nTypically, term extraction and termbase editing are implemented in separate applications. This was also the case for two applications, TermTreffer (van der Vliet 2015) and TermBeheerder, that were developed for Dutch by the Union for the Dutch Language (Taalunie), but that are no longer supported. In cooperation with the Taalunie, the Dutch Language Institute (INT -Instituut voor de Nederlandse Taal) has therefore developed TermWerk, a new, free and open-source web application that integrates automatic term extraction and termbase editing into a single environment and that allows multiple users to collaborate on joint terminology projects. The tool builds on top of the INT’s in-house developed BlackLab corpus management system (de Does et al. 2017) and therefore also offers fully-fledged corpus compilation and querying functionality. Additionally, users can also publish their  termbases online as publicly accessible and searchable web applications. \nTermWerk is developed and maintained by the INT in its role as Centre of Expertise for Dutch Terminology (ENT - Expertisecentrum Nederlandstalige Terminologie, Steurs 2021). Users can access TermWerk with the account of their own research institution via the CLARIN Single Sign On. Non-academic users can register for personal use. TermWerk is available at  https://termwerk.ivdnt.org/. \nReferences\nDoes, Jesse de, Jan Niestadt en Katrien Depuydt (2017), Creating research environments with BlackLab. In: Jan Odijk and Arjan van Hessen (eds.) CLARIN in the Low Countries, pp. 151-165. London: Ubiquity Press. DOI: https://doi.org/10.5334/bbi\nSteurs, Frieda. 2021. ‘Centre of Expertise for Dutch Terminology: A Digital Platform for Professional \nvan der Vliet, H. D. (2015). TermTreffer, a term extractor for Dutch. Poster at Computational Linguistics in the Netherlands 22, Antwerp."
            },
            {
              "id": 52,
              "title": "Woordpeiler: Visualizing and Analyzing Lexical Trends in Contemporary Dutch.",
              "authors": "Kris Heylen, Vincent Prins, Katrien Depuydt, Jesse de Does, Laura van Eerten, Thomas Haga",
              "affiliation": "Dutch Language Institute",
              "abstract": "Representative monitor corpora with detailed metadata offer a solid empirical basis for documenting lexical innovation and change. However, continuously updated time-stamped textual data presents challenges for data management, lexicographic analysis, and visualization. Building on its existing corpus infrastructure, the Dutch Language Institute (INT) has developed Woordpeiler (“Word Pollster”, https://woordpeiler.ivdnt.org/), an online application to (a) visualize and analyze word frequencies over time and (b) support the analysis of neologisms and lexical trends in Dutch since 2000.\nAs part of its mission to maintain a sustainable Dutch language infrastructure, INT developed the Corpus Hedendaags Nederlands (CHN), currently (April 2025) containing 4.3 billion tokens across 10.4 million documents. The corpus supports INT’s lexicographic workflow and is available via CLARIN. Daily and yearly data from major Dutch-language publishers (in the Netherlands, Belgium, Suriname, and the Dutch Caribbean) is processed via an automated workflow. All data is converted into a unified TEI format, enriched with metadata (e.g. language variety) and linguistic annotation. Using INT’s BlackLab system, the data is indexed and published as weekly (internal) or monthly (external) CHN updates.\nWhile CHN users could already obtain word frequencies through BlackLab’s query interface, Woordpeiler adds visualization and trend analysis tools. Frequency data for POS-tagged word forms, lemmas, and bigrams are exported to a PostgreSQL database optimized with TimeScaleDB. Through Woordpeiler’s interface, users can generate interactive graphs to visualize and compare changes in absolute and relative frequencies across customizable time intervals (day, week, month, year). Graphs can be filtered or split by language variety (Belgium, Netherlands, Suriname, Caribbean), with tooltips providing statistics and links to the underlying corpus data. INT-internal users can refine searches by lemma, part of speech, and newspaper, using operators similar to Google N-grams.\nA separate (currently internal) pane offers additional trend analyses. One function detects “trending” words or bigrams in a given interval using simple maths keyness (Kilgarriff 2009) relative to the preceding period. Users can adjust smoothing and also detect disappearing words via inverse keyness. A second function identifies new words or bigrams in a selected interval, optionally allowing earlier nonce occurrences. Results appear as sortable, POS-filtered lists with accompanying frequency graphs.\nWoordpeiler and its database are fully integrated into INT’s corpus-processing workflow, minimizing publication lags and ensuring quality control. The tool will support corpus-lexicographic work by adding validated frequency information to the central lexicon GiGaNT and improving workflows for identifying neologisms and out-of-dictionary words. Additionally, Woordpeiler serves science communication and outreach goals: it underpins a monthly and annual Woordpeiling (“Word Poll”) shared via INT’s website and social media, and is used in educational materials about language variation and change for secondary school students."
            },
            {
              "id": 58,
              "title": "Conventionality Produces More Robust, Learnable and Efficient Emergent Languages",
              "authors": "Jamie Wright, Lara Verheyen, Fabio De Ponte, Jerome Betoko Ekila, Katrien Beuls, Paul van Eecke",
              "affiliation": "Université de Namur, Vrije Universiteit Brussel",
              "abstract": "Human language is widely understood to be conventional: to express a certain meaning, language users expect a certain form to be used (Clark, 1993). The linguistic norms which guide this conventional use emerge from interactions between individuals in a population in a decentralised manner (Lewis, 1969; Steels, 1997, 2011). The wider literature suggests that languages evolve to be conventional because conventionality allows for certain important properties. These include robustness (Winter and Christiansen, 2012), in the sense that a conventional language is more predictable and is more likely to be effective in cases where there is loss of information, for learnability (Clark, 2009, 2010; Sabbagh and Henderson, 2007), in the sense that the smaller vocabulary and predictability of a conventional language will make it easier to learn, and because they require less cognitive effort to process (Regier et al., 2015; Kemp et al., 2018). However, there is little direct empirical evidence which supports these claims, due in part to the fact that unconventional natural languages with which to compare are non-existent. \n\nWe carry out a series of emergent language experiments with artificial agents designed to exhibit the effect of linguistic conventionality on robustness, learnability and in settings where cognitive load is limited. Languages that emerge within populations of artificial agents are not necessarily conventional as artificial emergent languages can communicate successfully even without converging on a fixed set of linguistic conventions. However, conventionality can be achieved in specific experimental settings. The experiments we carry out consist of single-utterance language games in which populations of autonomous agents can communicate to solve a referential task through decentralised interactions. This includes games in which there are discrete alternative convention spaces, where agents align form-referent pairings through repeated interactions, and those in which the convention space is continuous, where agents develop and align concepts to refer to features of the objects in the world.  In order to test the effect of conventionality on robustness, learnability and cognitive effort, we make a series of experimental interventions and test how the conventional and unconventional languages differ in their response to the intervention: i) to measure robustness, we introduce noise to the utterances once the language has converged in one case, and replace a portion of the population simulating population turnover in another; ii) to test the learnability of the language, we add a new agent after the language has converged, and measure how quickly they assimilate the language; iii) we test the effect of conventionality on cognitive effort by limiting the vocabulary size of the agents. We find that conventional emergent languages are more robust, more learnable, and are more likely to be effective if there are cognitive limitations, when compared with a language where linguistic convention has not been reached.\n\nReferences\n\nClark, E. V. (1993). The Lexicon in Acquisition. Cambridge University Press.\nClark, E. V. (2009). First Language Acquisition. Cambridge University Press\nClark, E. V. (2010). Learning a language the way it is: Conventionality and semantic domains. In Malt, B. C. and Wolff, P., editors, Words and the mind: How words capture human experience, pages 243–265. Oxford University Press.\nKemp, C., Xu, Y., and Regier, T. (2018). Semantic typology and efficient communication. Annual Review of Linguistics, 4(1):109–128.\nLewis, D. (1969). Convention: A Philosophical Study. Wiley-Blackwell, Oxford, United Kingdom.\nRegier, T., Kemp, C., and Kay, P. (2015). Word meanings across languages support efficient commu- nication. The handbook of language emergence, pages 237–263.\nSabbagh, M. A. and Henderson, A. M. (2007). How an appreciation of conventionality shapes early word learning. New Directions for Child and Adolescent Development, 2007(115):25–37.\nSteels, L. (1997). The synthetic modeling of language origins. Evolution of communication, 1(1):1–34.\nSteels, L. (2011). Modeling the cultural evolution of language. Physics of Life Reviews, 8(4):339–356.\nWinter, B. and Christiansen, M. H. (2012). Robustness as a design feature of speech communication. In The Evolution of Language, pages 384–391. World Scientific."
            },
            {
              "id": 59,
              "title": "Learning Through News: How Recommendation Strategies Shape Knowledge Retention in Dutch Readers",
              "authors": "Florian Debaene, Loic De Langhe, Orphée De Clercq, Véronique Hoste",
              "affiliation": "LT3, Ghent University",
              "abstract": "Being able to connect various mentions of people, objects and events in written and oral discourse is a fundamental aspect of human language understanding (Hobbs, 1979; Kehler 2006). Event coreference resolution (ECR) is a discourse-based NLP task which aims to link textual events referring to the same conceptual event (Lu & Ng 2018). In the framework of the ENCORE research project we have built the first end-to-end approach for Dutch cross-document ECR (De Langhe, 2024).\n \nIn this presentation we will explain how this system combines expert-level positional knowledge with graph-based representations in order to create a memory-efficient and accurate system. Through an extensive quantitative and qualitative analysis of the model's output we demonstrate that our proposed model architecture consistently detects and classifies harder cases of coreference compared to a series of solid LLM-based baseline algorithms.\n \nNext we present the results of a use case for which 125 participants were recruited on Prolific (https://www.prolific.com/) to investigate the impact of news recommendation strategies on users’ knowledge retention in Dutch. The experimental design builds on the idea that conventional news recommendation systems, often guided by popularity metrics or keyword-based heuristics, may fail to provide readers with comprehensive understanding or exposure to a wide range of perspectives. By contrast, topic-based article groupings grounded in semantic and event-level coherence -- such as the above-mentioned end-to-end ENCORE system -- are hypothesized to foster better knowledge retention and possibly encourage deeper engagement with content. To this purpose all participants were randomly assigned to two conditions (random versus topic-based) in which they each time first had to read five texts and then fill in a topic-specific multiple choice knowledge test. The objective is to measure not only factual recall and comprehension, but more specifically the impact of technology enhanced information delivery and contextual reinforcement on knowledge retention. In addition to knowledge scores, user demographics and self-reported news consumption behavior are recorded to explore correlations with test performance. The results contribute to ongoing debates around the design of (news) recommender systems and how these can be enhanced using state-of-the-art discourse modelling.\n\n\nReferences\n\nDe Langhe, L. (2024). Demystifying discourse in Dutch : a study on Event Coreference Resolution. Ghent University. Faculty of Arts and Philosophy, Ghent, Belgium.\n\nHobbs, J. R. (1979). Coherence and coreference. Cognitive science, 3(1), 67–90.\n\nKehler, A. (2006). Discourse coherence. The handbook of pragmatics, 241–265.\n\nLu, J., & Ng, V. (2018). Event Coreference Resolution: A Survey of Two Decades of Research. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (pp. 5479–5486)."
            },
            {
              "id": 63,
              "title": "Constructions All the Way Up: From Sensory Experiences to Construction Grammars",
              "authors": "Jérôme Botoko Ekila, Lara Verheyen, Katrien Beuls, Paul Van Eecke",
              "affiliation": "Vrije Universiteit Brussel, Université de Namur",
              "abstract": "Constructionist approaches to language posit that all linguistic knowledge is captured in form-meaning pairings called constructions (Fillmore, 1988; Goldberg, 2003). Within this framework, grammar and lexicon exist on a continuum, with constructions ranging from fully substantive lexical items to abstract grammatical patterns.\n\nCrucially, constructions are rooted in an individual’s embodied experience and interaction with the world. For instance, a construction mapping the word form “dog” to a DOG concept is shaped by an individual’s encounters with dogs, including what they have seen, learned or heard about them. Beyond the perceptual level, language users also acquire constructions that coordinate more abstract cognitive processes. Consider the sentence “the dog chases the cat” in which the transitive construction organises the relation between the CHASING event and its participants. The transitive construction maps the role that participants take in the event to the order of the corresponding constituents in the sentence. This abstract mapping is learned through repeated encounters of linguistic utterances and observations in the world. Importantly, we are always dealing with pairings of form and meaning, whether the meanings are concepts or cognitive processes. Some pairings are grounded in direct sensory experience whereas others encode more abstract patterns, yet all arise from lived experience.\n\nConstructions emerge from situated interactions with other language users (Tomasello, 2003; Bybee, 2010; Diessel, 2017). A computational approach to modelling this process has been the use of language games, where embodied agents acquire constructions through repeated situated communicative interactions (Steels, 1999). However, a key challenge remains: no existing computational model has yet demonstrated the emergence of a construction grammar that is both directly learned from sensory experience and capable of capturing a range of constructions, spanning from grounded lexical constructions to more abstract grammatical constructions, without a given lexicon, predefined categories or ontology.\n\nInspired by Beuls and Van Eecke (2024), we present a model that enables a learner agent to acquire a construction grammar from the ground up through situated communicative interactions with a tutor agent. The learner develops a grammar that spans constructions ranging from perceptually grounded concepts to abstract grammatical patterns. We validate the approach experimentally in a benchmark environment and thereby demonstrate that a computational construction grammar including more abstract constructions can be acquired directly from sensory experience.\n\nReferences:\nKatrien Beuls and Paul Van Eecke. 2024. Humans learn language from situated communicative interactions. What about machines? Computational Linguistics, 50(4):1277–1311.\nJoan Bybee. 2010. Language, usage and cognition. Cambridge University Press, Cambridge, United Kingdom.\nHolger Diessel. 2017. Usage-based linguistics.  In Oxford Research Encyclopedia of Linguistics, 10.1093/acrefore/9780199384655.013.363. Oxford University Press, Oxford, United Kingdom.\nCharles J. Fillmore. 1988. The mechanisms of “construction grammar”. In Annual Meeting of the Berkeley Linguistics Society, volume 14, pages 35–55.\nAdele E. Goldberg. 2003. Constructions: A new theoretical approach to language. Trends in Cognitive Sciences, 7(5):219–224.\nLuc Steels. 1999. The Talking Heads experiment: Volume I. Words and Meanings. Best of Publishing, Brussels, Belgium.\nMichael Tomasello. 2003. Constructing a Language: A Usage-Based Theory of Language Acquisition. Harvard University Press, Harvard, MA, USA."
            },
            {
              "id": 77,
              "title": "Leveraging Universal Dependencies (UD) annotations to create multilingual probing datasets",
              "authors": "Rohin Vijayakumar, Kushal Tatariya",
              "affiliation": "KU Leuven",
              "abstract": "In the field of multilingual NLP, the introduction and continued development of transformer-based large language models (LLMs) that are pretrained on multiple languages have proven their efficacy and efficiency in various tasks. However, despite their proven advantage in NLP tasks, these models essentially remain black-boxes. Among the several methods proposed to understand the underlying processes of such models is probing, specifically classifier-based probing (Belnikov, 2022). In this method, the model weights are frozen for each of the its layers, and a classifier is deployed that is trained to solve a task based on the aggregate representations of the intermediate embeddings of each specific layer.  \nA standard framework proposed for linguistic probing is SentEval (Conneau et al., 2018), though it only focuses on English. Even though there have been other studies and academic scholarship that focuses on probing, especially within the context of multilingual NLP (Conneau et al., 2019; Serikov et al., 2022), these studies present many flaws and challenges with respect to task robustness, language selection, or data quality and availability. \nThe UD treebanks contain comprehensive and high-quality multilingual datasets that are annotated by part-of-speech tags and dependency relations, which can be leveraged to create multilingual probing datasets. This study focuses on whether the multilingual annotated datasets in Universal Dependencies (UD) Treebanks can be used to create multilingual probing datasets. \nConsequently, the study focuses on creating robust multilingual datasets for probing based on the different surface, syntactic, and sematic tasks proposed by Conneau et al., 2018. A total of sixteen language datasets were chosen from the UD treebanks to represent a good collection of different languages. The datasets were then used to create probing datasets for eight probing tasks based on the SentEval framework. Furthermore, three pretrained multilingual transformer models were selected: m-BERT, XLM-RoBERTa, and Glot500 for the experiments. \nThis study aims to develop an improved multilingual probing dataset using UD treebanks which would provide a more detailed comprehension of the linguistic capabilities enclosed in multilingual transformer models.\n\nReferences:\nBelinkov, Yvonatan. “Probing Classifiers: Promises, Shortcomings, and Advances,” Computational Lingusitics 48, no.1 (2022): 207-2019.\nConneau, Alexis and Douwe Kiela. “SentEval: An Evaluation Toolkit for Universal Sentence Representations,” CoRR abs/1802.05449, 2018. eprint.\nConneau, Alexis, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126–2136, Melbourne, Australia. Association for Computational Linguistics.\nSerikov, Oleg, Vitaly Protasov, Ekaterina Voloshina, Viktoria Knyazkova and Tatiana Shavrina. “Universal and Independent: Multilingual Probing Framework for Exhaustive Model Interpretation and Evaluation,” Proceedings of the Fifth BlackBoxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, 2022: 441-456."
            },
	      {
		  "id": 12,
		  "title": "Unsupervised Detection of Formulaic Texts in Historical Corpora Using CNN-Based Embeddings",
		  "authors": "Paulien Lemay, Els Lefever, Klaas Bentein",
		  "affiliation": "Ghent University",
		  "abstract": "This study investigates unsupervised methods for identifying groups of similar texts within large historical corpora, aiming to trace the reuse of formulaic expressions across spatial, temporal, and corpus boundaries. Traditional string similarity measures—such as Levenshtein distance—are computationally infeasible at scale due to the need for exhaustive pairwise comparisons. To address this, I explore vector-based approaches using learned text embeddings, which allow for more efficient and scalable similarity searches. While the focus is on historical Greek texts, the method is broadly applicable to other languages and textual traditions. \n\nThe experimental pipeline involves two main steps. First, we generate meaningful embeddings for each text unit, experimenting with representations at the word, verse (or sentence), and full-text levels. The approach builds on the work of Dai et al., who, in their 2020 paper \"Convolutional Embedding for Edit Distance\", propose using convolutional neural networks (CNNs) to produce embeddings that approximate string edit distances. They suggest experimenting with both trained and untrained CNNs, with the latter offering a lightweight and scalable solution for large corpora with limited resources. We compare the results with other embedding techniques, including character-based RNNs, static (fastText) and contextual transformer-based embeddings (BERT), and traditional count-based methods such as TF-IDF. \n\nSecond, clustering techniques are applied to group similar texts based on their embeddings, using k-nearest neighbors, k-means clustering, and pairwise similarity comparisons. For initial experiments, we use FAISS for fast nearest-neighbor searches. For larger-scale analyses, we incorporate cuML, a GPU-accelerated machine learning library, to enable efficient clustering across corpora containing tens of thousands of texts. \n\nThe proposed methodology is validated by means of a case study for Byzantine Greek. We created a gold-standard subset from the Database of Byzantine Book Epigrams or DBBE (https://www.dbbe.ugent.be/, 2025), which collects metrical paratexts from Byzantine manuscripts. This subset includes manually annotated verse- and poem-level similarities (Deforche 2024), allowing for evaluation of clustering quality. Experiments with untrained CNNs produced fast results, but limited recall - not all known similar texts were grouped together. To improve performance, we trained the CNNs using a triplet matching approach, following Dai et al., in which the model learns to minimize the distance between similar texts while maximizing the distances from dissimilar ones. This significantly improved clustering quality, though with increased computational costs. \n\nIn a second phase, we expanded testing to a broader and more diverse corpus, including the full DBBE; the corpus of Andreas Rhoby (Rhoby 2009; 2010; 2018), which features Byzantine epigrams inscribed on a variety of materials; the documentary papyri (https://papyri.info/, 2025), which include non-metrical, pre-Byzantine texts across multiple genres; and the PHI Greek inscriptions (https://inscriptions.packhum.org/, 2025), comprising pre-Byzantine texts preserved on various media. Even with untrained CNNs, the results reveal recurring patterns of textual reuse across different writing surfaces, historical periods, and literary genres. \n\nFuture work will focus on optimizing the trained CNN model and applying it across the full corpus to uncover formulaic traditions over time and across corpora."
              },
			      {
		  "id": 39,
		  "title": "Using BERT for Mapping Ideologies in Dutch Politics",
		  "authors": "Wessel Ledder",
		  "affiliation": "Radboud University Nijmegen",
		  "abstract": "Dick Schoof was chosen as the Dutch Prime Minister in 2024, because he did not have a political color and could hence represent a neutral stance in the government. We aimed to investigate whether Schoof was really neutral in his political statements, or whether his words aligned more with certain political ideologies than others. For this study, we used a text mining approach on speeches of debates in the Dutch House of Representatives (Tweede Kamer). We mapped out the ideologies of political parties to create a political landscape, based on the debate speeches. We did this by fine-tuning a BERT model, resulting in embeddings, which can be plotted to create a political spectrum.  The results show a lot of overlap in terms of embeddings for every single speech, however, when taking the average embedding value of each party, opposition parties are clustered together, as well as the coalition parties. However, due to a large skew in the data distribution per party (the amount of speeches per party), which appears to be related to the distribution of seats in the House of Representatives, the model had some difficulties clustering left-wing and right-wing parties together. Prime Minister Schoof’s speeches were also assessed using the fine-tuned BERT model to place him in the political spectrum. These are semantically close to those of the opposition parties, indicating that Dick Schoof is not neutral in his political statements."
	      }
          ]
        }
      ]
    }
  ]
}
