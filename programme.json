[
  {
    "type": "break",
    "start": "08:30",
    "end": "09:15",
    "title": "Registration"
  },
  {
    "type": "session",
    "start": "09:15",
    "end": "10:35",
    "events": [
      {
        "track": "Track A",
        "title": "Language Learning",
        "talks": [
          {
            "time": "09:15–09:35",
            "title": "SASTA Self Assessment",
            "author": "Jan Odijk, Jelte van Boheemen, Xander Vertegaal, Tessel Boerma, Marijn Schraagen",
            "abstract": "SASTA [van Boheemen et al., 2024] is a web application that automates the application of established methods for analyzing transcripts of spontaneous language such as TARSP (for children below 4 years of age) [Schlichting, 2017], STAP (for children aged 4-8 years) [van Ierland et al., 2008; Verbeek et al., 2007] and ASTA (for patients with aphasia) [Boxum et al., 2013].\n\nSASTA achieves good results (F1-score for samples ranging from 74% to 96%, average  F1-score for datasets 90.4%), but is certainly not perfect. Therefore, a manual check of the results by human experts is often necessary.\n\nIf the human expert has to check the whole transcript, the gain in efficiency by using SASTA is limited. This gain can be increased if SASTA could point out specific utterances that it considers or suspects to be problematic, i.e. if SASTA could do some form of self-assessment. Here, we report on research investigating the feasibility of such a SASTA self-assessment procedure.\n\nSASTA Self Assessment aims to provide a list of utterances that have to be manually corrected. We assume that manual correction of an utterance leads to a 100% F1-score for that utterance. For SASTA Self Assessment, we initially set the target F1-score for the whole transcript at 95% and the maximum number of utterances that have to be manually corrected at 15. In the worst case, this means that less than half of the utterances of a sample need to be corrected. \n\nWe identified 15 criteria that could reliably indicate potential misanalysis by the parser used in SASTA (Alpino, [Van Noord, 2006]). For example, if the parser cannot combine all words of an utterance into one sentence, then the utterance is most probably parsed incorrectly. Approximately 40 measures in TARSP describe how the words and major phrases of an utterance are grammatically related. An example is the TARSP measure OndWB (declarative clause with a subject, a predicate and an adverbial modifier). It is highly likely that SASTA will not correctly assign such a code to an utterance that it has failed to parse as a single sentence. SASTA Self Assessment will allow for the identification of such an utterance as requiring a manual check. \n\nIn addition to identifying potentially misanalysed utterances, we also want the number of utterances that have to be manually checked to be as low as possible. This requires ordering the identified utterances descending by the utterance’s effect on F1-improvement. We determined a number of criteria that can be used to sort the found utterances. \n\nIn the presentation, we will introduce the criteria we used for identifying utterances to be corrected and the criteria we used to sort the utterances. We will report on the results we obtained and reflect on the approach taken.\n\nReferences can be found here: https://docs.google.com/document/d/1kW-cmMGm5oiXiyg0Kg_DRCk5C5NEMdWQBzOQ7Fje864/edit?usp=sharing"
          },
          {
            "time": "09:35–09:55",
            "title": "Analyzing reading and spelling errors in third and fourth graders with weak literacy skills",
            "author": "Wieke Harmsen, Roeland van Hout, Catia Cucchiarini, Helmer Strik",
            "abstract": "The ability to read and write is essential to participate in our literate society. Therefore, these skills have a prominent role in the learning curriculum in primary schools. Recent large-scale surveys have shown that Dutch children’s reading skills are declining (Swart et al., 2021) and that their writing proficiency consistently falls short of the reference standard (Inspectie van Onderwijs, 2021). These alarming studies call for more research on the development of literacy skills, such as oral reading and spelling. Although these two skills rely on a large set of overlapping competences, such as phonological awareness, morphological awareness, and knowledge of the alphabetic principle, research that combines simultaneous measurements of children’s reading and spelling proficiency is limited. \n\nReading and spelling errors constitute a rich source of data for investigating written language processing (Sandra et al., 2024) and reading and spelling strategies (McGeown et al., 2013). However, analyses of reading and spelling errors are difficult, because of lack of sufficient reading and spelling data pertaining to one and the same child and the large variety of errors that can occur. To overcome this problem, an algorithm was recently developed that can recognize reading and spelling errors automatically as Phoneme-Corresponding Units (PCUs) (i.e., mappings between a phoneme and the letter(s) used to write this phoneme) in Dutch words read and written by initial readers and spellers (Harmsen et al., 2024). In the current study, we extended this algorithm with a more detailed reading and spelling error classification scheme, in order to analyze more complex reading and spelling errors. To classify the errors according to this classification scheme, we enriched the data with automatically generated part-of-speech tags, morphemes and lemmas. More importantly, we used this extended algorithm to analyze novel oral reading and spelling data pertaining to one and the same child that were recently collected.  The reading data consists of recordings of the Drie-Minuten-Test (Three-Minutes-Test), a standardized test to measure oral word reading skills. These recordings were semi-automatically transcribed. The spelling data are handwritten dictations of the Cito spelling test at the pupil’s respective grade level. These dictations were manually digitized (i.e., typed). We posed the following research questions: 1) To what extent can we automatically extract reading and spelling errors from oral word reading tests and word spelling dictations? 2) To what extent do children exhibit an overlap in reading and spelling errors? \n\nFor the first time we will be able to present an overall reading and spelling error classification scheme and an analysis of overlap in reading and spelling errors in data pertaining to one and the same child. We will discuss our results and present avenues for future research.\n\nSwart et al. (2021). PIRLS-2021 Trends in leesprestaties, leesattitude en leesgedrag van tienjarigen uit Nederland.\nInspectie van onderwijs (2021). Peil.schrijfvaardigheid Einde (speciaal) Basisonderwijs 2018-2019. \nSandra et al. (2024). https://doi.org/10.1007/s11525-024-09424-z\nMcGeown et al. (2013). https://doi.org/10.1016/j.lindif.2013.09.013\nHarmsen et al. (2024). https://aclanthology.org/2024.cawl-1.2/"
          },
          {
            "time": "09:55–10:15",
            "title": "Assessing Large Language Models for Automatically Grading L2 Dutch Adult Learner Writing",
            "author": "Joni Kruijsbergen, Orphée De Clercq",
            "abstract": "In today’s knowledge-based society, the ability to write effectively remains important in both professional and academic contexts (De Smet et al., 2012). For highly-educated L2 learners that skill serves as a gateway to active participation in civic affairs and the global economy (Graham & Perin, 2007). In Flanders, as well as many other European countries, the proficiency level of non-natives learning a certain language is measured through the Common European Framework of Reference for Languages (CEFR). To test a certain learner’s language proficiency various exams have been developed, such as the CNaVT in Flanders. Besides other skills, writing forms a substantial subset of this exam.   Despite the recurring nature of the CNaVT exams and the large number of participants - over 2,000 adult learners of Dutch each year - only little efforts have been invested into automation (Chen, Vandeghinste & Vandommelen, 2024). This research focuses on automating the assessment of the writing tasks. Given the recent paradigm shift within NLP towards decoder-based LLMs and generative chatbots, zero-shotting is the preferred methodology to explore first. However, this requires that knowledge about the CEFR labels is present in modern-day LLMS. While prior research has demonstrated that some knowledge seems to be inherent, it is often not leveraged when performing downstream tasks such as readability prediction or predicting the CEFR label for L2 English texts. (Benedetto et al., 2025).   So far, this has not been researched for languages other than English which is why our research will examine several traditional and recent approaches for the classification of Dutch writing products of adult L2 speakers. To this purpose we relied on a proprietary dataset from the CNaVT comprising human-evaluated texts. We compared zero-shotting both mono- and multilingual generative LLMs to training a feature-based model. Based on our prior work on grammatical error detection in Dutch (Kruijsbergen et al., 2024), we also include fine-tuning monolingual and multilingual BERT-based language models as an additional point of reference.  References:  Benedetto, L., Gaudeau, G., Caines, A., Buttery, P. (2025).Assessing how accurately large language models encode and apply the common European framework of reference for languages. Computers and Education: Artificial Intelligence, vol. 8  Chen, S., Vandeghinste, V. & Vandommele, G. (2024). Automated Pass/Fail Classification for Dutch as a Second Language Using Large Language Models. Abstract presented at Computational Linguistics in the Netherlands 34.  de Smet, M.J.R., Brand-Gruwel, S., Broekkamp, H & Kirschner, P.A. (2012). Write between the lines: Electronic outlining and the organization of text ideas. Computers in human behavior 28(6), 2107-2116  Graham & Perin 2007. A meta-analysis of writing instruction for adolescent students. Journal of educational psychology 99(3), 445  Kruijsbergen, J., Van Geertruyen, S., Hoste, V., & De Clercq, O. (2024). Exploring LLMs’ Capabilities for Error Detection in Dutch L1 and L2 Writing Products. Computational Linguistics in the Netherlands Journal, 13, 173–191."
          },
          {
            "time": "10:15–10:35",
            "title": "Dictionaries, DeepL, and ChatGPT: Comparing Language Tool Effects on L2 English Speaking and Learner Perceptions",
            "author": "Irma Schampheleer, Lieve Macken, Vanessa De Wilde",
            "abstract": "In recent years, language learners have increasingly relied on digital tools such as machine translation (MT) systems and generative artificial intelligence (AI) tools for language support. While the effects of MT tools like DeepL on second language (L2) writing have been relatively well-documented, fewer studies have examined their influence on L2 speaking performance. Research on the impact of generative AI tools such as ChatGPT in this context is even more limited. This multi-method experimental study aims to assess how the use of MT and generative AI tools affects L2 English speaking performance, as well as learners’ experiences preparing oral presentations. Thirty second-year university language students were divided into three groups and assigned either dictionaries, DeepL, or ChatGPT for language support for preparing a group presentation. The presentations were recorded, transcribed, analysed for fluency and lexical complexity, and evaluated by their lecturer. Additionally, participants completed a two-part survey: one part reflecting on their assigned tool during the preparation process, and one evaluating their general experience with all three tools. Results showed that MT users outperformed other groups in fluency-related measures, while AI users demonstrated greater lexical diversity, density, and use of academic vocabulary. Lecturer evaluations favoured presentations prepared with MT and AI tools over those prepared with dictionaries. Additionally, participants using notes during the delivery demonstrated a higher lexical diversity and density. Survey responses indicated that learner attitudes varied across groups. Machine translation tools were generally rated as more enjoyable than dictionaries, while generative AI tools received mixed reactions, particularly from participants in the dictionary and MT groups. In terms of perceived usefulness, dictionaries and MT tools were rated positively overall, whereas AI tools received lower ratings, especially from dictionary users. When ranking the three tools for preparing speaking and writing tasks, MT tools emerged as the most consistently preferred, though AI tools attracted some highly positive ratings for speaking preparation. Both MT and AI tools were appreciated for their convenience, time efficiency, and ability to help learners review their work. However, for the presentation preparations, DeepL was mainly valued for looking up and expanding vocabulary, while learners in the AI group indicated using ChatGPT to generate ideas for argumentation and to answer both language-related and content-related questions. These findings suggest that while machine translation tools currently offer the most consistent benefits for enhancing L2 speaking fluency and remain a preferred, reliable option for learners, generative AI tools hold considerable potential, particularly for supporting idea development and promoting lexical density and diversity. However, the mixed learner attitudes towards AI tools underscore the need for further research into how these technologies can be optimally integrated into language learning contexts, considering both their impact on production and learner preferences."
          }
        ]
      },
      {
        "track": "Track B",
        "title": "Resources and Evaluation",
        "talks": [
          {
            "time": "09:15–09:35",
            "title": "Considerations in creating the GPT-NL Corpus",
            "author": "Frank Brinkkemper, Jesse van Oort, Saskia Lensink, Matthieu Laneuville, Duuk Baten, Freek Bomhof",
            "abstract": "For GPT-NL (gpt-nl.nl), a large language model initiative from TNO, SURF, and NFI, the goal was set to create one of the biggest Dutch pre-training corpora. GPT-NL adopts a conscientious approach that explicitly acknowledges and respects copyright legislation. Therefore GPT-NL has sought permission from content owners to build up the biggest Dutch corpus. In this work we discuss the values and considerations in the creation of the GPT-NL corpus.  \n\nThe biggest and most common source for pre-training content for LLMs is the web. Fineweb-2 (https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) contains around 69 billion Dutch words. However, a lot of this content is owned by copyright holders who did not give general permission to use this for training LLMs. Thus, with this giant source in question, GPT-NL has had to rethink how to collect enough permissive content.  \n\nFirst, we started by defining what content would be suitable for training GPT-NL.  \n\nSuitable content is: \n\n- Not subject to copyright  \n- Or has a permissive license (for GPT-NL this means CC-0 or CC-BY) \n- Or copyright license given to GPT-NL \n- And not social media content, as its content is deemed to be in opposition to the needs of GPT-NL. \n\nWith these constraints the content collection for GPT-NL set its strategy to (1) collect public domain/permissively licensed datasets, (2) figure out a way to effectively use web content, and (3) request license from the biggest copyright holders in the Netherlands for Dutch content. \n\nThe recently released Common Corpus (https://huggingface.co/datasets/PleIAs/common_corpus/), a collection of public domain and permissively licensed content, fulfilled the first part of the strategy as GPT-NL could filter public domain, CC-0, and CC-BY from this collection. However, this highlighted the importance of the other two areas as this content was 80% older than 140 years old, and mostly non-Dutch. \n\nFor web content GPT-NL worked together with Bram Vanroy from Instituut voor de Nederlandse Taal to annotate common crawl with licenses, see (https://huggingface.co/datasets/BramVanroy/CommonCrawl-CreativeCommons). Next to that, we worked with the Open State Foundation to retrieve large datasets from the Dutch government like https://openraadsinformatie.nl/, and https://www.officielebekendmakingen.nl/.  \n\nFinally, we engaged collaboratively with major copyright stakeholders to explore the conditions under which they would be willing to make their content available for large language model training. In alignment with GPT-NL’s commitment to transparency and impartiality, a content board was established comprising representatives from several leading content providers. Through this board, a reciprocal framework was developed whereby 50% of GPT-NL’s net revenues, coming from model licensing fees, are equitably distributed among all participating copyright holders.  \n\nThe combination of these efforts has led to the GPT-NL corpus, which will be detailed further in the upcoming months. There, we’ll also go into many more considerations we’ve had along the way."
          },
          {
            "time": "09:35–09:55",
            "title": "Towards MTEB-NL: Collecting Dutch Resources for a Unified Embedding Evaluation",
            "author": "Nikolay Banar, Ehsan Lotfi, Marija Kliocaite, Walter Daelemans",
            "abstract": "Massive Text Embedding Benchmark (MTEB; Muennighoff et al., 2023) has emerged as a standard framework for evaluating text embeddings across multiple NLP tasks. While the MTEB leaderboard was recently expanded to many languages (Enevoldsen et al., 2025), Dutch remains underrepresented in this evaluation. In this work in progress, we introduce MTEB-NL: a curated collection of Dutch-language resources designed to facilitate the standardized assessment of embedding models on a diverse set of downstream tasks, such as semantic textual similarity, classification, retrieval, and reranking.\n\nFirst, we review and select existing Dutch datasets that correspond to the tasks from MTEB. Next, we refine these datasets to ensure consistency and high data quality. In addition, we introduce new datasets to extend the task coverage of MTEB-NL. We adapt these datasets and select evaluation metrics to align with the established format of MTEB, ensuring that any Dutch embedding models can be evaluated without additional steps. We also provide baseline results obtained from multilingual and Dutch embedding models.\n\nBy working on MTEB-NL, we aim to fill a critical gap in the evaluation of Dutch embedding models. MTEB-NL will be added to the official MTEB leaderboard, enabling researchers to directly compare Dutch embedding models, track progress over time, and identify weaknesses in current models. MTEB-NL will lay the groundwork for future research and the development of embedding models for the Dutch language.\n\nMuennighoff, N., Tazi, N., Magne, L., & Reimers, N. (2023, May). MTEB: Massive Text Embedding Benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (pp. 2014-2037).\n\nEnevoldsen, K., Chung, I., Kerboua, I., Kardos, M., Mathur, A., Stap, D., ... & Muennighoff, N. (2025). Mmteb: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595."
          },
          {
            "time": "09:55–10:15",
            "title": "LoveHate: A Russian Debate Corpus for Argument Mining",
            "author": "Natalia Evgrafova, Pranaydeep Singh, Véronique Hoste, Els Lefever",
            "abstract": "Mining arguments from social media remains a significant challenge (ArgMinTutorial@LREC-COLING-2024) due to the informal nature of user-generated content (Schaefer & Stede, 2020), diversity of registers, domains, and presence of noise (Habernal & Gurevych, 2017). Moreover, the manual annotation of such content is both costly and time-consuming. In this context, online debate portals present a promising alternative: users engage in structured discussions on specific topics, producing argumentative content that retains a degree of informality while offering valuable metadata, such as date, topic, and stance.\n\nThis study presents a Russian-language argumentative corpus scraped from a debate platform lovehate.ru, and its analysis. \n\nTo date, the largest crawled argumentation corpora are the args.me corpus (Ajjour et al., 2019) and the Internet Argument Corpus (IAC) (Walker et al., 2012; Abbott et al., 2016). The args.me corpus includes 387,606 arguments collected up to May 2019 from four debate platforms: idebate.org, debatepedia.org, debatewise.org, and debate.org. The IAC comprises 390,704 posts across 11,800 discussions originally crawled from 4forums.com, and later extended by Abbott et al. (2016) to incorporate content from CreateDebate.com and ConvinceMe.net. Both corpora are in English and focus primarily on social and political issues such as Gay marriage, Abortion, Death penalty, God, Atheism, etc.\nTo the best of our knowledge, the largest existing Russian-language argument corpus is RuArg2022 (Kotelnikov et al., 2022), which contains 9,550 sentences related to COVID-19, crawled from Twitter and annotated for stance.\nTo create a comparable yet broader corpus in Russian, we utilize the debate platform lovehate.ru, accessed via the Wayback Machine. We crawled a total of 361,125 user posts on selected topics, such as Atheism, Death penalty, Communists, Liberalism, Feminists, God, and the USSR, spanning from the early 2000s to 2019, the year the website ceased operation. The topics were chosen to include both language-specific issues and topics shared with existing English-language corpora.\nIn this study, we provide a detailed description of the corpus and demonstrate its application in two key tasks: stance detection and conclusion generation. We explore both tasks in two experimental settings: (1) training on original Russian data, and (2) training on English data automatically translated into Russian. This allows us to assess whether models trained on translated data can match the performance of those trained on native Russian texts, particularly on both shared topics and culturally specific ones (e.g., the USSR).\nIn addition, we perform a comparative analysis of arguments in Russian (from our corpus) and in English (from args.me) on overlapping topics. We identify common, prominent, and unique premises across the two languages. Finally, we analyze temporal trends in argument usage across selected topics in the lovehate.ru corpus to determine whether new arguments emerge, some gain prominence over time, and whether these shifts correlate with spikes in public interest as measured by Google Trends data."
          },
          {
            "time": "10:15–10:35",
            "title": "Scaling and Evaluating Transformer-based ASR Model Applications in Children's Speech Educational Settings",
            "author": "Gus Lathouwers, Lingyun Gao, Catia Cucchiarini, Helmer Strik",
            "abstract": "Within the last decade, transformer-based architectures have brought improvements to Automatic Speech Recognition (ASR) performance. One challenging area of application for ASR is automatic recognition of children’s speech. In primary school educational settings, ASR can be used to automatically transcribe children utterances, which can be useful to measure language development in children (Shadiev and Liu, 2023). Studies have shown that transformer-based ASR models can achieve strong zero-shot performance for children's read speech in clean settings (Harmsen et al., 2023).\n\nAvailable research on ASR for educational purposes has investigated ASR model performance on fixed length datasets of children's speech, which contain only word lists or sentence lists, but have not focused on both. In practice, children's speech recordings may be variable, meaning consisting of different speaker profiles, recording lengths and word counts, and lacking text annotations. The development of ASR setups through pipelines that can accurately transcribe children speech or mark cases where ASR-model transcription falls short and human intervention is required is needed, because manually annotating thousands of children speech recordings is very time-consuming and costly. The current research examines means to automatically tag and transcribe children speech, through novel ASR model setups. \n\nThe research questions we would like to address are: (1) How effective are modern ASR models applied to a set of children recordings of variable length and speaker makeup, and (2) How can modern ASR technology procedures be used to tag data that lacks human annotation? For the second research question, an automated pipeline procedure was built that processes children speech through an ASR model agreement system.\n\nThe dataset used is a collection of child read-speech utterance recordings obtained for the Nationaal Onderwijslab AI (NOLAI) project (N=22430). Recorded utterances of children were between 1 and 28 word length, 1 to 44 seconds. An initial subsample of n=166 was selected through a stratified sampling approach to test a pipeline procedure and compare model performance. Six models were part of the study, namely SeamlessM4T Large (v1), Wav2Vec2 Large XLSR 53 Dutch, NVIDIA FastConformer-Hybrid Large (NL), and three whisper models (Whisper Large, Whisper Large-v2, Whisper Large-v3). To answer the first research question, model performance was assessed through Word Error Rate (WER). To answer the second question, a protocol was made using sentence-alignment algorithms applied to multiple ASR-models.\n\nOur research addresses a gap in the literature due to forming a system that can be used to effectively deal with the practical problem of large amounts of diverse recorded data that lack annotation. It presents detailed analysis of the outcome of individual model performance applied to variable children speech, as well as presenting means to tackle common automatic speech-recognition issues in educational settings.\n\nSources:\n\nShadiev, R., and Li, J. (2023). Review of research on applications of speech recognition technology to assist language learning. ReCALL, vol. 35, no. 1, pp. 74–88.\n\nHarmsen, W., Hubers, F., van Hout, R., Cucchiarini, C., and Strik, H. (2023). Measuring word correctness in young initial readers: Comparing assessments from teachers, phoneticians, and ASR models. Proceedings of the 9th Workshop on Speech and Language Technology in Education (SLaTE), 2023-03, pp. 11-15."
          }
        ]
      },
      {
        "track": "Track C",
        "title": "Linguistics 1",
        "talks": [
          {
            "time": "09:15–09:35",
            "title": "MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference",
            "author": "Mădălina Zgreaban, Tejaswini Deoskar, Lasha Abzianidze",
            "abstract": "The models that predict inference relations between sentences are obtaining high results on the standard natural language inference (NLI) datasets, and in many cases, are even surpassing the average annotator performance. However, the models are not so successful when it comes to generalization, i.e., predicting correct inference labels for sentence pairs that are dissimilar to the problems found in the training sets. There has been a series of works that reveal the generalization shortcomings of NLI models. One of the commonly adopted approaches is to design adversarial problems, problems that are created with the intention to make models fail on them. \nIn our presentation, we introduce a new type of generalization test called MERGE after the minimal expression-replacements generalization. In this test, existing NLI problems are minimally altered by automatically replacing shared words between sentences. For example, if the original NLI problem is the premise \"A man and a dog walk on the beach together.\" entailing \"A man is taking a walk on the beach with a dog.\", then its minimally altered variants can be obtained by replacing \"man\" with \"woman\", \"boy\", \"guitarist\", etc., \"dog\" with \"kid\", \"cow\", \"cat\", etc., or \"walk\" with \"hike\", \"stroll\", \"wander\", etc. Additionally, what makes this method minimal and model-friendly is that it preserves common heuristics per problem, e.g., a degree of word overlap remains the same. We automate the replacements with the help of large masked language models. While doing so, we apply certain preprocessing steps to obtain grammatical and sensible sentences and label-preserving inference pairs. We present the results and analyses on how NLI models generalize on such minimally altered variants obtained from the Stanford NLI (SNLI) dataset. While models' overall performance on the variant problems is comparable to the performance on the original problems, the models fail to retain the same prediction for variant problems. This shows that models are not consistent in their predictions and are sensitive to word choices that are irrelevant from an inference perspective."
          },
          {
            "time": "09:35–09:55",
            "title": "Meertje: an LLM for Dutch Dialect Classification",
            "author": "Nikki Beyer",
            "abstract": "This presentation introduces Meertje: a language model developed at the Meertens Institute to distinguish dialect from Standard Dutch. Meertje is a fine-tuned version of Dutch BERTje (de Vries et al. 2019); it automatically identifies, isolates, and extracts dialect from Dutch corpora.\n\nBackground\nBecause Large Language Models struggle with low-resource languages, they are typically unsuited for dialect classification (Faisal and Anastasopoulos 2025). However, re-training with even a small amount of data drastically improves their performance. Fine-tuned models have successfully identified features of, for example, Romanian, Mandarin, and Italian dialects (Gaman et al. 2020; Xie et al. 2024). For the Meertje Project, I developed such a model for Dutch.\n\nThe Meertens Dialect Novel Corpus\nThe project stems from the Dialect Novel Corpus, a collection of 1112 dialectal literary works at the Meertens Library. The texts are noisy, generically varied, and, crucially, made up of different proportions of dialect versus Standard Dutch. With the more general goal of using the corpus for the linguistic analysis of Dutch dialects, the Meertje Project set out to develop an LLM to isolate dialect material.\n\nMethods\nMeertje was trained on a subset of the corpus, comprising ten works by author Bart Veenstra from Drenthe. Pre-processing involved sentence tokenization and removing punctuation using Python’s nltk module. This yielded 23000 sentences, which I manually annotated as being either Standard Dutch or dialect. The data was split into a training, development, and test set, which were balanced to contain an equal number of datapoints for each class. I then fine-tuned BERTje on this Veenstra corpus, producing MeerBERTje, or Meertje for short.\n\nResults\nMeertje can distinguish Drents from Standard Dutch in Veenstra’s texts with 95% accuracy. Most pertinently for dialect extraction, the recall score was 0.94 – of all dialect sentences in the data, the model found 94%. For further testing, I compiled test sets for other authors from Drenthe and for texts from Groningen, Twente, and Zeeuws-Vlaanderen. As the results in Table 1 indicate, Meertje’s recall scores for other authors and even other dialects ranged between 0.86 and 0.99. This means the texts could be successfully split into dialect material on the one hand, and Standard Dutch on the other.\n\nTABLE 1\nAuthor                Precision | Recall | F1\nVeenstra        0.97 | 0.94 | 0.95                Test set\n\nVos                        0.95 | 0.99 | 0.97         Other authors from Drenthe\nVeendorp        1.00 | 0.93 | 0.96\nLoon                0.88 | 0.86 | 0.87\n\nWold                0.99 | 0.93 | 0.96                Groningen\nVloedbeld        1.00 | 0.96 | 0.98                Twente\nBootsgezel        0.73 | 0.91 | 0.81                Zeeuws-Vlaanderen\n\nImplications and Conclusion\nMeertje still underperforms when presented with noisy data (like OCR-mistakes), with less common Dutch (like the archaic 'goochemerd' or the Flemish 'allez'), and with sentences which are dialectal in their syntactic structure but Standard Dutch in their choice of lexical items. The model might therefore benefit from additional finetuning on other texts from the Dialect Novel Corpus. However, given the predictable and manageable nature of these limitations, Meertje could become a valuable tool for dialect extraction for a wide variety of Dutch corpora."
          },
          {
            "time": "09:55–10:15",
            "title": "Reading Time Prediction for Dutch Text Simplification",
            "author": "Sijbren van Vaals, Rik van Noord, Malvina Nissim",
            "abstract": "A substantial portion of the Dutch population finds journalistic media productions\ninaccessible: around 2.5 million people in the Netherlands struggle with reading, numeracy, and using smart devices such as computers or smartphones (Rijksoverheid, 2019). One way to improve accessibility of news articles is by simplifying the texts.\n\nText simplification\nText simplification aims to reduce complexity in vocabulary and syntax to improve the readability and comprehension of a text (Hobo et al., 2023; Seidl & Vandeghinste, 2024), and therefore plays an important role in improving accessibility. Unfortunately, automatic evaluation of text simplification is challenging. Common evaluation metrics such as SARI and BERTScore only look at superficial characteristics of the texts, and do not measure understanding by readers, showing a disconnect between automatic evaluation and real-world comprehension (Beks van Raaij et al., 2024; Säuberli et al., 2024).\n\nReading time prediction\nHowever, it has been shown that reading time correlates with both comprehension (Levy 2008; Wang et al., 2024) and text complexity (Singh et al., 2016; Hollenstein et al., 2022), allowing for effective simplification evaluation strategies. Nevertheless, integrating reading time prediction into simplification workflows remains an underexplored area. \n\nNewspaper collaboration\nWe collaborate with the Dagblad van het Noorden newspaper, giving us access to a dataset of around 2,000 news articles, with corresponding reading times across thousands of actual readers. We aim to model text complexity by using reading time as a proxy, assuming that more difficult texts take longer to read. \n\nFeature-based predictor\nFirst, we build a predictor that predicts the average reading time per token, based on a large set of linguistic features, calculated for each article. We find that the model predicts reading time reasonably well, with good features emerging across multiple linguistic levels. We then compute the correlation between our predictor and several readability metrics to determine to what extent they overlap. Interestingly, we find only a modest correlation between the predictor and readability metrics, potentially calling into question the validity of the readability metrics for real-world data.\n\nLLM-based prediction\nSecond, we use large language models (LLMs) to predict reading time and estimate text complexity, together with explanations supporting their answers, thereby yielding insight into the linguistic aspects they prioritise for their assessments. The LLMs report that they look into style, tone, flow, information load, and comprehension, and achieve high correlations with the gold data.\n\nConclusion\nFinally, we will combine the above insights to steer the process of text simplification. Automatic as well as human-based evaluation, run in collaboration with the newspaper and considering both original and simplified texts, will be used to assess several simplification methods. We aim to bridge the gap between reading time prediction, readability assessment, and text simplification, with the ultimate goal of automatically producing more accessible Dutch news articles. \n\n\nREFERENCES\n\nBeks van Raaij, N., Kolkman, D., & Podoynitsyna, K. (2024). Clearer governmental communication: Text simplification with ChatGPT evaluated by quantitative and qualitative research. In Proceedings of the Workshop on DeTermIt! Evaluating Text Difficulty in a Multilingual Context @ LREC-COLING 2024 (pp. 152–178). ELRA and ICCL.\n\nHobo, E., Pouw, C., & Beinborn, L. (2023). “Geen makkie”: Interpretable classification and simplification of Dutch text complexity. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023) (pp. 503–517). Association for Computational Linguistics.\n\nHollenstein, N., Gonzalez-Dios, I., Beinborn, L., & Jäger, L. (2022). Patterns of text readability in human and predicted eye movements. In Proceedings of the Workshop on Cognitive Aspects of the Lexicon (pp. 1–15). Association for Computational Linguistics.\n\nLevy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106(3), 1126–1177. \nRijksoverheid. (2019). Vervolg aanpak laaggeletterdheid: Tel mee met taal. \n\nSäuberli, A., Holzknecht, F., Haller, P., Deilen, S., Schiffl, L., Hansen-Schirra, S., & Ebling, S. (2024). Digital comprehensibility assessment of simplified texts among persons with intellectual disabilities. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI ’24). Association for Computing Machinery.\n\nSeidl, T., & Vandeghinste, V. (2024). Controllable sentence simplification in Dutch. Computational Linguistics in the Netherlands Journal, 13, 31–61.\n\nSingh, A. D., Mehta, P., Husain, S., & Rajakrishnan, R. (2016). Quantifying sentence complexity based on eye-tracking measures. In Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC) (pp. 202–212). The COLING 2016 Organizing Committee.\n\nWang, D., Sadrzadeh, M., Stanojević, M., Chow, W.-Y., & Breheny, R. (2024). How can large language models become more human? In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics (pp. 166–176). Association for Computational Linguistics."
          },
          {
            "time": "10:15–10:35",
            "title": "You shall know a construction by the company it keeps!",
            "author": "Lara Verheyen, Jonas Doumen, Paul Van Eecke, Katrien Beuls",
            "abstract": "Linguistic theories and models of natural language can be divided into two categories, depending on whether they represent and process linguistic information numerically or symbolically. Numerical representations, such as the embeddings that are at the core of today's large language models (LLMs) (Mikolov et al., 2013; Vaswani et al., 2017), have the advantage of being learnable from textual data, and of being robust and highly scalable. Symbolic representations, like the ones that are commonly used to formalise construction grammar theories (Fillmore, 1988; Kay and Fillmore, 1999; Steels and De Beule, 2006), have the advantage of being compositional and interpretable, and of supporting sound logic reasoning. As both approaches are rooted in very different mathematical frameworks, namely formal logic versus linear algebra, the integration of concepts and techniques from both fields is not straightforward. At the same time, logic-based and distributional approaches are widely regarded as complementary, and there exists no a priori reason to believe that they would be in any way incompatible. \nIn this talk, we explore how distributional representations can be integrated in a computational construction grammar framework, and how this integration of symbolic and numerical methods can enhance the robustness and generality of constructional language processing. In particular, we show how distributional representations of (i) linguistic forms, (ii) constructional slots, and (iii) grammatical categories can be integrated into the data structures and algorithms that underlie Fluid Construction Grammar (FCG) (Steels, 2004; Beuls and Van Eecke, 2023). Through a variety of examples, we demonstrate how this integration can benefit a broad-coverage FCG grammar learnt from PropBank-annotated corpora. Finally, we conclude that the future of construction grammar is neither symbolic nor numerical, but lies in a combination of both paradigms.\n\nReferences:\nKatrien Beuls and Paul Van Eecke. 2023. Fluid Construction Grammar: State of the art and future outlook. In Proceedings of the First International Workshop on Construction Grammars and NLP pages 41–50. ACL. \nCharles J. Fillmore. 1988. The mechanisms of “construction grammar”. In Annual Meeting of the Berkeley Linguistics Society, volume 14, pages 35–55.\nPaul Kay and Charles Fillmore. 1999. Grammatical constructions and linguistic generalizations: The what’s X doing Y? construction. Language, 75(1):1–33.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 1–9, Red Hook.\nLuc Steels. 2004. Constructivist development of grounded construction grammar. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 9–16.\nLuc Steels and Joachim De Beule. 2006. Unify and merge in Fluid Construction Grammar. In Symbol Grounding and Beyond, Third International Workshop on the Emergence and Evolution of Linguistic Communication, pages 197–223. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30 (NIPS 2017), pages 6000–6010."
          }
        ]
      },
      {
        "track": "Track D",
        "title": "Machine Translation",
        "talks": [
          {
            "time": "09:15–09:35",
            "title": "Context-Aware Speculative Decoding: Accelerating Inference in High-Copy Environments",
            "author": "Gianni Van de Velde, Cédric Goemaere, Chris Develder, Thomas Demeester",
            "abstract": "In recent years, Large Language Models (LLMs) have proven to be useful for many tasks, leading to wide adoption. The increased quality that made this possible, can be partially attributed to the models being scaled to sizes that were unseen before. Yet, in pace with the increasing model sizes, energy consumption went up, whereas inference speeds went down. Speculative decoding is a proven method that can alleviate both issues. It parallelizes the decoding of the next tokens by speculating multiple tokens ahead, and verifying all speculations in the same forward pass through the LLM.\n\nMany speculators have been made to try and predict what the generator might generate next. While most recent research focuses on speculating for general LLM behaviour, we focused on the recently popular Retrieval-Augmented Generation (RAG) applications. For such applications, the generator may literally copy phrases from the context. Building on this intuition, we propose a new method: Context-Aware Speculative Decoding (CASD), which is a hybrid method that augments the current state-of-the-art (SOTA) with a simple context copying mechanism. CASD detects when the generator has already copied a piece of the context, and then it speculates that the next tokens will continue copying from that context.\n\nTests on Llama 3 show that CASD speeds up decoding with 8-17% for the RAG tasks, when augmenting the current SOTA speculative decoder. This while adding limited overhead, seen by a slowdown of only 4% when applied to general benchmarks. Not only does it show that CASD successfully increases performance; it might also indicate that hybrid solutions are underexplored in the domain of speculative decoding. In the future, we see great potential for CASD to augment speculative decoders in RAG use cases, certainly in low-resource languages."
          },
          {
            "time": "09:35–09:55",
            "title": "Linguistic Richness and Generative AI Translation: A Comparative Study of English-Dutch Across Domains",
            "author": "Dimitar Shterionov, Noa van Helleman, Damen Naval, Eva Vanmassenhove",
            "abstract": "Generative AI tools, such as ChatGPT and DeepSeek, are taking the world by storm. Their use spans across a wide range of language-related tasks and their user community increases exponentially. Their impressive performance has led to incredible amounts of media attention, a widespread usage of these tools and a paradigm shift in the general domain of AI (Schneider et al., 2024) where a large number of tasks are now carried out by adjusting existing general-purpose foundation models rather than by creating or fine-tuning task-specific ones. This, by default, implies some form of homogenization since most of these models are owned by a few organizations and trained on a few (albeit extremely large) datasets. It furthermore implies that defects of these foundation (and large language) models very likely trickle down to the `child' models adapted for specific downstream tasks (Bommasani et al., 2021). Their increased adoption by a wide-range of different users (including researchers) at a large scale along with uncovered limitations in terms of the assessment of these models, calls for a similar shift in our evaluation of our foundation and large language models (LLMs) (McIntosh et al., 2024) and derivatives thereof.\n\nWhile it is generally accepted that machine translated text is of lower quality than translations generated by human (professional) translators, the seminal work of Vanmassenohve et al. (2019) and Toral et al. (2019), linked the exacerbation of bias and the loss of lexical richness to the statistical nature of MT. Now, we take this research to the next level and look at the loss of lexical richness when multilingual LLMs (MLLMs) are used for translation – the second most common task for (M)LLMs nowadays. Our work is centred around the question: How do translations generated by different multilingual LLMs compare on an English to Dutch translation task for texts from different domain categories – news articles, literature and poetry? Answering this question, we provide an updated view on the performance and relevance of some of the most widely used models on the task of translating different types of texts. In particular, we compare the translation performances of mBART (Liu et al., 2020), Jamba-1.6-large (Lieber et al., 2024), GPT4o (Achiam et al., 2023), and DeepSeek (Liu et al., 2024), on three types of texts: news articles, literature and poetry in the translation direction English (EN) to Dutch (NL). We evaluate the translations using standard MT evaluation metrics BLEU, TER, chrF; we also measure and analyse metrics reflecting the lexical richness and complexity of the translations (TTR, Yule's I, MTLD, Shannon, Inv Simpson, Simpson, vocabulary sizes and most common words), following the methodology of Vanmassenhove et al. (2021). Combining the analyses of the evaluation metrics and of the lexical richness metrics we provide a detailed view on the quality of the generated text, reflecting on the capacity of these models.\n\nREFERENCES:\nAchiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida et al. \"Gpt-4 technical report.\" arXiv preprint arXiv:2303.08774 (2023).\n\nLiu, Aixin, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao et al. \"Deepseek-v3 technical report.\" arXiv preprint arXiv:2412.19437 (2024).\n\nLieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz, S. and Abend, O., 2024. Jamba: A hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887.\n\nLiu, Yinhan, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. \"Multilingual denoising pre-training for neural machine translation.\" Transactions of the Association for Computational Linguistics 8 (2020): 726-742.\n\nMcIntosh, Timothy R, Teo Susnjak, Nalin Arachchilage, Tong Liu, Paul Watters, and Malka N Halgamuge. 2024. Inadequacies of large language model benchmarks in the era of generative artificial intelligence. arXiv preprint arXiv:2402.09880.\n\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n\nSchneider, Johannes, Christian Meske, and Pauline Kuss. 2024. Foundation models: a new paradigm for artificial intelligence. Business & Information Systems Engineering, pages 1–11.\n\nToral, A. (2019, August). Post-editese: an Exacerbated Translationese. In Proceedings of Machine Translation Summit XVII: Research Track (pp. 273-281).\n\nVanmassenhove, Eva, Dimitar Shterionov, and Andy Way. “Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation.” In Proceedings of Machine Translation Summit XVII: Research Track, pages 222–232, Dublin, Ireland. European Association for Machine Translation, 2019.\n\nVanmassenhove, Eva, Dimitar Shterionov, and Matthew Gwilliam. \"Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation.\" In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 2203-2213. 2021."
          },
          {
            "time": "09:55–10:15",
            "title": "WordNet and SignNet: dealing with Concepts, Glosses and Standard Language Variety",
            "author": "Ineke Schuurman, Caro Brosens, Kris Heylen",
            "abstract": "Machine translation for Spoken Languages (SpLs) is omnipresent, even for smaller languages, but not for Sign Languages (SL), neither to/from Spoken Languages (SpL) or another SL. One reason: by far not enough data to train translation systems! How  to facilitate  building such resources?  Word/Sign-Nets to the rescue!\n\nSLs are independent natural languages. However, SLs being minority languages surrounded by SpLs, a certain level of cross-linguistic, cross-cultural influence is unavoidable.\n\nOne complicating factor: SLs don’t have an orthography. Certain phonological notation systems exist, e.g. HamNoSys or SignWriting, to describe the parameters of a sign, i.e. handshape, movement, orientation, location. These are mostly used by linguists and researchers and not widely adopted among signers.\n\nDictionaries for SLs do exist, making use of glosses written in for example the surrounding SpL. This gloss acts as a label of the sign. Often some more words are presented, so-called key words. They are used to search for synonyms etc. in a dictionary.\n\nFor example in VGT there is a sign glossed as BOER, with key words 'boer' (farmer) and 'landbouw' (agriculture). Another sign has gloss SUPPORTER, and comes with key words supporteren, aanhanger, fan, aanmoedigen, supporter. In SpL these nouns and verbs belong to a series of synsets in a WordNet.\n\nA sign expresses a concept, just like a word, but often sign concept is broader. How to deal with that when linking signs and words: A sign should not be split up in order to fit in a WordNet!\n\nOur approach: one ‘umbrella’-identifier per international SL-concept. For SUPPORTER, that could be ‘vgt.911659’. The umbrella-id starts with ‘vgt’, at international level with ‘sl’. So we are not linking signs to WordNet, but rather the other way around: words to SignNet.\n\nSeveral SpL entries are to be added, especially representing expressions typically for the Deaf community, like APPLAUS-DOOF vs APPLAUS-HOREND. Or a sign expressing a vertical vs horizontal movement. Such a distinction is not always present in Open Multilingual WordNet (OMW)/Open Dutch WordNet (ODWN). so we’ll add it for the time being to our own interim version of ODWN.\n\nAnother issue: VGT being used in Flanders, the signs should be connected with the word(meanings) used there. For example, the verb 'lopen' means 'to walk' in the Netherlands, and 'to run' in Flanders; 'gracht' usually refers to 'canal' in the Netherlands, and 'ditch' in Flanders. Just to avoid confusion: chipolata - sausage vs type of pastry, voormiddag - morning vs first part of afternoon!\n\nThese meanings are standard in Flanders, as seen on television, in formal education, etc.: It is just a Standard Language Variety. The same holds for magnetron/microgolf, lavabo, zever, charcuterie. As it will take long before such words/wordmeanings are included in the OMW, we’ll add them in our interim adapted version of ODWN. Ideally (meanings of) words just used in the Netherlands should also be marked as such. So we’re adding geography=\"belg\" or \"dut\" to ODWN-entries whenever there is a distinction."
          },
          {
            "time": "10:15–10:35",
            "title": "CoCoS project. From Collaboration to Co-creation for Sign Language Machine Translation Research",
            "author": "Mirella De Sisto, Dimitar Shterionov, Lisa Lepp, Ifigeneia Mavridou, Phillip Brown",
            "abstract": "Despite the latest progress in sign language technologies and machine translation, the involvement of deaf and hard of hearing communities (DHH) in the research and development of these tools has been marginal or very limited. This lack of involvement has often led to the creation of tools which do not address user communities' needs and do not acknowledge (or to a very limited extent) their cultures and language use. 'From Collaboration to Co-creation for Sign Language Machine Translation Research' (CoCoS) is an ongoing project in collaboration with the Nederlands Gebarencentrum (NGC) with the objective of addressing this gap.\n\nThe CoCos project aims to further the involvement of the DHH community by engaging users of Nederlandse Gebarentaal (NGT, Sign Language of the Netherlands) in the development and evaluation of three data processing strategies (i.e. Motion Capture (MoCap), OpenPose and MediaPipe for sign language input (i.e. video). The goals of this comparison, besides identifying the best strategy from the user's perspective, are to i) compare this with a model-centered perspective and ii) single out the keypoints necessary for a good performance.\n\nThe project builds on the idea of co-creation, initially outlined in the SignON project and developed in Lepp et al. (2025). Starting from acknowledging that 'data and communities are not separate things' (Caselli et al. 2011:29), our approach promotes active involvement (of users and stakeholders) from a project’s initial phase. We take the user's perspective and needs as the driving force shaping methodological decisions and design directions.\n\nWith this goal in mind, we first conducted a discussion with NGC (to shape objectives and discuss use-cases); we then picked a list of 44 isolated signs from the field 'supermarket' which contains the complete inventory of phonological parameters characterizing NGT. Pre-existing videos of these signs were provided by NGC. We recorded the 44 signs in two conditions: i) with a 'start' and 'end' sign preceding and following the sign, and ii) just producing directly the sign. These two conditions will be used to identify the exact starting and ending frame of each sign (regarding challenges of sign-boundaries identification see Hanke et al. (2012). Additionally, we collected utterances of continuous signing in the form of semi-structured dialogue containing these 44 signs.\n\nBoth data collections are performed using MoCap (Optitrack system) and video recordings. Video data will be processed using OpenPose and MediaPipe, while high-resolution spatial motion data—captured via MoCap with 50 body markers—will provide detailed kinematic input. The combined data streams will be integrated into a pipeline for generating an avatar that reproduces the recorded human movements and signs. The avatar outputs will be compared from two perspectives: i) human-user evaluation and ii) machine-performance evaluation. Another goal is to identify the keypoints which are fundamental for accurate processing; this allows to significantly reduce the size of data inputs for comparable pipelines. The identification of the best strategy will provide a standard for the development of future projects, and particularly those targeting machine translation between signed and spoken or signed and signed languages."
          }
        ]
      }
    ]
  },
  {
    "type": "break",
    "start": "10:35",
    "end": "11:45",
    "title": "Coffee + Poster Session 1"
  },
  {
    "type": "plenary",
    "start": "11:45",
    "end": "13:00",
    "title": "Keynote: Marie-Catherine de Marneffe",
    "abstract": "Consensus is a myth: Human label variation in Natural Language Inference"
  },
  {
    "type": "break",
    "start": "13:00",
    "end": "14:00",
    "title": "Lunch"
  },
  {
    "type": "session",
    "start": "14:00",
    "end": "15:20",
    "events": [
      {
        "track": "Track A",
        "title": "Low-resource and multilingual NLP",
        "talks": [
          {
            "time": "14:00–14:20",
            "title": "Translation as a Bridge: Assessing the Feasibility of English BERT for Low-Resource Languages",
            "author": "Giulia Rivetti, Hielke Muizelaar, Marcel Haas, Marco Spruit",
            "abstract": "BERT models represent a significant advancement in Natural Language Processing (NLP), establishing themselves as state-of-the-art due to their robust ability to handle un- structured text across diverse languages and domains. However, developing high-quality BERT models for non-English languages remains a major challenge, often requiring ex- tensive computational resources and large annotated datasets, resources that are scarce for many minority or low-resource languages. A promising alternative to building sepa- rate language-specific models is to translate non-English data into English and leverage existing, pre-trained English BERT models. Because these English models are typically trained on broader and more diverse corpora, they often offer improved generalization and robustness. Although initial studies suggest that translation-based approaches can yield competitive or even superior results compared to native-language models, research in this area remains limited, with most efforts still focused on developing dedicated mod- els for each language. This thesis investigates whether translating text into English and fine-tuning English BERT models can serve as a viable and scalable strategy for multi- lingual NLP. We evaluate this approach across six core NLP tasks (Sentiment Analysis, Hate Speech Detection, Question Answering, Named Entity Recognition, Part-of-Speech Tagging, and Natural Language Inference), using datasets translated from five typologically diverse languages: Bulgarian, Chinese, Dutch, Italian, and Russian. Our findings indicate that translation-based models match or surpass native-language BERT models in many cases, particularly in tasks like POS tagging and QA, where lexical semantics and structural alignment are less sensitive to translation artifacts. Performance was especially promising for Dutch, likely due to its linguistic proximity to English. In contrast, results for Chinese were consistently weaker, reflecting the greater typological distance from English and the presence of strong native models trained on extensive Chinese corpora. Moreover, tasks requiring fine-grained token-level precision or cultural nuance, such as NER and Hate Speech Detection, tended to suffer under the translation-based method, revealing clear limitations. While this approach proved effective in 57% of the evaluated cases and demonstrates real potential for scalable, resource-efficient multilingual NLP, this thesis also highlights its boundaries. The results show that translation isn’t a universal solution and requires careful consideration, particularly in contexts where it could introduce ambiguity or when robust native-language models already exist. Nonetheless, the translation-based strategy remains a valuable tool for extending NLP capabilities to underrepresented languages and contributes to ongoing efforts toward linguistic inclusivity and sustainability in AI."
          },
          {
            "time": "14:20–14:40",
            "title": "TIPA: Typologically Informed Parameter Aggregation",
            "author": "Stef Accou, Wessel Poelman, Miryam de Lhoneux",
            "abstract": "Massively multilingual language models have extended the reach of NLP, but they still perform poorly on the majority of the world's languages. Low-resource languages, in particular, often lack dedicated models and sufficient training data, excluding them from multilingual NLP systems. While parameter-efficient fine-tuning methods offer modular solutions, they typically require per-language training, limiting scalability.\nThis work proposes a generalisable and training-free approach to improve multilingual model performance across typologically diverse and under-represented languages: Typologically Informed Parameter Aggregation (TIPA). TIPA constructs proxy language adapters by combining existing modules, using typological similarity to guide parameter aggregation. These proxy adapters can be integrated with task adapters in a frozen Transformer model, enabling zero-shot cross-lingual transfer without additional training.\nWe evaluate TIPA on five NLP tasks across 230+ languages. The method consistently outperforms strong baselines, including English-only fine-tuning and the use of typologically closest adapters. Gains are largest for languages without existing dedicated adapters and those typologically distant from training languages.\nTIPA demonstrates that typological information can be leveraged to scale multilingual NLP efficiently with model arithmetic methods, offering a practical alternative to per-language adaptation and advancing inclusive multilingual NLP."
          },
          {
            "time": "14:40–15:00",
            "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs",
            "author": "Jaap Jumelet, Leonie Weissweiler, Joakim Nivre, Arianna Bisazza",
            "abstract": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages, 6 linguistic phenomena and containing more than 125,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. Universal Dependencies provide syntactic structure that serves as a starting point for identifying linguistic constructions. We then use this information to create ungrammatical counterparts to a sentence, by inflecting words for a feature using the UniMorph annotation scheme. By comparing a LLM's probability judgment of the grammatical and ungrammatical sentence, we can draw insights into their understanding of the underlying agreement conditions.\n\nMultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages. We demonstrate salient differences across contemporary LLMs, highlighting the impact of pre-training, post-training and model size on multilingual performance."
          },
          {
            "time": "15:00–15:20",
            "title": "Multilingual Pretraining for Pixel Language Models",
            "author": "Ilker Kesen, Jonas Lotz, Ingo Ziegler, Phillip Rust, Desmond Elliott",
            "abstract": "Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages."
          }
        ]
      },
      {
        "track": "Track B",
        "title": "LLMs",
        "talks": [
          {
            "time": "14:00–14:20",
            "title": "Simplifying News, Losing Nuance? The Impact of Automatic Text Simplification on Linguistic Devices for News Text",
            "author": "Reshmi Pillai, Antske Fokkens, Mariken van der Velden",
            "abstract": "News plays a vital role in cultivating and maintaining an active and aware citizenry, which is an essential requirement of a democratic society. Audience preferences and characteristics (e.g. non-native readers, individuals with cognitive challenges) often necessitate simplification of news text. Despite promising empirical results in the use of Large Language Models for text simplification, two key research gaps remain. First, current analyses and evaluations of simplified outputs rarely account for the specific linguistic devices commonly employed in news discourse. Second, most studies on the performance of text simplification models rely on datasets derived from Newsela or Wikipedia. However, these datasets do not realistically reflect the linguistic characteristics and complexity of everyday news reporting. There, thus, is a lack of publicly available datasets featuring simplified versions of news texts, limiting the reproducibility and broader applicability of existing research in newsrooms. In this study, We take a first step in addressing these research gaps; we assess language models to determine how well they preserve the essential linguistic components of news text. We explore the following research question. \nRQ : To what extent does text simplification based on large language models preserve linguistic devices when simplifying news text?\nWe address this as follows. Based on a literature review on language used in news text, we choose nine different linguistic devices in four categories for our analysis : 1. Grammatical (Tense, Voice (active/passive), sentence mood) 2. Stylistic (Modal words) 3. Affective (Sentiment score, Change in sentiment intensity, Subjective/objective) 4. Informational (Content preservation, Named entities). We compiled a new dataset of 500 sentences drawn from news reports accessed via the Nexis Uni news archive. These sentences were simplified using seven diverse language models, each prompted individually. Using automated methods and human annotation, we explored the retention of nine linguistic characteristics in a dataset consisting of 500 original sentences and 3,500 simplified sentences generated using 7 different LLMs. The results showed varying performance of the LLMs in their ability to maintain linguistic characteristics that contribute to the original sentence's message. No single LLM scores consistently high in all the features we examined. Our findings motivate linguistic fidelity to be incorporated in simplification evaluation metrics, particularly in text domains like news reports."
          },
          {
            "time": "14:20–14:40",
            "title": "Examining the Influence of Support and Answer Positioning on LLM's when Answering Multiple Choice Questions",
            "author": "Nathalie de Palm, Leo Zotos, Malvina Nissim",
            "abstract": "Language Models have been greatly developing in the field of Question Answering. Recently, their increase in knowledge and resilience to the diversity of prompts has brought interesting findings. With this contribution, we present results on the influence of Support and Answer Positioning on language models when answering multiple choice questions.\n\nWe make use of two language models of different sizes to answer multiple choice questions from a Science-based data set. As the data set includes a passage that supports each question, we explore the effects that the positioning of relevant information has on the performance of the models. Furthermore, we explore whether the positioning of the correct answer also has an effect. To study these influences, we approach this task by positioning the relevant support in three different locations. Each experiment is also run twice, where in the first instance, the correct answer is A, and in the second, the correct answer is D. The metric used to evaluate these results is the accuracy of the models. These will be compared to one another to examine the difference to the performance these slight changes make. These results are the focus of the CLIN presentation."
          },
          {
            "time": "14:40–15:00",
            "title": "How creative are language models with the Dutch language?",
            "author": "Pauline van Nies, Lisa van der Goes, Janneke van der Zwaan, Chenghong Yang, Marten Koopmans, Paul Verhaar",
            "abstract": "When choosing a language model for a business context, it is important to know how well the language model performs on Dutch texts. The EuroEval ranking summarizes the evaluation on Dutch benchmark datasets on tasks such as summarization, reading comprehension, knowledge, linguistic acceptability, and common-sense reasoning. We are interested in evaluating and optimizing language models on a complex task that requires linguistic insight and creativity. Therefore, we set up an evaluation pipeline for language models to test their ability to solve cryptic crosswords and clues. In this presentation, we will share the results of comparing different language models (commercial ones like: gpt-family, mistral, gemini, claude and open source models) and optimization approaches on performing this specifc task. With the zero-shot approach, the state-of-the-art language models only solve ~25% of the cryptic clues correctly (gpt-4.1-mini, gpt-4o, claude-3.5-sonnet). We apply standard optimization search methods like adjusting the temperature and using Chain-of-thought, adding domain expertise and the few-shot approach. Additionally, we apply automated prompt engineering using the DSPy framework. We will showcase this approach of systematically experimenting, tracking results (mlflow) and optimizing a language model and prompt for a specific task.\n\nQuestions that will be answered are:\nCan (and which of) the language models benefit from examples and writing out their reasoning?\n- What is the effect of temperature on the different models on solving this creative task?\n- What is the difference in performance between standard chat completion models and reasoning models?\n- What is the best optimization method and prompt to solve the cryptic clues?\n- How much better do the answers become if language models have access to a (puzzle) dictionairy?\n\nThis will give us a better understanding of the capabilities and bottlenecks of the tested language models, as well as an interesting insight in how language models solve a task that in humans is described as a feeling of ‘the solution just falls in place’."
          },
          {
            "time": "15:00–15:20",
            "title": "Making LLM evaluation tangible for non-technical professionals",
            "author": "Michael Bauwens, Heike Pauli, José Tummers",
            "abstract": "Many SMEs with an innovative mindset, but limited AI knowledge, feel compelled by fear of missing out (FOMO) to join the hype around generative AI (GenAI). As a result, tools like ChatGPT Team or Copilot for Microsoft 365 are often purchased impulsively, without thorough evaluation (Mohanty, 2023). This approach overlooks the importance of an actual analysis to determine whether the acquired system truly meets expectations and is worth the investment. This issue is particularly relevant for GenAI systems based on Large Language Models (LLMs), of which the output is open-ended. Such systems do not deliver straightforward answers, but create outputs that vary depending on the task, such as text writing (Duranton, 2024).\n\nCurrently, users often evaluate these systems based on initial impressions or feelings - a so-called vibe check (Dunlap et al., 2024). While understandable, this \"gut feeling\" is a limited approach. Users frequently dismiss a system or prompt prematurely due to disappointing results, often because testing was only ad hoc and on a limited basis. This leads to incorrect conclusions and missed opportunities, while the effectiveness of a system strongly depends on the specific task, context, and application.\n\nIn this project, we aim to develop an accessible holistic evaluation framework that helps non-technical end-users in Flemish SMEs to systematically assess the performance of GenAI systems. The framework tests the open-ended output of these systems against specific tasks, expectations and predefined example answers. As a result, end-users gain insight into not just one aspect of performance, but can analyse the system and its accompanying prompts across multiple dimensions. The framework offers concrete guidelines for identifying and understanding deviations from desired outcomes. Through this structured approach, businesses obtain a nuanced view of the strengths and weaknesses of AI tools, leading to more informed decisions.\n\nThis project is a work in progress, but some hands-on insights can already be shared from our preliminary research. As a pilot study, we’ve set up an evaluation framework for a software development partner that has built a platform for user-friendly GenAI-applications for social workers, amongst others. Evaluation within our framework proceeds in three phases: (1) an isolated pre-evaluation, (2) evaluation during usage (e.g., in a pilot test), and (3) actions that can be taken post-evaluation. Certain aspects of evaluation are particularly relevant for developers or technical profiles, while others are essential for end users of the GenAI systems.\n\nSecondly, we’ve researched ways to implement the LLM-as-a-judge system G-Eval (Liu et al., 2023) in a manner that is useable for non-technical end-users. We’ll show the evaluation platform that has been set up in cooperation with students of our programme of applied computer science and discuss the limitations. These usecases serve as an example of how the project enables SMEs to optimize their investments in GenAI and select tools that align more closely with their specific business objectives. By doing so, it prevents waste, boosts productivity, and strengthens confidence in generative AI technology as a sustainable solution.\n\n\nSources:\n\nDunlap, L., Mandal, K., Darrell, T., Steinhardt, J., & Gonzalez, J. E. (2024). VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models (arXiv:2410.12851). arXiv. https://doi.org/10.48550/arXiv.2410.12851\n\nDuranton, S. (2024). Beyond Accuracy: The Changing Landscape Of AI Evaluation. Forbes. https://www.forbes.com/sites/sylvainduranton/2024/03/14/beyond-accuracy-the-changing-landscape-of-ai-evaluation/\n\nLiu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C. (2023). G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment (arXiv:2303.16634). arXiv. https://doi.org/10.48550/arXiv.2303.16634\n\nMohanty, S. (2023). Council Post: From FOMO To JOMO With GenAI: Setting Up A Structure That Scales. Forbes Technology Council. https://www.forbes.com/councils/forbestechcouncil/2023/10/05/from-fomo-to-jomo-with-genai-setting-up-a-structure-that-scales/"
          }
        ]
      },
      {
        "track": "Track C",
        "title": "Linguistics 2",
        "talks": [
          {
            "time": "14:00–14:20",
            "title": "Studying verb constructions in an automatically parsed corpus of 150 years of written Dutch",
            "author": "Gerlof Bouma, Evie Coussé, Gertjan van Noord",
            "abstract": "Dutch verb constructions show a great amount of variation. Examples are: verb order (1), optionality of infinitive marker \"te\" (2), alternations in verb form (3), and the position of verb particles (4).\n\n1) ...omdat   ik al           een ijsje           heb   gegeten / gegeten heb.\n      because I   already an    ice cream have eaten        eaten     have\n     ‘...because I have already eaten an icecream.’\n\n2) Ik weet  niet  waar  ik moet beginnen (te) zoeken.\n     I  know  not  where I  must start          TE  look\n     ‘I don't know where to start looking.’\n\n3) Ze     hebben het proberen / geprobeerd te  repareren.\n    They  have     it    try.INF        try.PRFP      TE repair\n    ‘They have tried to repair it.’\n\n4) Hoe zal      ik dat  ooit kunnen goed-maken / goed kunnen maken?\n     How shall  I  that ever can       well-make       well can          make\n     ‘How will I ever be able to make up for that?’\n\nThis – typically contextually, lexically, and/or regionally conditioned – variation has been well-described, synchronically and diachronically. However, we have relatively little knowledge about how these and other verb construction variations have changed in recent language history, in the last ~200 years.\n\nThis talk presents an on-going project that studies this variation in the CCLAMP corpus (Piersoul et al. 2021), approximately 180 million words of literary and linguistic journal text from Belgium and The Netherlands from 1837–1999.\n\nWe located verb constructions of relevance by parsing the corpus using Alpino (Van Noord 2006) and extracting constructions using XQuery. To improve parsing of the historical material, we used error mining (De Kok et al. 2009) to guide changes to Alpino itself and to the orthographic corpus preprocessing (Van Cranenburgh and Van Noord 2022). Even after this, parsing CCLAMP remains a challenging task: only 84% of sentences receive a full parse. For context: on present-day newspaper text (in-domain for Alpino) this is as high as 96%.\n\nNevertheless, manual inspection of extracted data shows we can identify verb constructions with great precision. A first look at trends in the data confirms verb order findings of Coussé (2008). We are therefore confident our computer-assisted methodology can support the planned corpus study.\n\nIn the talk, we will discuss corpus processing, data extraction and quality assessment, as well as some early corpus study results.\n\n—\nCoussé, Evie. 2008. Motivaties voor volgordevariatie. Een diachrone studie van werkwoordsvolgorde in het Nederlands. Doctoral dissertation, Gents universiteit.\n\nde Kok, Daniël, Jianqiang Ma and Gertjan van Noord. 2009. A generalized method for iterative error mining in parsing results. In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, pp71–79\n\nPiersoul, Jozefien, Robbert De Troij and Freek Van de Velde. 2021. 150 years of written Dutch: The construction of the Dutch Corpus of Contemporary and Late Modern Periodicals. Nederlandse Taalkunde 26, pp339–362.\n\nvan Cranenburgh, Andreas and Gertjan van Noord. 2022. OpenBoek: A Corpus of Literary Coreference and Entities with an Exploration of Historical Spelling Normalization. CLIN Journal 12.\n\nvan Noord, Gertjan. 2006. At last parsing is now operational. In Verbum Ex Machina. Actes de la 13e conference sur le traitement automatique des langues naturelles, pp20–42."
          },
          {
            "time": "14:20–14:40",
            "title": "Morphosyntactic variation: recognising and characterising the differences automatically",
            "author": "Floris Nijhuis, Freek Van de Velde",
            "abstract": "The availability of computer-based methods has enabled more advanced analyses in many fields of linguistics. This approach is called dialectometry when applied to ‘micro-variation’ (i.e., dialects). Dialectometry, like dialectology itself, has been employed to examine differences in pronunciation (Heeringa, 2004), morphosyntax (Spruit, 2006), and lexicon (Spruit et al., 2009). Nonetheless, morphosyntax has received comparatively little attention in this respect (Wieling & Nerbonne, 2015). The limited work in this area can be broadly divided into two categories: work based on questionnaires (i.e., elicited data) and work based on corpora. Even though research in the former category is rare, the latter is even rarer. Finding and characterising morphosyntactic anomalies in a large corpus is not trivial, let alone doing so computationally and in an unsupervised manner. The mere fact that this type of research is complicated does not detract from its importance. Spontaneous data can sometimes tell much more about the actual language structure than elicited data.\n\nRecently, a large historical collection of Southern Dutch dialectal recordings was digitised (Breitbarth et al., 2024). This collection – the spoken corpus of Southern Dutch dialects, abbreviated as GCND – provides a unique dataset, not merely because of its size (in total, 650 recordings), but also because it is fully morphosyntactically parsed. Geographically, this corpus ranges from Northern France to the great rivers in the Netherlands. To this day, no comprehensive aggregated analysis of this dataset has been conducted.\n\nThis contribution presents various approaches to fill this gap, demonstrating what works and, explicitly, what does not. We aim not only to answer the question of which features define certain dialects, but also how similar dialects are overall. More specifically, we use more ‘naive’ methods, such as frequency distribution, linear POS-n-grams and various tree-path measures, as applied earlier by Sanders (2010) to analyse (dis)similarity among Swedish dialects.  Moreover, we also employed and adapted existing, more complicated pattern mining techniques (Kroon, 2022; Tatti & Vreeken, 2012). We apply all outcomes of these analyses to the toolbox of dialectometry: dimension reduction and cluster analysis (Heeringa, 2004).\n\nOddly, we found that the ‘simpler’ approaches, such as n-grams, outperformed the more complicated pattern mining approaches and clearly show (geographic) structure in the distribution of morphosyntactic features. Additionally, we reaffirm that striking the delicate balance between detail and generalisability is pivotal. As is often the case: less is more (Nerbonne, 2024)."
          },
          {
            "time": "14:40–15:00",
            "title": "Romanian differential object marking (DOM): A multi-factorial corpus study",
            "author": "Maria Tepei, Jelke Bloem, Marieke Schouwstra, Eva van Lier",
            "abstract": "Differential object marking (DOM) is a cross-linguistically widespread phenomenon that sheds light on how languages encode information about participants in events (‘who does what to whom’). In Romanian, DOM refers to the phenomenon by which certain direct objects are overtly marked with the accusative marker pe, with or without a doubling clitic, when they meet specific semantic or syntactic criteria such as animacy, definiteness, or specificity (Hill & Mardale, 2021). While DOM phenomena are widespread across languages, the conditions under which DOM emerges remain only partially understood — especially in light of the significant cross- and intra-linguistic variation it displays.\nFor Romanian, previous studies have hypothesised animacy, specificity, definiteness, and some syntactic contexts as the main factors triggering DOM (Bleam, 2005; Chiriacescu & Heusinger, 2009; David, 2015; Hill & Mardale, 2019; Irimia, 2023, a.o.); however, most of these accounts tend to examine potential factors in relative isolation, which often results in an incomplete picture of the phenomenon. Moreover, work on DOM in Modern Romanian has not focused on quantitative, corpus-driven approaches,  which results in a limited coverage of DOM patterns across a range of real-world contexts and genres. As such, we still lack a comprehensive, usage-based account of Romanian DOM that models multiple underlying factors and their interaction.\nIn this study, we conduct a multifactorial analysis of Romanian DOM by surveying the theoretical and experimental literature to identify all factors previously hypothesised to influence DOM, including semantic (e.g. animacy), morphosyntactic (e.g. presence of a determiner), and discourse-structural properties (e.g. givenness, referential persistence), among others. Using a large collection of recent Romanian text from various sources (literary, news, Wikipedia), we construct a custom corpus of ~2.3M sentences, which we automatically annotate for all variables, using Python-based annotation (e.g. BERT for NER, Dumitrescu & Avram, 2020) and parsing (Stanza; Qi et al., 2020) tools, and we use these features as predictors in a mixed-effects logistic regression model. \nWe hypothesise that DOM marking will be strongly associated with animacy and definiteness, in line with previous findings from synchronic and diachronic studies on Romanian, as well as with findings from DOM in other Romance languages. However, we also expect to find significant effects of discourse-related properties such as topicality or word order (e.g. pre-verbal positioning), as a result of cognitive factors influencing language production. Overall, by modelling DOM as a product of multiple interacting factors, we hope to contribute to a more nuanced and empirically grounded understanding of this variation in Modern Romanian, as well as to add further quantitative evidence in broader theoretical discussions about the nature of DOM across languages, the interplay between cognition and language production, while demonstrating the value of multifactorial approaches in the analysis of linguistic phenomena.\n(word count: 434 words)\n\n\nBleam, T. (2005). The Role of Semantic Type in Differential Object Marking. Belgian Journal of Linguistics, 19, 3–27. https://doi.org/10.1075/bjl.19.03ble\nChiriacescu, S., & Heusinger, K. V. (2009). Pe-marking and referential persistence in Romanian. Universität Stuttgart. https://doi.org/10.18419/OPUS-5710\nDavid, O. (2015). Clitic doubling and differential object marking: A study in diachronic construction grammar. Constructions and Frames, 7(1), 103–135. https://doi.org/10.1075/cf.7.1.04dav\nDumitrescu, S. D., & Avram, A.-M. (2020). Introducing RONEC -- the Romanian Named Entity Corpus (No. arXiv:1909.01247). arXiv. https://doi.org/10.48550/arXiv.1909.01247\nHill, V., & Mardale, A. (2019). Patterns for differential object marking in the history of Romanian. Journal of Historical Syntax, Vol 3 No 5, 1-47 Pages. https://doi.org/10.18148/HS/2019.V3I5.25\nHill, V., & Mardale, A. (2021). The Diachrony of Differential Object Marking in Romanian. Oxford University Press. https://doi.org/10.1093/oso/9780192898791.001.0001\nIrimia, M. A. (2023). Interactions between Differential Object Marking and Definiteness in Standard and Heritage Romanian. Languages, 8(1), 63. https://doi.org/10.3390/languages8010063\nQi, P., Zhang, Y., Zhang, Y., Bolton, J., & Manning, C. D. (2020). Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. https://nlp.stanford.edu/pubs/qi2020stanza.pdf"
          },
          {
            "time": "15:00–15:20",
            "title": "PyFCG: Fluid Construction Grammar in Python",
            "author": "Paul Van Eecke, Katrien Beuls",
            "abstract": "The PyFCG software library (Van Eecke & Beuls 2025) was recently pre-released as an open source library that ports Fluid Construction Grammar (FCG - Steels 2004, Beuls and Van Eecke 2023) to the Python programming language. PyFCG enables its users to seamlessly integrate constructional language processing and learning into Python programs, and to combine this functionality with that of other libraries within Python's rich ecosystem. The library is publicly distributed as a pip-installable package and was designed to feel 'native' to Python users rather than familiar to users already accustomed to FCG's Common Lisp reference implementation.\n\nAs a computational construction grammar framework, (Py)FCG provides a collection of high-level building blocks for representing, processing and learning fully-operational construction grammars. The FCG framework is conceived as an open instrument that is not tied to a particular construction grammar theory, but that strives for compatibility with any linguistic theory that adheres to the most fundamental tenets underlying constructionist approaches to language (see e.g. Fillmore, 1988; Croft, 2001; Goldberg, 2003). As such, it subscribes to the view (i) that language users dynamically build up their own linguistic systems as they communicate with other members of their community, (ii) that these linguistic systems can be captured as a network of form-meaning mappings called constructions, and (iii) that these constructions can pair forms and meanings of arbitrary complexity and degree of abstraction, thereby facilitating a uniform handling of both compositional and non-compositional linguistic phenomena.\n\nApart from a general description of the library, we will present at CLIN three walk-through tutorials that demonstrate example usage of PyFCG in typical use cases of FCG: (i) formalising and testing construction grammar analyses, (ii) learning usage-based construction grammars from corpora, and (iii) implementing agent-based experiments on emergent communication.\n\nReferences:\n\n- Beuls, K. & Van Eecke, P. 2023. Fluid Construction Grammar: State of the Art and Future Outlook. In: Proceedings of the First International Workshop on Construction Grammars and NLP (CxGs+NLP, GURT/SyntaxFest 2023). Pages 41-50.\n- Croft, W. 2001. Radical Construction Grammar. Oxford University Press.\n- Goldberg, A. 2003. Constructions: a new theoretical approach to language. Trends in Cognitive Sciences 7(5): 219-224.\n- Fillmore, C. 1988. The Mechanisms of 'Construction Grammar'. In: Proceedings of the Fourteenth Annual Meeting of the Berkeley Linguistics Society. Pages 35-55.\n- Steels, L. 2004. Constructivist Development of Grounded Construction Grammar. In: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics. Pages 9-16.\n- Van Eecke, P. & Beuls, K. 2025. PyFCG: Fluid Construction Grammar in Python. ArXiv:2505.12920."
          }
        ]
      },
      {
        "track": "Track D",
        "title": "Applications",
        "talks": [
          {
            "time": "14:00–14:20",
            "title": "Would you be a good AI Translator? Practical insights on GenAI upskilling for Flemish SMEs",
            "author": "Michael Bauwens, Heike Pauli, José Tummers",
            "abstract": "As generative AI (GenAI), and especially Large Language Models, transform business practices, many small and medium-sized enterprises (SMEs) struggle to develop the AI maturity needed to implement GenAI tools, such as ChatGPT or Claude, effectively (De Marez et al., 2024; De Vylder et al., 2024; Standaert et al., 2024; VAIA et al., 2023). SMEs are often unaware of what GenAI entails and are concerned about possible risks (de Bellefonds et al., 2023). At GPT Academy, a project focused on practice-oriented GenAI upskilling, SMEs with low AI maturity are taught how to responsibly and confidently use GenAI in their day-to-day practices. We share practical insights from our experiences demystifying GenAI for SME decision-makers.\n\nDrawing from over 100 workshops for more than 3,500 professionals, we examine how SMEs progress from a lack of AI awareness to an understanding and successful use of the technology. We have gained significant insights into their concerns and needs on this topic and have identified key elements for lifelong learning about AI, ensuring that SMEs can ‘discover’ AI in an effective manner.\n\nA key success factor that supports upskilling is a good AI translator. We identify a profile that includes (1) credible technical knowledge on the topic, (2) engaging public speaking skills, (3) the capability to relate to the audience through hands-on industry-specific use cases (Soudi and Bauters, 2024), (4) and sensitivity to the societal impact of GenAI. These practical insights are vital for educators and trainers seeking to guide SMEs through the adoption of GenAI in a rapidly evolving digital landscape where upskilling for improved AI literacy is even required through the EU AI Act (Chapter 1, Article 4). The project contributes to the growing need of AI translators (De Ketelaere, 2021), building bridges between technology, research and business.\n\nSources:\nde Bellefonds, N., Kleine, D., Grebe, M., Ewald, C., & Nopp, C. (2023). BCG Digital Acceleration Index (DAI). Boston Consulting Group. \nDe Ketelaere, G. M. (2021). Wanted: Human-AI Translators: Artificial Intelligence Demystified. Pelckmans.\nDe Marez, L., Sevenhant, R., Denecker, F., Georges, A., Wuyts, G., & Schuurman, D. (2024). Imec.digimeter.2023. Digitale trends in Vlaanderen. imec.\nDe Vylder, P., Hallemeesch, G., Van Boxstael, T., Van Looy, J., & van den Berg, L. (2024). Trust in Generative AI: A Belgian Perspective. Deloitte Generative AI.\nSoudi, M. S., & Bauters, M. (2024). AI Guidelines and Ethical Readiness Inside SMEs: A Review and Recommendations. Digital Society, 3(1), 3.\nStandaert, T., Lecocq, C., Andries, P., & Evens, T. (2024). AI-barometer – Adoptie en gebruik van Artificiële Intelligentie bij Vlaamse bedrijven – situatie 2023. Departement EWI.\nVAIA, D’hooge, K., & BUFFL. (2023). Vraagarticulatie: Resultaten en aanbevelingen uit een bevraging van Vlaamse KMO’s. BUFFL.\n\nDisclosure of use of generative AI:\nDuring the preparation of this abstract, the authors used the following GenAI tools for the purpose of: brainstorming, rephrasing and rewriting of the text, and summarising academic articles and reports.\n-        Claude Pro (Anthropic)\n-        ChatGPT Team (OpenAI)\n-        Perplexity Pro (Perplexity)\n-        Copilot for Microsoft 365 (Microsoft)\nAfter using these tools/services, the authors reviewed and edited the content as needed and take full responsibility for the content of the abstract."
          },
          {
            "time": "14:20–14:40",
            "title": "How Multilingual News Sentiment Drives Crypto Price Prediction: A Hybrid Deep Learning Approach",
            "author": "Tanya Zhang, Vincent Vandeghinste",
            "abstract": "Bitcoin (BTC), currently the world’s seventh-largest asset by market capitalization, plays an increasingly crucial role across both traditional finance and cryptocurrency markets. With the crypto market being highly volatile and sensitive to market sentiment, understanding how public discourse impacts asset prices remains a key challenge in financial forecasting. In this study, we investigate the extent to which multilingual news sentiment can improve the prediction of short- to medium-term trends in BTC prices.\nTo this end, we constructed a rich multimodal dataset that combines cryptocurrency technical indicators, global stock market data, and news sentiment signals derived from both English and Chinese news sources. We systematically benchmarked a variety of models, including ARIMAX, Support Vector Machines (SVM), Random Forest, XGBoost, and multiple LSTM variants, evaluated on both classification and regression tasks. A hybrid deep learning architecture—combining Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), and attention mechanism—achieved the best overall performance, reaching 94.94% directional accuracy on unseen test data, along with high precision and recall.\nTo better understand the contribution of sentiment signals, we conducted ablation experiments in which the sentiment features were removed while keeping the remaining dataset and model structure unchanged. In these runs, model performance dropped significantly, with directional accuracy decreasing to near chance levels (~50%), and no actionable trading signals were produced. This confirms that multilingual sentiment provides crucial predictive power beyond technical indicators alone.\nThis research was conducted in close collaboration with industry partners, with the long-term goal of deploying the models for live trading applications. Our findings also suggest that combining multilingual sentiment with market data offers a promising avenue for further research at the intersection of financial forecasting and natural language processing."
          },
          {
            "time": "14:40–15:00",
            "title": "Issue Detection and Future Proofing Dutch Government Apps Using Language Technologies",
            "author": "Anca-Mihaela Matei, Flor Miriam Plaza-del-Arco, Natalia Amat-Lefort",
            "abstract": "As public services increasingly shift to digital platforms due to e-Government initiatives, understanding and incorporating user feedback has become critical for improving the quality and usability of government applications. Natural Language Processing (NLP) has emerged as a crucial response to the need for processing and analyzing vast and diverse user feedback, offering techniques for extracting meaningful insights from human language. Among these techniques, Large Language Models (LLMs) have become key scalable and versatile tools. They can perform a wide range of tasks, such as summarization, instruction following, and classification, without the need for extensive input preprocessing.\nBuilding on these capabilities, our research explores the application of LLMs to extract, classify, and forecast issues reported in over 5,000 user reviews from four Dutch government applications, namely KopieID, Reisapp, MijnOverheid, and DigiD. We structure this research in four core tasks: (1) issue extraction, (2) multi-label review classification, (3) assessment of how different issues impact star ratings, including a temporal analysis, and (4) forecasting of future issues and actionable recommendations. Finally, we perform a comparative analysis between LLMs and Latent Dirichlet Allocation (LDA) to evaluate coherence and classification confidence (via Shannon Entropy). In this regard, four different LLMs are compared: ChatGPT 4o-mini, Gemini (1.5-Pro and 2.0-Flash), and Mistral Large.\nOur results show that LLMs outperform LDA in coherence, flexibility, and interpretability, though challenges such as hallucination and classification ambiguity are observed. Moreover, Gemini-2.0-Flash obtained the best results in terms of overall coherence scores and was thus selected for further analyses. To assess the consistency across LLM outputs, we compute agreement metrics including Jensen-Shannon Divergence, Cohen’s Kappa, and Krippendorff’s Alpha. The star-rating assessment highlights that technical reliability (e.g., technical issues such as app stability or login difficulties) remains a key driver of user dissatisfaction, while usability-related concerns (e.g., user interface/experience problems, or missing features) exhibit more variable effects across applications. Our forecasting analysis reveals that LLMs can partially identify emerging issues and generate precise, app-specific recommendations. For example, for KopieID, the model anticipated problems related to \"Incorrect Masking/Redaction\" and \"Image Quality Issues\", while for Reisapp, it predicted \"App Stability and Performance\" and \"Language Support\" as key future concerns. Nevertheless, the prediction of issues’ frequency remains limited, and the model tended to focus predominantly on narrowly defined issues, overlooking broader concerns, which occasionally led to gaps between forecasted and actual issue frequencies.\nOur research offers a replicable, unsupervised pipeline for multilingual user feedback analysis and provides practical insights for enhancing citizen-centric digital services in the public sector. Government institutions could use the approach and results obtained from this study to identify critical pain points in their applications, create an evidence-based prioritization framework based on the evolution of discovered issues, and use focused recommendation strategies. In short, this research offers the means to move from a reactive problem-solving approach, to proactive decision-making initiatives. However, key limitations include the absence of a gold-standard labeled dataset for Dutch government applications, the inherent challenges of LLM hallucination, and the use of unoptimized prompts."
          },
          {
            "time": "15:00–15:20",
            "title": "Benchmarking LLMs for Dutch Municipal Use Cases",
            "author": "Iva Gornishka, Gossa Lo, Laurens Samson",
            "abstract": "The City of Amsterdam is researching the responsible adoption of Large Language Models (LLMs) by evaluating their performance, environmental impact, and alignment with human values. As these models are becoming increasingly relevant in organizational processes, it is crucial to assess their responsible and context-specific use. This is especially important and challenging in Dutch-language contexts, where benchmark resources are still limited.\n\nTogether with a panel of colleagues with diverse expertise, we identified and prioritized the aspects that are important when selecting LLMs for different Dutch governmental use cases. These include factuality, inclusion, performance on use-case-specific tasks such as summarization and simplification, as well as levels of openness and quality of training data. We explore these aspects, their definitions, and benchmarks that can be used to measure the desired model qualities or abilities.\n\nAiming for efficient and sustainable implementation, we include existing Dutch benchmarks or their translations whenever available. In some cases, we translate datasets ourselves - for example, to be able to use smaller, more environmentally friendly benchmark versions such as TinyBenchmarks. However, acknowledging the challenges of translating benchmarks while preserving their structure and content, we occasionally curate tailored benchmarks (semi-)manually from scratch - for example, the HonestCity benchmark, which quantifies honesty in the Dutch municipal context.\n\nFinally, we share the results on a dedicated publicly available LLM benchmarking platform. Following a user-centered approach and aiming to reduce over-reliance on minor performance differences, our LLM overview focuses on interpretable categories (honesty, inclusion, etc.) rather than individual benchmark scores.\n\nIn this work, we present our ethical considerations, results, and findings so far. Furthermore, we highlight the need for creating more Dutch benchmarks to measure diverse aspects of societal importance and to inspire the ethical use of LLMs within Dutch society."
          }
        ]
      }
    ]
  },
  {
    "type": "break",
    "start": "15:20",
    "end": "16:30",
    "title": "Coffee + Poster Session 2"
  },
  {
    "type": "session",
    "start": "16:30",
    "end": "17:30",
    "events": [
      {
        "track": "Track A",
        "title": "Language Modeling and the Humanities",
        "talks": [
          {
            "time": "16:30–16:50",
            "title": "StorytimeLM: Improving the Effectiveness and Efficiency of Language Models with Narrative Data",
            "author": "Sabijn Perdijk, Suzan Verberne, Max van Duijn",
            "abstract": "Despite the fact that the first aim of AI was to mimic human intelligence, the learning process of contemporary language models (LMs) differs considerably from that of human infants: whereas LMs use enormous amounts of internet-based training data to obtain human-like linguistic capabilities, children become competent speakers based on language input that is a few magnitudes smaller yet much more socially contextualised. To approach the data-efficient learning of children, researchers have argued for curriculum-learning or multimodal approaches, to enhance grounding linguistic items in physical contexts [4]. We combine curriculum learning with narrative training data, which can be seen as “internally-grounded”: stories naturally create an imaginative world in which lexical items and syntactic patterns gain meaning [3]. We expect this to be beneficial for the learning process of LMs.  In order to analyse the impact of building a curriculum around narrative data, the first step is to obtain a high-quality narrative dataset. The starting point for such a dataset is ChiSCor [2], a corpus of freely told Dutch fantasy stories by and for children. This corpus contains ∼70k tokens and is thus, as a stand-alone dataset, insufficient to train an LM. We therefore generate a high-quality corpus based on the existing in-domain data in ChiSCor. Research on dataset generation is, especially since the rise of LLMs, not uncommon; however, it is little exposed in the narrative domain. An additional challenge within the narrative domain is our inability to objectively measure the quality of narratives.   Therefore, in this project, we first collect and organise the various existing evaluation methods for dataset generation and assess their usefulness for narrative datasets. The evaluation methods are then divided into objective and subjective measures, exemplified by lexical and syntactic metrics for the former, and human evaluations for creativity for the latter. We test the applicability of the selected evaluation methods by comparing an in-use fully-generated narrative dataset, TinyStories [1], with a natural narrative dataset.  After clearly defining the evaluation metrics, we will create our new synthetic dataset. We explore three different generation methods to gauge its quality: lexically constrained prompting, few-shot prompting, and few-shot prompting integrated into multi-agent pipelines to leverage the strengths of various model generations.   All in all, the contributions of this research will be a clearly defined set of language-agnostic metrics to evaluate synthetic narrative datasets, a well-defined generation method for synthetic narrative datasets, and a high-quality Dutch narrative dataset, which can then be used to investigate the impact of adding narrative data to the pre-training regime of LMs, amounting to our main contribution.  [1] Eldan & Li (2023). TinyStories: How Small Can Language Models Be and Still Speak Coherent English?  [2] Van Dijk & Van Duijn (2023). ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science.  [3] Van Dijk (2025). Theory of mind in language, minds, and machines: a multidisciplinary approach.  [4] Warstadt et al (2023). Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora."
          },
          {
            "time": "16:50–17:10",
            "title": "Interpretation Modeling: Social Grounding of Sentences by Reasoning Over Their Implicit Moral Judgments",
            "author": "Liesbeth Allein, Maria Mihaela Trusca, Marie-Francine Moens",
            "abstract": "The social and implicit nature of human communication ramifies readers' understandings of written sentences. Single gold-standard interpretations rarely exist, challenging conventional assumptions in natural language processing. This work introduces the interpretation modeling (IM) task which involves modeling several interpretations of a sentence's underlying semantics to unearth layers of implicit meaning. To obtain these, IM is guided by multiple annotations of social relation and common ground - in this work approximated by reader attitudes towards the author and their understanding of moral judgments subtly embedded in the sentence. We propose a number of modeling strategies that rely on one-to-one and one-to-many generation methods that take inspiration from the philosophical study of interpretation. A first-of-its-kind IM dataset is curated to support experiments and analyses. The modeling results, coupled with scrutiny of the dataset, underline the challenges of IM as conflicting and complex interpretations are socially plausible. This interplay of diverse readings is affirmed by automated and human evaluations on the generated interpretations. Finally, toxicity analyses in the generated interpretations demonstrate the importance of IM for refining filters of content and assisting content moderators in safeguarding the safety in online discourse."
          },
	  {
	    "time": "17:10-17:30",
	    "title": "Handwritten Text Recognition for Historical Latin Manuscripts",
	    "author": "Maria Mihaela Trusca, Tim Van de Cruys, Violet Soen, Margherita Fantoli, Mark Depauw",
	    "abstract": "Handwritten Text Recognition (HTR) of historical Latin manuscripts presents unique challenges due to the variability in handwriting styles, the evolution of the language, and the degradation of documents over time. Accurate transcription of these manuscripts is essential for digital humanities, linguistic research, and historical preservation. In this work, we propose a comprehensive pipeline for robust HTR tailored to historical Latin texts, integrating modern computer vision and generative modeling techniques.\n\nThe pipeline begins with line detection, where we fine-tune a YOLOv8 model—originally pretrained for object recognition—on scanned manuscript pages. This results in a model adapted specifically to detect text lines in historical documents. Once lines are detected, we employ a TrOCR-based encoder-decoder model for HTR. Since TrOCR is a model pretrained on English, we adapt it to recognize Latin text written in various historical handwriting styles by fine-tuning on a diverse dataset using parameter-efficient modules.\n\nTo induce diversity during fine-tuning, we develop DiffWord, a novel text-to-handwriting diffusion model that generates synthetic, line-level handwriting images conditioned on both textual content and writing style. During training, DiffWord learns to predict the noise added to the latent representation of an image depicting a given text, conditioned on both style and text. To improve flexibility when one or both conditions are missing, the style and text encoders are configured to produce null embeddings in 95% of the training instances. This enables the model to support classifier-free guidance (CFG) during inference, allowing it to generate realistic handwritten lines conditioned simultaneously on style and text.\n\nOnce a diverse fine-tuning dataset is prepared, we further adapt TrOCR for Latin historical HTR using parameter-efficient modules for language and style transfer, implemented with LoRA. For the language transfer module, we first adapt TrOCR to perform historical HTR in a target language (Dutch), and then remove the differences between Dutch and Latin. Dutch is chosen due to the availability of a manuscript dataset covering the same historical period that we want our HTR model to cover. Similarly, for the style transfer module, we adapt TrOCR to recognize specific historical handwriting styles. Both modules are trained using parameter-efficient fine-tuning with LoRA and further integrated into the TrOCR architecture.\n\nFor the experiments, we rely on a heterogeneous collection of manuscripts spanning the 16th to 18th centuries. Compared to other baselines for handwritten text generation, our model shows significant improvement, primarily due to the classifier-free guidance (CFG) module conditioned on both style and text. Additional gains are achieved through the integration of the style and language transfer modules.\n\nAlthough our pipeline is designed for historical Latin manuscripts, the framework can be readily adapted to transcribe manuscripts written in other low-resource languages."
	  }
        ]
      },
      {
        "track": "Track B",
        "title": "Multi-Modal Studies",
        "talks": [
          {
            "time": "16:30–16:50",
            "title": "Using Multiple Instance Learning to Analyze Korean Historical Print",
            "author": "Aron van de Pol, Angus Mol, Jelena Prokic",
            "abstract": "In this talk we will presents an interpretable deep learning approach to analyzing printed Korean texts from the colonial period (1910–1945) using Multiple Instance Learning (MIL). While recent advances in computer vision and Distant Viewing (Arnold and Tilton, 2023a) have enabled large-scale analysis of visual sources, a persistent challenge remains: deep learning models often achieve high classification accuracy without offering insights into what visual features drive their decisions. This limits their value for humanities and linguistics research, where interpretability and feature transparency are essential.\nWe address this challenge through a novel application of MIL—a model architecture originally developed for medical image analysis—to the problem of identifying typographic distinctions among four major colonial Korean printshops. Using a corpus of over 66,000 digitized pages sourced from the Hyundam Mun’go archives, our approach classifies pages by printshop while revealing which visual elements (e.g., letterform structure, stroke patterns, spacing) contribute to those classifications.\nUnlike traditional convolutional or transformer-based models, which risk overfitting to page margins or irrelevant layout features, our attribute-constrained MIL model achieves interpretability by treating each page as a “bag” of visual instances. Each instance is evaluated not only for its contribution to overall classification but also for how distinctive it is relative to other printshops. Two constraints—spatial coherence and attribute ranking—further ensure the model’s attention focuses on meaningful, localized typographic features. The model achieved 92% classification accuracy (F1 = 0.92), with high-attention patches aligning with known historical typographic distinctions, such as the shape and weight of Korean consonants like ieung (ᄋ) and tigŭt (ᄃ).\nTo validate these findings and ensure that detected features reflect meaningful patterns rather than model-specific artifacts, we extracted embeddings directly from our trained MIL model and used them to generate visual embeddings from high-attention patches.  This method allowed us to leverage the learned visual characteristics the model developed during printshop classification training.  We then applied dimensionality reduction via TSNE and UMAP to these embeddings, revealing distinct clustering of character components and syllables across printshops. Specifically, at 16×16 resolution, patches clustered around complete high-frequency grammatical particles (josa 조사), such as ŭi (의) and e (에). A finer 8×8 resolution revealed clustering around specific character components, such as the angled bottom stroke of tigŭt, which aligns with findings from prior scholarship on the matter (De Fremery, 2011).\nOur approach demonstrates how MIL can support not just classification but also interpretability in computational analysis of historical textual materials. By identifying and clustering subtle but consistent typographic features, the model enables scalable visual-linguistic analysis that was previously only possible through close manual inspection. The success of MIL in this domain suggests its broader applicability to multilingual OCR correction, authorship attribution, and typographic studies."
          },
          {
            "time": "16:50–17:10",
            "title": "A Multi-Modal Approach to Analyzing Facebook Posts Using NLP and Image Processing",
            "author": "Jelena Prokic, Matthew Sung, Mirjam de Bruijn",
            "abstract": "Social media communication has become increasingly visual, with users sharing images, videos, and text. Platforms like Facebook now host large volumes of multi-modal content, where traditional text-based analysis alone is insufficient for capturing the full semantic landscape. This shift requires computational approaches that integrate natural language processing (NLP) with image analysis to better understand how narratives are formed, reinforced, and circulated online—especially in contexts of political unrest or conflict.\nIn this study, we present ongoing research based on a multi-modal dataset of Facebook posts collected as part of the project Digital Warfare in the Sahel. The dataset comprises posts from approximately 100 public Facebook pages associated with the Sahel region and includes multilingual textual content (primarily in French and English), still images, and metadata such as user/page identifiers. Our current study focuses on the integration of text, images, and metadata to investigate the emergence, propagation, and evolution of conflict-related narratives.\nFor the textual data, we apply standard natural language processing techniques. This includes automatic language detection followed by language-specific processing using appropriate spaCy pipelines for English and French. For the visual data, we employ the CLIP (Contrastive Language–Image Pretraining) model to generate image embeddings, enabling semantic alignment with textual concepts within a shared embedding space. We apply UMAP (Uniform Manifold Approximation and Projection) to the image embeddings to cluster semantically similar images and identify recurring visual motifs. Additionally, we use CLIP’s text encoder to generate embeddings for key textual themes, allowing us to retrieve and analyze images that are closely aligned with specific linguistic concepts in the joint embedding space.\nFurthermore, we enrich the semantic analysis with contextual metadata by integrating geographic information and network relationships. By associating text–image pairs with geospatial markers and page-level connectivity (e.g., shared content, page follower connections and international collaborations of page administration), we construct network visualizations that map the diffusion and localization of narratives across digital and regional spaces. This approach allows us to uncover latent structures in the data, such as regional clustering of thematic content, co-occurrence patterns across geographically proximate or socially linked pages, and the emergence of influential nodes in the dissemination of conflict-related narratives.\nThis multi-modal framework allows us to move beyond isolated analysis of text or image content and instead model the interplay between visual and linguistic elements as they contribute to the online construction of meaning."
          },
	    {
		"time": "17:10–17:30",
		"title": "What do self-supervised speech models learn about Dutch? Probing for linguistic structure in Wav2Vec2-NL and HuiBERT",
		"author": "Marianne de Heer Kloots, Hosein Mohebbi, Charlotte Pouw, Gaofei Shen, Willem Zuidema, Martijn Bentum",
		"abstract": "What can self-supervised audio encoders learn about language from being trained on speech recordings? Existing work has shown that a range of linguistic features can be successfully decoded from the inner representations of such models — including phonetic, lexical and syntactic information. However, it's less clear to what extent pre-training on specific languages improves the encoding of language-specific linguistic information.\n\nTo investigate this question, we train two new self-supervised models of spoken Dutch (Wav2Vec2-NL and HuiBERT), and develop an analysis suite to test the encoding of Dutch linguistic features across their internal representations. Both models are trained on 831 hours of spoken Dutch, extracted from the Spoken Dutch Corpus (conversations, interviews, read speech and news reports), as well as the Dutch sections of Multilingual LibriSpeech (audiobook segments), and CommonVoice (read speech).\n\nWe find that pre-training exclusively on Dutch improves the representation of Dutch linguistic features (e.g. phone categories and word-distributional structure) as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition: when fine-tuning a set of comparable self-supervised models for speech-to-text transcription, the Dutch Wav2Vec2-NL model on average obtains a 27% lower word error rate than the multilingual model (de Heer Kloots et al., 2025).\n\nIn a further study, we examine the distribution of different levels of linguistic structure across model layers, as well as their development across model training time. Our analysis suite comprises a range of probing techniques designed to localize model layers with highest alignment to various types of continuous (e.g. acoustic), categorical (e.g. phonetic, syllabic) or combinatorial (e.g. syntactic) structures present in speech recordings. In Wav2Vec2-NL, we find an interesting layerwise organization of linguistic features across model layers, which broadly aligns with the learning rate of those features across model training. In ongoing work, we are testing the generalizability of both findings to another architecture, by pre-training HuiBERT.\n\nOur work develops a methodology for analyzing the encoding of higher-order levels of linguistic structure in self-supervised audio encoders. We also present two self-supervised speech models as a new resource for studying Dutch-specific speech representations. We encourage future work using these resources to examine additional benefits of language-specific pre-training, on model-internal representations as well as downstream applications in speech technology.\n\nReference:\nde Heer Kloots, M., Mohebbi, H., Pouw, C., Shen, G., Zuidema, W., & Bentum, M. (2025). What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training. *Proc. INTERSPEECH*. https://arxiv.org/abs/2506.00981"
	    }
        ]
      },
      {
        "track": "Track C",
        "title": "Analysis of NLP models",
        "talks": [
          {
            "time": "16:30–16:50",
            "title": "Testing the uniform information density hypothesis with large language models",
            "author": "Sander van den Bent",
            "abstract": "The Uniform Information Density (UID) hypothesis posits that, for communication to be efficient and comprehensible, the information conveyed within an utterance should be distributed as evenly as possible. While human language naturally tends toward this balance, the extent to which large language models (LLMs) approximate or can be guided toward UID remains an open area of investigation. This study explores how the application of information-theoretic regularization into the fine-tuning process affects the information density and perplexity of responses generated by LLMs, specifically within the context of Dutch-language generation using a Dutch-language pre-trained GPT-2 model.\nBuilding on prior research that suggests information-theoretic principles—such as minimizing surprisal variance—can improve text generation, this project modifies the loss function used in fine-tuning by incorporating regularizers that target UID principles. The model was fine-tuned on two Dutch-language datasets (OpenSubtitles and Europarl) that vary in structure and content. Regularization was controlled using a λ (lambda) parameter to determine the strength of the UID-based constraint during training.\nTo evaluate the impact of the regularization, we measured both perplexity—a standard metric for fluency and predictive capability in language modeling—and the distribution of information density in generated outputs. UID was quantified using token-level surprisal values derived from the model’s output probabilities.\nThe results show a modest effect of the regularization approach. One of the two datasets showed improvements in both perplexity and information density, but only when a smaller subset of the data was used and the λ values were kept low. In this case, the regularizer appears to guide the model toward generating responses with more uniform information density, without severely impacting fluency. However, when applied to the full dataset or with higher λ values, the regularization often led to diminished performance, including increased perplexity and less natural outputs. In contrast, the second dataset did not exhibit any significant benefits from regularization under any tested conditions.\nThese findings suggest that while UID-inspired regularization holds promise, its effectiveness is highly sensitive to dataset characteristics and regularization strength. Smaller or possibly more semantically uniform datasets may benefit more from UID-based training objectives, while larger corpora might resist such improvements or require more nuanced approaches. This study contributes to a deeper understanding of how information-theoretic principles can be integrated into LLM training and highlights the importance of applying regularization strategies to specific linguistic and dataset contexts."
          },
          {
            "time": "16:50–17:10",
            "title": "Lost in Translation: Investigating LLM Interpretability Through Semantic Similarity and Cross-Lingual Translation",
            "author": "Quin Ye, Jelke Bloem",
            "abstract": "This thesis investigates the cross-lingual semantic consistency of multilingual large language models through the case of BLOOMZ 7b1.\n\nDrawing on previous studies on LLM interpretability with Multi-SimLex similarity ratings, and using LLM and BLOOMZ as machine translation models, this study uses a manually verified subset of 250 word pairs from the Multi-SimLex dataset to evaluate the model's ability to rate semantic similarity and translate word pairs across English and Mandarin through zero-shot prompting. We compare BLOOMZ’s similarity ratings with human-annotated benchmarks and examine its interpretability by assessing the stability and reliability of its forward and back translations.\n\nEvaluation metrics include correlation between BLOOMZ and human similarity ratings, match rates of forward- and back-translated word pairs, and consistency in similarity ratings before and after translation. We complement these evaluations with an error analysis of mismatched translations and rating inconsistencies.\n\nResults show that BLOOMZ ratings has low correlation with human rating in word similarity and tends to avoid assigning extreme similarity scores. For the translation task, BLOOMZ demonstrates the ability to produce correct or near-synonymous translations. However, error analysis reveals frequent hallucinations and illogical outputs, and back-translation preserved surface forms in only about two-thirds of cases.\n\nThis study highlights both the capabilities and limitations of multilingual LLMs in understanding and preserving semantic relationships through translation. Unlike prior work focused solely on translation quality or similarity rating comparison, we investigate the semantic consistency of multilingual LLMs using both similarity benchmarks and translation stability analysis. By combining quantitative analysis with manual annotation and targeted evaluation prompts, we contribute a scalable and interpretable framework for assessing cross-lingual semantic consistency in LLMs. \n\nThis study contribute to the methodology and understanding of researching LLMs model transparency and behaviour in cross-lingual applications. Our results bring valuable insight for multilingual LLM interpretability, LLM machine translation, and translation quality evaluation."
          },
          {
            "time": "17:10–17:30",
            "title": "Words That Matter: Vocabulary Pruning in ModernBERT",
            "author": "Wout De Rijck, Karel D'Oosterlinck, Chris Develder, Thomas Demeester",
            "abstract": "In this study we examine how task-specific vocabulary pruning can improve the efficiency of ModernBERT, a compact encoder-only transformer model. Most language model encoders typically allocate substantial resources to encoder layers, but ModernBERT stands out with a number of architectural and training optimizations to limit its size, despite excellent representational capabilities. \nYet, we hypothesize that the embedding layer, which accounts for approximately 25% of ModernBERT's parameters, still contains significant redundancy, especially when fine-tuning on specific downstream tasks. We propose a set of simple but effective vocabulary pruning techniques that work as a pre-fine-tuning step, using only task training data to identify and remove low-utility tokens.\nOur pruning strategies include frequency-based methods (TF-IDF and raw frequency), attention-based importance scores, semantic clustering, and random baselines. We also introduce an out-of-vocabulary (OOV) handling mechanism based on semantic clustering. This allows removed tokens to be mapped to semantically similar retained tokens during inference. We evaluate our methods on the General Language Understanding Evaluation (GLUE) benchmark and compare results to LoSparse, a state-of-the-art gradient-based encoder-layer pruning technique.\nOur experiments demonstrate that TF-IDF-based pruning with OOV handling consistently outperforms LoSparse for compression ratios up to 20%. The method maintains 97.6% of original performance across GLUE tasks despite removing over 77% of embedding parameters or 20% of total parameters. This compares favorably to LoSparse's 96.2% performance at the same compression level. In addition, our method requires no backpropagation or fine-grained tuning during pruning, making it significantly more lightweight and suitable for deployment in resource-constrained environments. We observe practical benefits including a 14.8% reduction in GPU memory usage and 20% smaller storage requirements, with minimal impact on inference time.\nOur findings challenge the common focus on encoder-layer pruning and demonstrate the underestimated potential of vocabulary-level compression. We make our complete implementation and evaluation tools publicly available to support further research and application in efficient model deployment."
          }
        ]
      },
      {
        "track": "Track D",
        "title": "Conversational agents",
        "talks": [
          {
            "time": "16:30–16:50",
            "title": "As an AI Language Model, I...: Unpacking Chatbot Interaction Patterns Through Sequential Topic Modeling",
            "author": "Eirik Lidsheim",
            "abstract": "Hundreds of millions of people habitually interact with Large Language Model (LLM)-based chatbots. The underlying mechanism of LLM language production has little in common with human cognition, but the fluency of these models’ linguistic output can give a deceptive impression of human-likeness. In turn, users might get false impressions of the capabilities and ‘knowledge’ of their chatbot interlocutors, leading them to request models to do things they are incapable of. This study investigates how and in response to what GPT models (3.5 and 4) communicate their limitations to users, by drawing using 4,064 user-LLM exchanges consisting of a turn 1 user message, and a GPT response prefaced with \"As an AI [language model]”.\n\nAn original sequential neural topic modeling approach was developed to analyze the interactions. The method used two BERTopic models: one for the turn 1 user prompts and another for the \"as an AI\"-prefaced GPT responses; resulting in 14 user topics and 16 GPT topics. Subsequently, proportional co-occurrence rates were calculated for all user- and GPT-topic cross-sections. The findings reveal that the “as an AI”-preface is frequently utilized in several key contexts: to decline unethical user requests; to articulate functional system constraints, such as the inability to generate images or access real-time information; to caution users against accepting uncritically accepting AI-generated advice as truth; and to clarify that LLMs lack the capacity to predict future events.   \n\nThe function and frequency of the \"as an AI...\"-preface is likely explained by how Reinforcement Learning from Human Feedback (RLHF) policies, were applied to moderate model output in sensitive contexts. For example, prompts asking for medical or financial advice often trigger responses that use the preface in what seems like attempts to depersonalize the model and emphasize that any information provided by AI should be treated with caution. Similarly, requests for unethical content such as explicit erotic stories or jailbreaking attempts, are commonly refused with response like \"As an AI Language model I cannot [...] due to ethical guidelines]”.   \n\nThe study also raises broader concerns regarding the implications of LLMs employing self-referential language: the “as an AI”-preface is commonly followed by the first-person pronoun “I”, which falsely implies personhood or sentience where there is none. This is contradictory seeing that the preface is often invoked in attempts to depersonalize the model and to highlight its non-human nature. Such language may obscure the true nature of LLMs as next-token prediction systems, and potentially hinder users' understanding of the technology and its inherent constraints.  \n\nThis investigation contributes to a deeper understanding of user-LLM interaction dynamics, common misconceptions about AI, and how RLHF is applied to moderate LLM behavior. The study also calls for further consideration of how self-referential language used by LLMs might be refined to more accurately represent their capabilities and avoid misleading users. Lastly, the novel sequential modeling approach is flexible and can be re-appropriated in human-LLM interaction research."
          },
          {
            "time": "16:50–17:10",
            "title": "Hybrid Argumentation and Responsible Artificial Intelligence",
            "author": "Khalid Al Khatib, Jan Albert van Laar, Bart Verheij",
            "abstract": "Large Language Models (LLMs) have introduced new possibilities for improving critical areas such as education, scientific research, policy-making, and public debate. While these systems are proficient at generating fluent and persuasive language, they often suffer from critical limitations, including the production of unsubstantiated claims, hallucinated facts, and the spread of misinformation. These risks raise pressing concerns about epistemic integrity and responsible AI use. This research proposes a novel approach to this challenge: the design and evaluation of a hybrid argumentation interface that enables structured, collaborative reasoning between humans and AI systems.\nThe central research question is: How can humans and AI systems engage in argumentative dialogues to collaboratively improve their reasoning? The envisioned system supports iterative dialogue in which both parties, human and AI, contribute to forming, testing, and refining opinions, theories, and proposals. Unlike conventional chatbots that respond with unstructured suggestions, this hybrid system integrates formal argumentation structures into the interaction. It builds on the concept of argument schemes, which are generalized patterns of reasoning, such as practical reasoning or expert opinion. Each scheme is associated with critical questions that help evaluate the strength and validity of an argument. These structures can be used to guide the interaction, enabling both humans and AI to engage critically with each other’s contributions.\nA practical example illustrates the system’s potential. When a user asks how to reduce their carbon footprint, a typical AI might suggest buying an electric vehicle. However, the hybrid interface supports a deeper exchange. If the user raises concerns about cost or charging infrastructure, the system applies argumentation patterns to ask targeted questions such as “Are subsidies available in your region?” or “What alternatives match your personal circumstances?” This encourages a more thoughtful and personalized dialogue, potentially leading to solutions like joining a community solar program.\nThe research combines three key strands: argumentation theory, computational argumentation, and argument mining. It connects formal reasoning frameworks with LLMs to support structured co-reasoning, while embedding ethical safeguards to prevent manipulation and maintain user autonomy. The AI’s outputs are constrained by valid reasoning patterns, and users are given mechanisms to challenge, question, or override AI responses through structured rebuttals. All interactions are recorded through transparent audit logs to ensure accountability.\nThis work aims to support epistemically responsible and context-sensitive human-AI collaboration. It contributes to the development of hybrid intelligence, where AI systems enhance rather than replace human reasoning and deliberation."
          },
          {
            "time": "17:10–17:30",
            "title": "Conversational AI in the Wild: Evaluating Speech Recognition Performance and User Perceptions of Beatrix, a Chatbot Prototype for Geriatric Care",
            "author": "Bram van Dijk, Sirin Aoulad si Ahmed, Simon Mooijaart, Armel Lefebvre, Jan Duin, Jake Johnson, Tiberon Kuiper, Marco Spruit",
            "abstract": "Older populations are increasing on a global scale with one in six individuals predicted to be 60 years or older by 2030 [1]. These populations are vulnerable as they face challenges regarding their physical and mental well-being, social isolation [2], loss of cognitive functioning [3], and a time where less healthcare workers need to care for a larger number of older people with complex needs [4].\n\nConversational AI in the form of Large Language Model (LLM) driven chatbots has shown potential in e.g. applications for mental health [5], but little work has focused on underrepresented populations like older individuals in both the development and evaluation of AI. We developed a chatbot prototype intended to support common geriatric assessments by retrieving quality of life and frailty of an older individual through natural conversations. \n\nThis prototype was evaluated in a small user expert panel with 10 older individuals. The goal was to identify the impact of known bottlenecks before developing and investing future versions. Examples are user perception/experience and speech processing functionality [6, 7]. Here we present preliminary results from in-depth interviews on user experiences after the test session, that showed overall positive attitudes towards the prototype, for example regarding ease of use and added value, though opinions on the human-likeness of the interaction were more mixed. We also report on automatic speech recognition with state-of-the-art transcription models, which are essential in supporting the interaction. We find that larger, multilingual models fare better in terms of word error rate compared to models specifically tailored to Dutch and older populations. \n\n[1] World Health Organisation. Mental health of older adults, https://www.who.int/news-room/fact-sheets/detail/mental-health-of-older-adults (2023).\n\n[2] World Health Organisation. Social isolation and loneliness among older people, https://www.who.int/publications/i/item/9789240030749 (2021).\n\n[3] N. Abdoli, N. Salari, N. Darvishi, S. Jafarpour, M. Solaymani, M. Mohammadi, S. Shohaimi. The global prevalence of major depressive disorder (MDD) among the elderly: a systematic review and meta-analysis. Neurosci. Biobehav. Rev., 132 (2022), pp. 1067-1073\n\n[4] European Union. Measures to tackle labour shortages: Lessons for future policy, https://www.eurofound.europa.eu/en/publications/2023/measures-tackle-labour-shortages-lessons-future-policy) (2023).\n\n[5] Z. Guo, A. Lai, J.H. Thygesen, J. Farrington, T. Keen, & K. Li. Large language models for mental health applications: Systematic review. JMIR mental health, 11(1), e57400, (2024).\n\n[6] B.M.A. van Dijk, A.E.J.L. Lefebvre, & M.R. Spruit. Welzijn. AI: Developing responsible conversational AI for the care of older people through stakeholder involvement. Maturitas, 108616, (2025).\n\n[7] W. Klaassen, B. van Dijk, M. Spruit. A review of challenges in speech-based conversational AI for elderly care. Stud. Health Technol. Inform., 327 (2025), pp. 858-862."
          }
        ]
      }
    ]
  },
  {
    "type": "break",
    "start": "17:30",
    "end": "19:00",
    "title": "Reception"
  }
]
